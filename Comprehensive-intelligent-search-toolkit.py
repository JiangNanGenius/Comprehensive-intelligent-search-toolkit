"""
title: ğŸ” ç»¼åˆæ™ºèƒ½æœç´¢å·¥å…·é›† - Kimi AI + Bocha + RAGä¼˜åŒ– + LLMæ™ºèƒ½æ‘˜è¦ + é“¾æ¥å™ªå£°æ²»ç† (å®Œæ•´ä¿®å¤ç‰ˆ)
author: JiangNanGenius
Github: https://github.com/JiangNanGenius
description: é›†æˆKimi AIåŸºç¡€æœç´¢ã€Bochaä¸“ä¸šæœç´¢ã€ç½‘é¡µè¯»å–ï¼Œæ”¯æŒLLMæ™ºèƒ½æ‘˜è¦æå–ã€RAGå‘é‡åŒ–ã€è¯­ä¹‰é‡æ’åºçš„æ™ºèƒ½æœç´¢å·¥å…·é›†ï¼Œå¼ºåŒ–é“¾æ¥å™ªå£°æ²»ç†å’Œä¼˜é›…å›é€€ï¼Œä¿®å¤è¯­æ³•é”™è¯¯å’Œåˆ†ç‰‡é‡å é—®é¢˜ï¼Œå®ç°å¹¶å‘LLMè°ƒç”¨
required_open_webui_version: 0.4.0
requirements: openai>=1.0.0, requests, beautifulsoup4, numpy, aiohttp
version: 3.8.2
license: MIT
"""

import os
import requests
import json
import asyncio
import aiohttp
import numpy as np
import hashlib
from pydantic import BaseModel, Field
from typing import Callable, Any, Optional, List, Dict
from urllib.parse import urlparse
import unicodedata
import re
from bs4 import BeautifulSoup
from openai import OpenAI
from datetime import datetime
import traceback
from uuid import uuid4
from collections import defaultdict


class Tools:
    class Valves(BaseModel):
        # è°ƒè¯•é…ç½®
        DEBUG_MODE: bool = Field(
            default=False, description="ğŸ› è°ƒè¯•æ¨¡å¼å¼€å…³ - æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯"
        )

        # Kimi AI é…ç½®
        MOONSHOT_API_KEY: str = Field(
            default="", description="ğŸŒ™ Moonshot APIå¯†é’¥ (ç”¨äºKimi AIåŸºç¡€æœç´¢åŠŸèƒ½)"
        )
        MOONSHOT_BASE_URL: str = Field(
            default="https://api.moonshot.cn/v1", description="ğŸŒ™ Moonshot APIåŸºç¡€URL"
        )
        KIMI_MODEL: str = Field(
            default="moonshot-v1-auto", description="ğŸ¤– Kimiä½¿ç”¨çš„æ¨¡å‹"
        )
        KIMI_TEMPERATURE: float = Field(default=0.3, description="ğŸŒ¡ï¸ Kimiæ¨¡å‹æ¸©åº¦å‚æ•°")

        # Bocha é…ç½®
        BOCHA_API_KEY: str = Field(
            default="YOUR_BOCHA_API_KEY",
            description="ğŸ”‘ Bocha AI APIå¯†é’¥ (ç”¨äºä¸“ä¸šä¸­æ–‡æœç´¢å’ŒAIæœç´¢)",
        )
        LANGSEARCH_API_KEY: str = Field(
            default="YOUR_LANGSEARCH_API_KEY",
            description="ğŸ—ï¸ LangSearch APIå¯†é’¥ (ç”¨äºä¸“ä¸šè‹±æ–‡æœç´¢)",
        )

        # è±†åŒ…å‘é‡åŒ–é…ç½®
        ARK_API_KEY: str = Field(
            default="YOUR_ARK_API_KEY",
            description="ğŸ¯ è±†åŒ…ARK APIå¯†é’¥ (ç”¨äºæ–‡æœ¬å‘é‡åŒ–)",
        )
        EMBEDDING_BASE_URL: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3",
            description="ğŸ¯ è±†åŒ…å‘é‡åŒ–APIåŸºç¡€URL",
        )
        EMBEDDING_MODEL: str = Field(
            default="doubao-embedding-large-text-250515",
            description="ğŸ“Š å‘é‡åŒ–æ¨¡å‹åç§°",
        )

        # LLMæ™ºèƒ½æ‘˜è¦é…ç½®
        ENABLE_SMART_SUMMARY: bool = Field(
            default=True, description="ğŸ§  æ˜¯å¦å¯ç”¨LLMæ™ºèƒ½æ‘˜è¦æå–"
        )
        SUMMARY_MIN_CHARS: int = Field(default=300, description="ğŸ“ å•æ¡æ‘˜è¦æœ€å°å­—ç¬¦æ•°")
        SUMMARY_MAX_CHARS: int = Field(default=800, description="ğŸ“ å•æ¡æ‘˜è¦æœ€å¤§å­—ç¬¦æ•°")
        SUMMARY_TEMPERATURE: float = Field(
            default=0.2, description="ğŸŒ¡ï¸ æ‘˜è¦æå–æ¸©åº¦å‚æ•°"
        )
        SUMMARY_COUNT_PER_PAGE: int = Field(default=20, description="ğŸ“„ æ¯é¡µæ‘˜è¦æ•°é‡")

        # è¯­ä¹‰å®‰å…¨åˆ†ç‰‡é…ç½®
        TARGET_CHUNK_CHARS: int = Field(
            default=2800, description="ğŸ¯ ç›®æ ‡åˆ†ç‰‡å¤§å°ï¼ˆå­—ç¬¦ï¼‰"
        )
        MAX_CHUNK_CHARS: int = Field(default=3500, description="â›” å•ç‰‡æœ€å¤§å­—ç¬¦")
        OVERLAP_SENTENCES: int = Field(default=3, description="ğŸ”— ç›¸é‚»åˆ†ç‰‡çš„å¥å­é‡å æ•°")
        MAX_TOTAL_CHUNKS: int = Field(default=32, description="ğŸ“š æ¯é¡µæœ€å¤šå¤„ç†çš„åˆ†ç‰‡æ•°")

        # å¹¶å‘æ§åˆ¶é…ç½®
        LLM_MAX_CONCURRENCY: int = Field(
            default=3, description="ğŸ•Šï¸ åˆ†ç‰‡å¹¶å‘æ‘˜è¦çš„æœ€å¤§å¹¶å‘æ•°"
        )
        LLM_RETRIES: int = Field(default=2, description="ğŸ” LLMè°ƒç”¨å¤±è´¥çš„é‡è¯•æ¬¡æ•°")
        LLM_BACKOFF_BASE_SEC: float = Field(
            default=0.8, description="â³ é‡è¯•é€€é¿åŸºæ•°ï¼ˆç§’ï¼‰"
        )

        # åˆ†ç‰‡ä¿æŠ¤ç­–ç•¥
        PRESERVE_TABLES: bool = Field(default=True, description="ğŸ“Š åˆ†ç‰‡æ—¶æ•´å—ä¿ç•™è¡¨æ ¼")
        PRESERVE_CODEBLOCKS: bool = Field(
            default=True, description="ğŸ§© åˆ†ç‰‡æ—¶æ•´å—ä¿ç•™ä»£ç å—"
        )
        PRESERVE_LINKS: bool = Field(
            default=True, description="ğŸ”— åˆ†ç‰‡æ—¶ä¿è¯é“¾æ¥ä¸è¢«æ‰“æ•£"
        )
        DENOISE_LINK_SECTIONS: bool = Field(
            default=True, description="ğŸ§¹ ç§»é™¤çº¯é“¾æ¥/å¯¼èˆªæ®µè½"
        )

        # æ‘˜è¦ç­–ç•¥
        MAP_SUMMARY_PER_CHUNK: int = Field(
            default=7, description="ğŸ§­ map é˜¶æ®µï¼šæ¯ä¸ªåˆ†ç‰‡è¦æå–çš„æ‘˜è¦æ¡æ•°"
        )
        REDUCE_SUMMARY_LIMIT: int = Field(
            default=20, description="ğŸ§° reduce é˜¶æ®µï¼šæ•´é¡µä¿ç•™çš„æ‘˜è¦æ¡æ•°ä¸Šé™"
        )
        ENABLE_DETAILED_EXTRACTION: bool = Field(
            default=True, description="ğŸ“ æ˜¯å¦å¯ç”¨è¯¦ç»†ä¿¡æ¯æå–æ¨¡å¼"
        )
        ENCOURAGE_COMPREHENSIVE: bool = Field(
            default=True, description="ğŸ¯ æ˜¯å¦é¼“åŠ±å…¨é¢è¦†ç›–å„ä¸ªæ–¹é¢"
        )

        # LLMé…ç½®
        SEGMENTER_API_KEY: str = Field(
            default="", description="ğŸ”§ LLM APIå¯†é’¥ (ç•™ç©ºåˆ™ä½¿ç”¨Moonshotå¯†é’¥)"
        )
        SEGMENTER_BASE_URL: str = Field(
            default="", description="ğŸ”§ LLM APIåŸºç¡€URL (ç•™ç©ºåˆ™ä½¿ç”¨Moonshot URL)"
        )
        SEGMENTER_MODEL: str = Field(
            default="moonshot-v1-auto", description="ğŸ¤– LLMä½¿ç”¨çš„æ¨¡å‹"
        )
        SEGMENTER_TEMPERATURE: float = Field(default=0.1, description="ğŸŒ¡ï¸ LLMæ¸©åº¦å‚æ•°")

        # è¯„åˆ†æƒé‡é…ç½®
        ENABLE_ANSWERABILITY: bool = Field(
            default=True, description="ğŸ’¡ æ˜¯å¦å¯ç”¨å¯å›ç­”æ€§è¯„åˆ†"
        )
        ANSWERABILITY_WEIGHT: float = Field(
            default=0.25, description="âš–ï¸ å¯å›ç­”æ€§åˆ†æ•°æƒé‡"
        )
        SUMMARY_RELEVANCE_WEIGHT: float = Field(
            default=0.35, description="ğŸ¯ LLMæ‘˜è¦ç›¸å…³åº¦æƒé‡"
        )
        RERANK_WEIGHT: float = Field(default=0.4, description="âš–ï¸ é‡æ’åºåˆ†æ•°æƒé‡")
        RAG_WEIGHT: float = Field(default=0.3, description="âš–ï¸ RAGç›¸ä¼¼åº¦æƒé‡")

        # æœç´¢ç«¯ç‚¹é…ç½®
        CHINESE_WEB_SEARCH_ENDPOINT: str = Field(
            default="https://api.bochaai.com/v1/web-search",
            description="ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç½‘é¡µæœç´¢APIç«¯ç‚¹",
        )
        ENGLISH_WEB_SEARCH_ENDPOINT: str = Field(
            default="https://api.langsearch.com/v1/web-search",
            description="ğŸŒ è‹±æ–‡ç½‘é¡µæœç´¢APIç«¯ç‚¹",
        )
        AI_SEARCH_ENDPOINT: str = Field(
            default="https://api.bochaai.com/v1/ai-search",
            description="ğŸ¤– AIæ™ºèƒ½æœç´¢APIç«¯ç‚¹",
        )
        RERANK_ENDPOINT: str = Field(
            default="https://api.bochaai.com/v1/rerank",
            description="ğŸ¯ è¯­ä¹‰é‡æ’åºAPIç«¯ç‚¹",
        )

        # RAGå’Œé‡æ’åºé…ç½®
        ENABLE_RAG_ENHANCEMENT: bool = Field(
            default=True, description="ğŸ§  æ˜¯å¦å¯ç”¨RAGå‘é‡åŒ–ä¼˜åŒ–"
        )
        ENABLE_SEMANTIC_RERANK: bool = Field(
            default=True, description="ğŸ¯ æ˜¯å¦å¯ç”¨è¯­ä¹‰é‡æ’åº"
        )
        RERANK_MODEL: str = Field(default="gte-rerank", description="ğŸ¯ é‡æ’åºæ¨¡å‹åç§°")
        SIMILARITY_THRESHOLD: float = Field(
            default=0.08, description="ğŸ“Š RAGç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        EMIT_ONLY_RAG_PASS: bool = Field(
            default=True, description="ğŸ¯ ä»…è¿”å›é€šè¿‡é˜ˆå€¼çš„RAGç»“æœ"
        )
        RERANK_TOP_N: int = Field(default=25, description="ğŸ¯ é‡æ’åºè¿”å›ç»“æœæ•°é‡")

        # å†…å®¹è¿”å›æ§åˆ¶
        RETURN_CONTENT_IN_RESULTS: bool = Field(
            default=True, description="ğŸ“„ æ˜¯å¦åœ¨ç»“æœJSONä¸­æºå¸¦contentå­—æ®µ"
        )
        RETURN_CONTENT_MAX_CHARS: int = Field(
            default=-1, description="ğŸ“ è¿”å›contentçš„æœ€å¤§å­—ç¬¦æ•°ï¼Œ<=0è¡¨ç¤ºä¸æˆªæ–­"
        )
        CITATION_DOC_MAX_CHARS: int = Field(
            default=-1, description="ğŸ“‹ å¼•ç”¨ä¸­æ–‡æ¡£æœ€å¤§å­—ç¬¦æ•°ï¼Œ<=0è¡¨ç¤ºä¸æˆªæ–­"
        )
        CITATION_CHUNK_SIZE: int = Field(
            default=0, description="ğŸ”— å¼•ç”¨åˆ†ç‰‡å¤§å°ï¼Œ<=0è¡¨ç¤ºä¸åˆ†ç‰‡"
        )
        UNIQUE_REFERENCE_NAMES: bool = Field(
            default=True, description="ğŸ¯ å¼•ç”¨åå”¯ä¸€ï¼Œé¿å…UIåˆå¹¶/æŠ˜å "
        )
        PERSIST_CITATIONS: bool = Field(
            default=True, description="ğŸ’¾ å¤šæ¬¡è°ƒç”¨æ—¶ä¿ç•™å¹¶é‡å‘å†å²å¼•ç”¨"
        )
        PERSIST_CITATIONS_MAX: int = Field(
            default=100, description="ğŸ“š å†å²å¼•ç”¨æœ€å¤šä¿å­˜æ¡æ•°"
        )
        RAW_OUTPUT_FORMAT: str = Field(
            default="json", description="ğŸ“„ rawè¾“å‡ºæ ¼å¼: jsonæˆ–text"
        )

        # é€šç”¨é…ç½®
        CHINESE_SEARCH_COUNT: int = Field(default=15, description="ğŸ‡¨ğŸ‡³ ä¸­æ–‡æœç´¢ç»“æœæ•°é‡")
        ENGLISH_SEARCH_COUNT: int = Field(default=15, description="ğŸŒ è‹±æ–‡æœç´¢ç»“æœæ•°é‡")
        AI_SEARCH_COUNT: int = Field(default=15, description="ğŸ¤– AIæœç´¢ç»“æœæ•°é‡")
        CITATION_LINKS: bool = Field(
            default=True, description="ğŸ”— æ˜¯å¦å‘é€å¼•ç”¨é“¾æ¥å’Œå…ƒæ•°æ®"
        )
        FRESHNESS: str = Field(
            default="noLimit",
            description="â° æœç´¢æ—¶é—´èŒƒå›´ (noLimit, oneDay, oneWeek, oneMonth, oneYear)",
        )
        MAX_RETRIES: int = Field(default=3, description="ğŸ”„ æœ€å¤§é‡è¯•æ¬¡æ•°")

        # Jinaç½‘é¡µè¯»å–é…ç½®
        JINA_API_KEY: str = Field(
            default="jina_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
            description="ğŸŒ Jina APIå¯†é’¥ (ç”¨äºç½‘é¡µè¯»å–)",
        )

    def __init__(self):
        self.valves = self.Valves()
        self.kimi_client = None
        self.segmenter_client = None
        self.embedding_cache = {}
        self.citation = False
        self.run_seq = 0
        self.citations_history = []

    # ======================== Kimi AI æœç´¢ ========================
    async def kimi_ai_search(
        self,
        search_query: str,
        context: str = "",
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ™ Kimi AIåŸºç¡€æœç´¢"""

        # === å†…åµŒå·¥å…·å‡½æ•° ===
        def get_segmenter_client():
            api_key = self.valves.SEGMENTER_API_KEY or self.valves.MOONSHOT_API_KEY
            base_url = self.valves.SEGMENTER_BASE_URL or self.valves.MOONSHOT_BASE_URL
            if not api_key:
                raise ValueError("éœ€è¦APIå¯†é’¥")
            if (
                self.segmenter_client is None
                or self.segmenter_client.api_key != api_key
                or getattr(self.segmenter_client, "base_url_stored", None) != base_url
            ):
                self.segmenter_client = OpenAI(base_url=base_url, api_key=api_key)
                self.segmenter_client.base_url_stored = base_url
            return self.segmenter_client

        def next_run_id(tool: str) -> str:
            self.run_seq += 1
            return f"{tool}-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{self.run_seq}"

        def take_text(text: str, max_chars: int) -> str:
            if text is None:
                return ""
            return text if (max_chars is None or max_chars <= 0) else text[:max_chars]

        def split_text_chunks(text: str, size: int) -> List[str]:
            if text is None:
                return [""]
            if size is None or size <= 0:
                return [text]
            return [text[i : i + size] for i in range(0, len(text), size)]

        async def emit_citation_data(r: Dict, __event_emitter__, run_id: str, idx: int):
            if not (__event_emitter__ and self.valves.CITATION_LINKS):
                return

            full_doc = r.get("content") or ""
            doc_for_emit = take_text(full_doc, self.valves.CITATION_DOC_MAX_CHARS)
            chunks = split_text_chunks(doc_for_emit, self.valves.CITATION_CHUNK_SIZE)

            base_title = (r.get("title") or "") or (r.get("url") or "Source")
            base_url = (r.get("url") or "").strip()

            for ci, chunk in enumerate(chunks, 1):
                if self.valves.UNIQUE_REFERENCE_NAMES:
                    src_name = f"{base_title} | {base_url} | {run_id}#{idx}-{ci}-{uuid4().hex[:6]}"
                else:
                    src_name = base_url or base_title

                payload = {
                    "type": "citation",
                    "data": {
                        "document": [chunk],
                        "metadata": [
                            {
                                "title": base_title,
                                "date_accessed": datetime.now().isoformat(),
                            }
                        ],
                        "source": {
                            "name": src_name,
                            "url": base_url or "",
                            "type": r.get("source_type", "webpage"),
                        },
                    },
                }
                await __event_emitter__(payload)

                if self.valves.PERSIST_CITATIONS:
                    self.citations_history.append(payload)
                    if len(self.citations_history) > self.valves.PERSIST_CITATIONS_MAX:
                        self.citations_history = self.citations_history[
                            -self.valves.PERSIST_CITATIONS_MAX :
                        ]

        # è·å–run_idå¹¶é‡æ”¾å†å²å¼•ç”¨
        run_id = next_run_id("kimi")
        if self.valves.PERSIST_CITATIONS and __event_emitter__:
            for old in self.citations_history:
                await __event_emitter__(old)

        def debug_log(message: str, error=None):
            if self.valves.DEBUG_MODE:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"[DEBUG {timestamp}] {message}")
                if error:
                    print(f"[DEBUG ERROR] {str(error)}")
                    print(f"[DEBUG TRACEBACK] {traceback.format_exc()}")

        async def emit_status(
            description: str, status: str = "in_progress", done: bool = False
        ):
            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "status": status,
                            "description": description,
                            "done": done,
                            "action": f"kimi_search:{run_id}",
                        },
                    }
                )

        def parse_search_results(content: str):
            debug_log(f"è§£ææœç´¢ç»“æœå†…å®¹: {content[:200]}...")

            source_pattern = r"\[æ¥æºï¼š(https?://[^\]]+)\]"
            sources = re.findall(source_pattern, content)

            reference_pattern = (
                r"å‚è€ƒç½‘ç«™é“¾æ¥ï¼š\s*\n((?:\d+\.\s*\[https?://[^\]]+\]\([^\)]+\)\s*\n?)+)"
            )
            reference_match = re.search(reference_pattern, content, re.MULTILINE)

            if reference_match:
                reference_links = reference_match.group(1)
                link_pattern = r"\[(https?://[^\]]+)\]\([^\)]+\)"
                additional_sources = re.findall(link_pattern, reference_links)
                sources.extend(additional_sources)

            unique_sources = list(set(sources))
            debug_log(f"æ‰¾åˆ° {len(unique_sources)} ä¸ªå”¯ä¸€æ¥æº")

            sections = re.split(r"\n\d+\.\s*\*\*([^*]+)\*\*ï¼š", content)
            search_results = []

            for i in range(1, len(sections), 2):
                if i + 1 < len(sections):
                    title = sections[i].strip()
                    content_part = sections[i + 1].strip()

                    part_urls = re.findall(source_pattern, content_part)
                    main_url = (
                        part_urls[0]
                        if part_urls
                        else (unique_sources[0] if unique_sources else "")
                    )

                    clean_content = re.sub(
                        r"\[æ¥æºï¼š[^\]]+\]", "", content_part
                    ).strip()

                    search_results.append(
                        {
                            "content": clean_content,
                            "title": title,
                            "url": main_url,
                            "site_name": (
                                main_url.split("/")[2] if main_url else "Unknown"
                            ),
                            "date_published": datetime.now().strftime("%Y-%m-%d"),
                            "source_type": "Kimi AIåŸºç¡€æœç´¢",
                        }
                    )

            if not search_results and unique_sources:
                search_results.append(
                    {
                        "content": re.sub(r"\[æ¥æºï¼š[^\]]+\]", "", content).strip(),
                        "title": search_query,
                        "url": unique_sources[0],
                        "site_name": (
                            unique_sources[0].split("/")[2]
                            if unique_sources
                            else "Unknown"
                        ),
                        "date_published": datetime.now().strftime("%Y-%m-%d"),
                        "source_type": "Kimi AIåŸºç¡€æœç´¢",
                    }
                )

            debug_log(f"è§£æå®Œæˆï¼Œå¾—åˆ° {len(search_results)} ä¸ªç»“æœ")
            return search_results, unique_sources

        def get_kimi_client():
            if (
                self.kimi_client is None
                or self.kimi_client.api_key != self.valves.MOONSHOT_API_KEY
            ):
                if not self.valves.MOONSHOT_API_KEY:
                    raise ValueError("Moonshot APIå¯†é’¥æ˜¯å¿…éœ€çš„")
                self.kimi_client = OpenAI(
                    base_url=self.valves.MOONSHOT_BASE_URL,
                    api_key=self.valves.MOONSHOT_API_KEY,
                )
            return self.kimi_client

        def chat_with_kimi(messages: list):
            client = get_kimi_client()
            completion = client.chat.completions.create(
                model=self.valves.KIMI_MODEL,
                messages=messages,
                temperature=self.valves.KIMI_TEMPERATURE,
            )
            return completion.choices[0]

        try:
            debug_log(f"å¼€å§‹Kimi AIæœç´¢: {search_query}, ä¸Šä¸‹æ–‡: {context}")

            if context:
                enhanced_query = f"åœ¨'{context}'çš„èƒŒæ™¯ä¸‹ï¼Œæœç´¢å…³äº'{search_query}'çš„ä¿¡æ¯ã€‚è¯·æä¾›è¯¦ç»†ä¸”æœ‰å¼•ç”¨æ¥æºçš„å›ç­”ã€‚"
                await emit_status(
                    f"ğŸŒ™ å¼€å§‹Kimi AIæœç´¢: {search_query} (èƒŒæ™¯: {context})"
                )
            else:
                enhanced_query = (
                    f"æœç´¢å…³äº'{search_query}'çš„ä¿¡æ¯ã€‚è¯·æä¾›è¯¦ç»†ä¸”æœ‰å¼•ç”¨æ¥æºçš„å›ç­”ã€‚"
                )
                await emit_status(f"ğŸŒ™ å¼€å§‹Kimi AIæœç´¢: {search_query}")

            system_prompt = """ä½ æ˜¯Kimi AIï¼Œä¸€ä¸ªåŸºç¡€çš„æœç´¢åŠ©æ‰‹ã€‚
è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¦æ±‚å›ç­”ï¼š

**å›ç­”ç»“æ„ï¼š**
1. **æ ‡é¢˜1**ï¼šå†…å®¹æè¿°ã€‚[æ¥æºï¼šå®Œæ•´URL]
2. **æ ‡é¢˜2**ï¼šå†…å®¹æè¿°ã€‚[æ¥æºï¼šå®Œæ•´URL]  
3. **æ ‡é¢˜3**ï¼šå†…å®¹æè¿°ã€‚[æ¥æºï¼šå®Œæ•´URL]

**å¼•ç”¨æ ¼å¼ï¼š**
- åœ¨æ¯ä¸ªä¿¡æ¯ç‚¹åä½¿ç”¨ï¼š[æ¥æºï¼šå®Œæ•´URL]
- åœ¨å›ç­”æœ«å°¾åˆ—å‡ºæ‰€æœ‰å‚è€ƒç½‘ç«™é“¾æ¥

è¯·åŸºäºæœç´¢ç»“æœæä¾›ç›¸å…³ä¿¡æ¯ã€‚"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": enhanced_query},
            ]

            retry_count = 0
            while retry_count < self.valves.MAX_RETRIES:
                try:
                    debug_log(f"å‘é€Kimiè¯·æ±‚ï¼Œé‡è¯•æ¬¡æ•°: {retry_count}")
                    choice = chat_with_kimi(messages)
                    await emit_status("âœ… æœç´¢å®Œæˆï¼Œæ­£åœ¨å¤„ç†ç»“æœ...")

                    content = choice.message.content
                    debug_log(f"Kimiæœ€ç»ˆå›ç­”: {content[:500]}...")

                    search_results, sources = parse_search_results(content)

                    for idx, r in enumerate(search_results):
                        await emit_citation_data(r, __event_emitter__, run_id, idx)

                    results_data = []
                    for r in search_results:
                        result_item = {
                            "title": r.get("title", ""),
                            "url": r.get("url", ""),
                            "snippet": take_text(r.get("content", ""), 300),
                        }
                        if self.valves.RETURN_CONTENT_IN_RESULTS:
                            result_item["content"] = take_text(
                                r.get("content", ""),
                                self.valves.RETURN_CONTENT_MAX_CHARS,
                            )
                        results_data.append(result_item)

                    result = {
                        "summary": {
                            "total_results": len(search_results),
                            "total_sources": len(sources),
                            "search_query": search_query,
                            "context": context,
                            "search_type": "ğŸŒ™ Kimi AIåŸºç¡€æœç´¢",
                            "timestamp": datetime.now().isoformat(),
                        },
                        "results": results_data,
                    }

                    await emit_status(
                        "ğŸ‰ Kimi AIæœç´¢å®Œæˆï¼", status="complete", done=True
                    )
                    return json.dumps(result, ensure_ascii=False, indent=2)

                except Exception as e:
                    retry_count += 1
                    debug_log(
                        f"Kimiè¯·æ±‚å¤±è´¥ï¼Œé‡è¯• {retry_count}/{self.valves.MAX_RETRIES}", e
                    )
                    if retry_count < self.valves.MAX_RETRIES:
                        await emit_status(
                            f"âš ï¸ é‡è¯• {retry_count}/{self.valves.MAX_RETRIES}: {str(e)}"
                        )
                        await asyncio.sleep(1)
                    else:
                        raise e

            raise Exception("æœç´¢è¿‡ç¨‹å¼‚å¸¸ç»“æŸ")

        except Exception as e:
            debug_log("Kimi AIæœç´¢å¤±è´¥", e)
            await emit_status(
                f"âŒ Kimi AIæœç´¢å¤±è´¥: {str(e)}", status="error", done=True
            )

            error_result = {
                "search_results": [],
                "error": str(e),
                "summary": {
                    "total_results": 0,
                    "total_sources": 0,
                    "search_query": search_query,
                    "context": context,
                    "search_type": "ğŸŒ™ Kimi AIåŸºç¡€æœç´¢",
                    "timestamp": datetime.now().isoformat(),
                    "status": "error",
                },
            }
            return json.dumps(error_result, ensure_ascii=False, indent=2)

    # ======================== ä¸“ä¸šä¸­æ–‡æœç´¢ ========================
    async def search_chinese_web(
        self,
        query: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸ‡¨ğŸ‡³ ä¸“ä¸šä¸­æ–‡ç½‘é¡µæœç´¢å·¥å…·"""

        def next_run_id(tool: str) -> str:
            self.run_seq += 1
            return f"{tool}-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{self.run_seq}"

        def take_text(text: str, max_chars: int) -> str:
            if text is None:
                return ""
            return text if (max_chars is None or max_chars <= 0) else text[:max_chars]

        def split_text_chunks(text: str, size: int) -> List[str]:
            if text is None:
                return [""]
            if size is None or size <= 0:
                return [text]
            return [text[i : i + size] for i in range(0, len(text), size)]

        async def emit_citation_data(r: Dict, __event_emitter__, run_id: str, idx: int):
            if not (__event_emitter__ and self.valves.CITATION_LINKS):
                return

            full_doc = r.get("content") or ""
            doc_for_emit = take_text(full_doc, self.valves.CITATION_DOC_MAX_CHARS)
            chunks = split_text_chunks(doc_for_emit, self.valves.CITATION_CHUNK_SIZE)

            base_title = (r.get("title") or "") or (r.get("url") or "Source")
            base_url = (r.get("url") or "").strip()

            for ci, chunk in enumerate(chunks, 1):
                if self.valves.UNIQUE_REFERENCE_NAMES:
                    src_name = f"{base_title} | {base_url} | {run_id}#{idx}-{ci}-{uuid4().hex[:6]}"
                else:
                    src_name = base_url or base_title

                payload = {
                    "type": "citation",
                    "data": {
                        "document": [chunk],
                        "metadata": [
                            {
                                "title": base_title,
                                "date_accessed": datetime.now().isoformat(),
                            }
                        ],
                        "source": {
                            "name": src_name,
                            "url": base_url or "",
                            "type": r.get("source_type", "webpage"),
                        },
                    },
                }
                await __event_emitter__(payload)

                if self.valves.PERSIST_CITATIONS:
                    self.citations_history.append(payload)
                    if len(self.citations_history) > self.valves.PERSIST_CITATIONS_MAX:
                        self.citations_history = self.citations_history[
                            -self.valves.PERSIST_CITATIONS_MAX :
                        ]

        async def get_text_embedding(text: str) -> Optional[List[float]]:
            if not self.valves.ENABLE_RAG_ENHANCEMENT or not self.valves.ARK_API_KEY:
                return None

            text_hash = hashlib.sha256(text.encode("utf-8")).hexdigest()
            if text_hash in self.embedding_cache:
                return self.embedding_cache[text_hash]

            try:
                headers = {
                    "Authorization": f"Bearer {self.valves.ARK_API_KEY}",
                    "Content-Type": "application/json",
                }

                clean_text = text.strip()[:4000]
                if not clean_text:
                    return None

                payload = {
                    "model": self.valves.EMBEDDING_MODEL,
                    "input": [clean_text],
                    "encoding_format": "float",
                }

                response = requests.post(
                    f"{self.valves.EMBEDDING_BASE_URL}/embeddings",
                    headers=headers,
                    json=payload,
                    timeout=30,
                )
                response.raise_for_status()
                data = response.json()

                if "data" in data and len(data["data"]) > 0:
                    embedding = data["data"][0]["embedding"]
                    self.embedding_cache[text_hash] = embedding
                    return embedding
                else:
                    return None
            except Exception:
                return None

        def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:
            try:
                v1 = np.array(vec1)
                v2 = np.array(vec2)
                return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))
            except Exception:
                return 0.0

        run_id = next_run_id("zh-web")
        if self.valves.PERSIST_CITATIONS and __event_emitter__:
            for old in self.citations_history:
                await __event_emitter__(old)

        def debug_log(message: str, error=None):
            if self.valves.DEBUG_MODE:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"[DEBUG {timestamp}] {message}")
                if error:
                    print(f"[DEBUG ERROR] {str(error)}")
                    print(f"[DEBUG TRACEBACK] {traceback.format_exc()}")

        async def emit_status(
            description: str, status: str = "in_progress", done: bool = False
        ):
            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "status": status,
                            "description": description,
                            "done": done,
                            "action": f"zh_search:{run_id}",
                        },
                    }
                )

        async def enhance_results_with_rag(results: List[Dict]) -> List[Dict]:
            if not self.valves.ENABLE_RAG_ENHANCEMENT or not results:
                debug_log("RAGæœªå¯ç”¨æˆ–ç»“æœä¸ºç©º")
                return results

            try:
                await emit_status(f"ğŸ§  æ­£åœ¨è¿›è¡ŒRAGå‘é‡åŒ–ä¼˜åŒ– ({len(results)} ä¸ªç»“æœ)")
                debug_log(f"å¼€å§‹RAGä¼˜åŒ–ï¼ŒæŸ¥è¯¢: {query}, ç»“æœæ•°: {len(results)}")

                query_embedding = await get_text_embedding(query)
                if not query_embedding:
                    debug_log("æŸ¥è¯¢å‘é‡åŒ–å¤±è´¥ï¼Œè¿”å›åŸç»“æœ")
                    return results

                enhanced_results = []
                for i, result in enumerate(results):
                    content = result.get("content", "")
                    if not content:
                        debug_log(f"ç»“æœ {i} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                        continue

                    content_embedding = await get_text_embedding(content)
                    if content_embedding:
                        similarity = calculate_similarity(
                            query_embedding, content_embedding
                        )
                        result["rag_similarity"] = similarity
                        result["rag_enhanced"] = True
                        debug_log(f"ç»“æœ {i} ç›¸ä¼¼åº¦: {similarity:.3f}")
                        enhanced_results.append(result)
                    else:
                        result["rag_similarity"] = 0.0
                        result["rag_enhanced"] = False
                        enhanced_results.append(result)
                        debug_log(f"ç»“æœ {i} å‘é‡åŒ–å¤±è´¥ï¼Œä¿ç•™åŸç»“æœ")

                enhanced_results.sort(
                    key=lambda x: x.get("rag_similarity", 0), reverse=True
                )
                debug_log(f"RAGä¼˜åŒ–å®Œæˆï¼Œä¿ç•™ {len(enhanced_results)} ä¸ªç»“æœ")
                await emit_status(f"âœ… RAGä¼˜åŒ–å®Œæˆ")
                return enhanced_results

            except Exception as e:
                debug_log("RAGä¼˜åŒ–å¤±è´¥", e)
                return results

        async def rerank_results(results: List[Dict]) -> List[Dict]:
            if (
                not self.valves.ENABLE_SEMANTIC_RERANK
                or not results
                or not self.valves.BOCHA_API_KEY
            ):
                debug_log("è¯­ä¹‰é‡æ’åºæœªå¯ç”¨æˆ–é…ç½®ä¸å®Œæ•´")
                return results

            try:
                await emit_status(f"ğŸ¯ æ­£åœ¨è¿›è¡Œè¯­ä¹‰é‡æ’åº ({len(results)} ä¸ªç»“æœ)")
                debug_log(f"å¼€å§‹è¯­ä¹‰é‡æ’åºï¼ŒæŸ¥è¯¢: {query}")

                documents = []
                for i, result in enumerate(results):
                    content = result.get("content", "")[:4000]
                    if content:
                        documents.append(content)

                if not documents:
                    debug_log("æ²¡æœ‰æœ‰æ•ˆæ–‡æ¡£ï¼Œè·³è¿‡é‡æ’åº")
                    return results

                headers = {
                    "Authorization": f"Bearer {self.valves.BOCHA_API_KEY}",
                    "Content-Type": "application/json",
                }

                payload = {
                    "model": self.valves.RERANK_MODEL,
                    "query": query,
                    "documents": documents,
                    "top_n": min(self.valves.RERANK_TOP_N, len(documents)),
                    "return_documents": False,
                }

                response = requests.post(
                    self.valves.RERANK_ENDPOINT,
                    headers=headers,
                    json=payload,
                    timeout=60,
                )
                response.raise_for_status()
                data = response.json()

                if "data" in data and "results" in data["data"]:
                    rerank_results_data = data["data"]["results"]
                    reranked_results = []

                    for rerank_item in rerank_results_data:
                        index = rerank_item.get("index", 0)
                        relevance_score = rerank_item.get("relevance_score", 0.0)
                        if index < len(results):
                            result = results[index].copy()
                            result["rerank_score"] = relevance_score
                            result["rerank_enhanced"] = True
                            reranked_results.append(result)

                    debug_log(f"é‡æ’åºå®Œæˆï¼Œè¿”å› {len(reranked_results)} ä¸ªç»“æœ")
                    await emit_status(f"âœ… è¯­ä¹‰é‡æ’åºå®Œæˆ")
                    return reranked_results
                else:
                    debug_log("é‡æ’åºå“åº”æ ¼å¼å¼‚å¸¸")
                    return results

            except Exception as e:
                debug_log("è¯­ä¹‰é‡æ’åºå¤±è´¥", e)
                return results

        try:
            debug_log(f"å¼€å§‹ä¸­æ–‡ç½‘é¡µæœç´¢: {query}")
            await emit_status(f"ğŸ” æ­£åœ¨è¿›è¡Œä¸“ä¸šä¸­æ–‡ç½‘é¡µæœç´¢: {query}")

            headers = {
                "Authorization": f"Bearer {self.valves.BOCHA_API_KEY}",
                "Content-Type": "application/json",
            }

            payload = {
                "query": query,
                "freshness": self.valves.FRESHNESS,
                "summary": True,
                "count": self.valves.CHINESE_SEARCH_COUNT,
            }

            await emit_status("â³ æ­£åœ¨è¿æ¥ä¸“ä¸šä¸­æ–‡æœç´¢æœåŠ¡å™¨...")
            resp = requests.post(
                self.valves.CHINESE_WEB_SEARCH_ENDPOINT,
                headers=headers,
                json=payload,
                timeout=120,
            )
            resp.raise_for_status()
            data = resp.json()

            source_context_list = []
            len_raw = 0

            if "data" in data and "webPages" in data["data"]:
                web_pages = data["data"]["webPages"]
                if "value" in web_pages and isinstance(web_pages["value"], list):
                    len_raw = len(web_pages["value"])
                    await emit_status(f"ğŸ“„ æ­£åœ¨å¤„ç† {len_raw} ä¸ªä¸­æ–‡ç½‘é¡µç»“æœ...")

                    for i, item in enumerate(web_pages["value"]):
                        url = item.get("url", "")
                        snippet = item.get("snippet", "")
                        summary = item.get("summary", "")
                        name = item.get("name", "")
                        site_name = item.get("siteName", "")
                        date_published = item.get("datePublished", "")

                        content = summary or snippet
                        if not content:
                            debug_log(f"ä¸­æ–‡æœç´¢ç»“æœ {i} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                            continue

                        result_item = {
                            "content": content,
                            "title": name,
                            "url": url,
                            "site_name": site_name if site_name else "",
                            "date_published": date_published if date_published else "",
                            "source_type": "ä¸“ä¸šä¸­æ–‡ç½‘é¡µ",
                        }
                        source_context_list.append(result_item)

            debug_log(f"åŸå§‹ä¸­æ–‡æœç´¢ç»“æœæ•°: {len(source_context_list)}")

            if self.valves.ENABLE_RAG_ENHANCEMENT:
                source_context_list = await enhance_results_with_rag(
                    source_context_list
                )

            if self.valves.ENABLE_SEMANTIC_RERANK:
                source_context_list = await rerank_results(source_context_list)

            if self.valves.ENABLE_RAG_ENHANCEMENT and self.valves.EMIT_ONLY_RAG_PASS:
                _thr = float(self.valves.SIMILARITY_THRESHOLD)
                source_context_list = [
                    r
                    for r in source_context_list
                    if float(r.get("rag_similarity") or 0.0) >= _thr
                ]

            for idx, r in enumerate(source_context_list):
                await emit_citation_data(r, __event_emitter__, run_id, idx)

            await emit_status(
                status="complete",
                description=f"ğŸ‰ ä¸­æ–‡ç½‘é¡µæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(source_context_list)} ä¸ªç»“æœ",
                done=True,
            )

            results_data = []
            for r in source_context_list:
                result_item = {
                    "title": (r.get("title") or ""),
                    "url": r.get("url"),
                    "rag_similarity": float(r.get("rag_similarity") or 0.0),
                    "rerank_score": float(r.get("rerank_score") or 0.0),
                    "snippet": take_text(r.get("content", ""), 300),
                }
                if self.valves.RETURN_CONTENT_IN_RESULTS:
                    result_item["content"] = take_text(
                        r.get("content", ""), self.valves.RETURN_CONTENT_MAX_CHARS
                    )
                results_data.append(result_item)

            return json.dumps(
                {
                    "summary": {
                        "rag_threshold": self.valves.SIMILARITY_THRESHOLD,
                        "kept": len(source_context_list),
                        "total": len_raw,
                        "search_type": "ğŸ‡¨ğŸ‡³ ä¸“ä¸šä¸­æ–‡ç½‘é¡µ",
                    },
                    "results": results_data,
                },
                ensure_ascii=False,
                indent=2,
            )

        except Exception as e:
            debug_log("ä¸­æ–‡ç½‘é¡µæœç´¢å¤±è´¥", e)
            error_details = {
                "error": str(e),
                "type": "âŒ ä¸“ä¸šä¸­æ–‡ç½‘é¡µæœç´¢é”™è¯¯",
                "debug_info": (
                    traceback.format_exc() if self.valves.DEBUG_MODE else None
                ),
            }
            await emit_status(
                status="error", description=f"âŒ æœç´¢å‡ºé”™: {str(e)}", done=True
            )
            return json.dumps(error_details, ensure_ascii=False, indent=2)

    # ======================== ä¸“ä¸šè‹±æ–‡æœç´¢ ========================
    async def search_english_web(
        self,
        query: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ ä¸“ä¸šè‹±æ–‡ç½‘é¡µæœç´¢å·¥å…·"""

        def next_run_id(tool: str) -> str:
            self.run_seq += 1
            return f"{tool}-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{self.run_seq}"

        def take_text(text: str, max_chars: int) -> str:
            if text is None:
                return ""
            return text if (max_chars is None or max_chars <= 0) else text[:max_chars]

        def split_text_chunks(text: str, size: int) -> List[str]:
            if text is None:
                return [""]
            if size is None or size <= 0:
                return [text]
            return [text[i : i + size] for i in range(0, len(text), size)]

        async def emit_citation_data(r: Dict, __event_emitter__, run_id: str, idx: int):
            if not (__event_emitter__ and self.valves.CITATION_LINKS):
                return

            full_doc = r.get("content") or ""
            doc_for_emit = take_text(full_doc, self.valves.CITATION_DOC_MAX_CHARS)
            chunks = split_text_chunks(doc_for_emit, self.valves.CITATION_CHUNK_SIZE)

            base_title = (r.get("title") or "") or (r.get("url") or "Source")
            base_url = (r.get("url") or "").strip()

            for ci, chunk in enumerate(chunks, 1):
                if self.valves.UNIQUE_REFERENCE_NAMES:
                    src_name = f"{base_title} | {base_url} | {run_id}#{idx}-{ci}-{uuid4().hex[:6]}"
                else:
                    src_name = base_url or base_title

                payload = {
                    "type": "citation",
                    "data": {
                        "document": [chunk],
                        "metadata": [
                            {
                                "title": base_title,
                                "date_accessed": datetime.now().isoformat(),
                            }
                        ],
                        "source": {
                            "name": src_name,
                            "url": base_url or "",
                            "type": r.get("source_type", "webpage"),
                        },
                    },
                }
                await __event_emitter__(payload)

                if self.valves.PERSIST_CITATIONS:
                    self.citations_history.append(payload)
                    if len(self.citations_history) > self.valves.PERSIST_CITATIONS_MAX:
                        self.citations_history = self.citations_history[
                            -self.valves.PERSIST_CITATIONS_MAX :
                        ]

        async def get_text_embedding(text: str) -> Optional[List[float]]:
            if not self.valves.ENABLE_RAG_ENHANCEMENT or not self.valves.ARK_API_KEY:
                return None

            text_hash = hashlib.sha256(text.encode("utf-8")).hexdigest()
            if text_hash in self.embedding_cache:
                return self.embedding_cache[text_hash]

            try:
                headers = {
                    "Authorization": f"Bearer {self.valves.ARK_API_KEY}",
                    "Content-Type": "application/json",
                }

                clean_text = text.strip()[:4000]
                if not clean_text:
                    return None

                payload = {
                    "model": self.valves.EMBEDDING_MODEL,
                    "input": [clean_text],
                    "encoding_format": "float",
                }

                response = requests.post(
                    f"{self.valves.EMBEDDING_BASE_URL}/embeddings",
                    headers=headers,
                    json=payload,
                    timeout=30,
                )
                response.raise_for_status()
                data = response.json()

                if "data" in data and len(data["data"]) > 0:
                    embedding = data["data"][0]["embedding"]
                    self.embedding_cache[text_hash] = embedding
                    return embedding
                else:
                    return None
            except Exception:
                return None

        def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:
            try:
                v1 = np.array(vec1)
                v2 = np.array(vec2)
                return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))
            except Exception:
                return 0.0

        run_id = next_run_id("en-web")
        if self.valves.PERSIST_CITATIONS and __event_emitter__:
            for old in self.citations_history:
                await __event_emitter__(old)

        def debug_log(message: str, error=None):
            if self.valves.DEBUG_MODE:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"[DEBUG {timestamp}] {message}")
                if error:
                    print(f"[DEBUG ERROR] {str(error)}")
                    print(f"[DEBUG TRACEBACK] {traceback.format_exc()}")

        async def emit_status(
            description: str, status: str = "in_progress", done: bool = False
        ):
            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "status": status,
                            "description": description,
                            "done": done,
                            "action": f"en_search:{run_id}",
                        },
                    }
                )

        async def enhance_results_with_rag(results: List[Dict]) -> List[Dict]:
            if not self.valves.ENABLE_RAG_ENHANCEMENT or not results:
                return results

            try:
                await emit_status(f"ğŸ§  RAGä¼˜åŒ– ({len(results)} ä¸ªç»“æœ)")
                query_embedding = await get_text_embedding(query)
                if not query_embedding:
                    return results

                enhanced_results = []
                for i, result in enumerate(results):
                    content = result.get("content", "")
                    if not content:
                        continue

                    content_embedding = await get_text_embedding(content)
                    if content_embedding:
                        similarity = calculate_similarity(
                            query_embedding, content_embedding
                        )
                        result["rag_similarity"] = similarity
                        result["rag_enhanced"] = True
                        enhanced_results.append(result)
                    else:
                        result["rag_similarity"] = 0.0
                        result["rag_enhanced"] = False
                        enhanced_results.append(result)

                enhanced_results.sort(
                    key=lambda x: x.get("rag_similarity", 0), reverse=True
                )
                await emit_status(f"âœ… RAGä¼˜åŒ–å®Œæˆ")
                return enhanced_results

            except Exception as e:
                debug_log("RAGä¼˜åŒ–å¤±è´¥", e)
                return results

        async def rerank_results(results: List[Dict]) -> List[Dict]:
            if (
                not self.valves.ENABLE_SEMANTIC_RERANK
                or not results
                or not self.valves.BOCHA_API_KEY
            ):
                return results

            try:
                await emit_status(f"ğŸ¯ è¯­ä¹‰é‡æ’åº ({len(results)} ä¸ªç»“æœ)")
                documents = [
                    result.get("content", "")[:4000]
                    for result in results
                    if result.get("content")
                ]

                if not documents:
                    return results

                headers = {
                    "Authorization": f"Bearer {self.valves.BOCHA_API_KEY}",
                    "Content-Type": "application/json",
                }

                payload = {
                    "model": self.valves.RERANK_MODEL,
                    "query": query,
                    "documents": documents,
                    "top_n": min(self.valves.RERANK_TOP_N, len(documents)),
                    "return_documents": False,
                }

                response = requests.post(
                    self.valves.RERANK_ENDPOINT,
                    headers=headers,
                    json=payload,
                    timeout=60,
                )
                response.raise_for_status()
                data = response.json()

                if "data" in data and "results" in data["data"]:
                    rerank_results_data = data["data"]["results"]
                    reranked_results = []

                    for rerank_item in rerank_results_data:
                        index = rerank_item.get("index", 0)
                        relevance_score = rerank_item.get("relevance_score", 0.0)
                        if index < len(results):
                            result = results[index].copy()
                            result["rerank_score"] = relevance_score
                            result["rerank_enhanced"] = True
                            reranked_results.append(result)

                    await emit_status(f"âœ… è¯­ä¹‰é‡æ’åºå®Œæˆ")
                    return reranked_results
                else:
                    return results

            except Exception as e:
                debug_log("è¯­ä¹‰é‡æ’åºå¤±è´¥", e)
                return results

        try:
            await emit_status(f"ğŸ” è‹±æ–‡ç½‘é¡µæœç´¢: {query}")

            headers = {
                "Authorization": f"Bearer {self.valves.LANGSEARCH_API_KEY}",
                "Content-Type": "application/json",
            }

            payload = {
                "query": query,
                "freshness": self.valves.FRESHNESS,
                "summary": True,
                "count": self.valves.ENGLISH_SEARCH_COUNT,
            }

            await emit_status("â³ è¿æ¥è‹±æ–‡æœç´¢æœåŠ¡å™¨...")
            resp = requests.post(
                self.valves.ENGLISH_WEB_SEARCH_ENDPOINT,
                headers=headers,
                json=payload,
                timeout=120,
            )
            resp.raise_for_status()
            data = resp.json()

            source_context_list = []
            len_raw = 0

            if "data" in data and "webPages" in data["data"]:
                web_pages = data["data"]["webPages"]
                if "value" in web_pages and isinstance(web_pages["value"], list):
                    len_raw = len(web_pages["value"])
                    await emit_status(f"ğŸ“„ å¤„ç† {len_raw} ä¸ªè‹±æ–‡ç»“æœ...")

                    for i, item in enumerate(web_pages["value"]):
                        content = item.get("summary", "") or item.get("snippet", "")
                        if not content:
                            continue

                        result_item = {
                            "content": content,
                            "title": item.get("name", ""),
                            "url": item.get("url", ""),
                            "site_name": item.get("siteName", ""),
                            "date_published": item.get("datePublished", ""),
                            "source_type": "ä¸“ä¸šè‹±æ–‡ç½‘é¡µ",
                        }
                        source_context_list.append(result_item)

            if self.valves.ENABLE_RAG_ENHANCEMENT:
                source_context_list = await enhance_results_with_rag(
                    source_context_list
                )

            if self.valves.ENABLE_SEMANTIC_RERANK:
                source_context_list = await rerank_results(source_context_list)

            if self.valves.ENABLE_RAG_ENHANCEMENT and self.valves.EMIT_ONLY_RAG_PASS:
                _thr = float(self.valves.SIMILARITY_THRESHOLD)
                source_context_list = [
                    r
                    for r in source_context_list
                    if float(r.get("rag_similarity") or 0.0) >= _thr
                ]

            for idx, r in enumerate(source_context_list):
                await emit_citation_data(r, __event_emitter__, run_id, idx)

            await emit_status(
                status="complete",
                description=f"ğŸ‰ è‹±æ–‡ç½‘é¡µæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(source_context_list)} ä¸ªç»“æœ",
                done=True,
            )

            results_data = []
            for r in source_context_list:
                result_item = {
                    "title": (r.get("title") or ""),
                    "url": r.get("url"),
                    "rag_similarity": float(r.get("rag_similarity") or 0.0),
                    "rerank_score": float(r.get("rerank_score") or 0.0),
                    "snippet": take_text(r.get("content", ""), 300),
                }
                if self.valves.RETURN_CONTENT_IN_RESULTS:
                    result_item["content"] = take_text(
                        r.get("content", ""), self.valves.RETURN_CONTENT_MAX_CHARS
                    )
                results_data.append(result_item)

            return json.dumps(
                {
                    "summary": {
                        "rag_threshold": self.valves.SIMILARITY_THRESHOLD,
                        "kept": len(source_context_list),
                        "total": len_raw,
                        "search_type": "ğŸŒ ä¸“ä¸šè‹±æ–‡ç½‘é¡µ",
                    },
                    "results": results_data,
                },
                ensure_ascii=False,
                indent=2,
            )

        except Exception as e:
            debug_log("è‹±æ–‡ç½‘é¡µæœç´¢å¤±è´¥", e)
            error_details = {
                "error": str(e),
                "type": "âŒ è‹±æ–‡ç½‘é¡µæœç´¢é”™è¯¯",
                "debug_info": (
                    traceback.format_exc() if self.valves.DEBUG_MODE else None
                ),
            }
            await emit_status(
                status="error", description=f"âŒ æœç´¢å‡ºé”™: {str(e)}", done=True
            )
            return json.dumps(error_details, ensure_ascii=False, indent=2)

    # ======================== æ™ºèƒ½ç½‘é¡µè¯»å–åŠŸèƒ½ï¼ˆå®Œæ•´ä¿®å¤ç‰ˆï¼‰ ========================
    async def web_scrape(
        self,
        urls: List[str],
        user_request: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ æ™ºèƒ½ç½‘é¡µè¯»å–å·¥å…· (å®Œæ•´ä¿®å¤ç‰ˆ)"""

        # === å†…åµŒè¯­ä¹‰å®‰å…¨åˆ†ç‰‡å·¥å…·å‡½æ•° ===
        def _protect_blocks_and_links(text: str):
            """ä¿æŠ¤ä»£ç å—ã€è¡¨æ ¼ã€é“¾æ¥"""
            holders = {"code": {}, "tables": {}, "md": {}, "url": {}}

            if self.valves.PRESERVE_CODEBLOCKS:
                code_pat = re.compile(r"```.*?```", re.S)

                def _code_sub(m):
                    key = f"âŸ¦CODE{len(holders['code'])}âŸ§"
                    holders["code"][key] = m.group(0)
                    return key

                text = code_pat.sub(_code_sub, text)

            if self.valves.PRESERVE_TABLES:
                lines = text.splitlines()
                out, i = [], 0
                while i < len(lines):
                    line = lines[i]
                    if line.strip().startswith("|") and "|" in line:
                        start = i
                        i += 1
                        while i < len(lines) and (
                            lines[i].strip().startswith("|")
                            or re.match(r"^\s*[:\-\|\s]+$", lines[i])
                        ):
                            i += 1
                        block = "\n".join(lines[start:i])
                        key = f"âŸ¦TBL{len(holders['tables'])}âŸ§"
                        holders["tables"][key] = block
                        out.append(key)
                    else:
                        out.append(line)
                        i += 1
                text = "\n".join(out)

            if self.valves.PRESERVE_LINKS:

                def _md_sub(m):
                    key = f"âŸ¦MD{len(holders['md'])}âŸ§"
                    holders["md"][key] = m.group(0)
                    return key

                text = re.sub(r"\[[^\]]+\]\([^)]+\)", _md_sub, text)

                def _url_sub(m):
                    key = f"âŸ¦URL{len(holders['url'])}âŸ§"
                    holders["url"][key] = m.group(0)
                    return key

                text = re.sub(r"https?://[^\s\)\]]+", _url_sub, text)

            return text, holders

        def _restore_placeholders(text: str, holders: dict) -> str:
            """æ¢å¤å ä½ç¬¦"""
            for k, v in holders.get("url", {}).items():
                text = text.replace(k, v)
            for k, v in holders.get("md", {}).items():
                text = text.replace(k, v)
            for k, v in holders.get("tables", {}).items():
                text = text.replace(k, v)
            for k, v in holders.get("code", {}).items():
                text = text.replace(k, v)
            return text

        def _split_sentences_zh_en(text: str) -> List[str]:
            """ä¸­è‹±æ··åˆå¥å­åˆ‡åˆ†"""
            text = re.sub(r"\n{3,}", "\n\n", text)
            text = re.sub(r"[ \t]{2,}", " ", text)
            text = text.replace("\n\n", "âŸ¦PARAâŸ§")
            text = re.sub(r"([ã€‚ï¼ï¼Ÿï¼›â€¦])", r"\1âŸ¦SPLITâŸ§", text)
            text = re.sub(r"([.!?;])(\s+)(?=[A-Z0-9\"'])", r"\1âŸ¦SPLITâŸ§", text)
            text = text.replace("âŸ¦PARAâŸ§", "âŸ¦SPLITâŸ§")

            parts = [p.strip() for p in text.split("âŸ¦SPLITâŸ§") if p.strip()]

            if self.valves.DENOISE_LINK_SECTIONS:
                cleaned = []
                for s in parts:
                    tokens = s.split()
                    link_like = sum(
                        1
                        for t in tokens
                        if t.startswith("http")
                        or t.startswith("âŸ¦URL")
                        or t.startswith("âŸ¦MD")
                    )
                    if len(tokens) <= 4 and link_like >= max(2, int(len(tokens) * 0.8)):
                        continue
                    cleaned.append(s)
                parts = cleaned

            return parts

        def _pack_sentences_to_chunks(sentences: List[str]) -> List[dict]:
            """å°†å¥å­æ‰“åŒ…æˆåˆ†ç‰‡ï¼ˆä¿®å¤é‡å é€»è¾‘ï¼‰"""
            tgt = max(800, int(self.valves.TARGET_CHUNK_CHARS))
            hard = max(tgt, int(self.valves.MAX_CHUNK_CHARS))
            ovl = max(0, int(self.valves.OVERLAP_SENTENCES))

            chunks = []
            i = 0

            while i < len(sentences) and len(chunks) < int(
                self.valves.MAX_TOTAL_CHUNKS
            ):
                buf, size, start = [], 0, i

                while i < len(sentences):
                    s = sentences[i]
                    s_len = len(s) + 1

                    if size + s_len > tgt:
                        if not buf:
                            s_cut = s[:hard]
                            buf.append(s_cut)
                            sentences[i] = s[hard:]
                        break

                    buf.append(s)
                    size += s_len
                    i += 1

                if buf:
                    text = " ".join(buf).strip()
                    end = start + len(buf) - 1
                    chunks.append({"text": text, "start_sent": start, "end_sent": end})

                    # ä¿®å¤åˆ†ç‰‡é‡å é€»è¾‘ï¼šæ­£ç¡®çš„å›é€€
                    if i < len(sentences):  # åªæœ‰åœ¨è¿˜æœ‰å‰©ä½™å¥å­æ—¶æ‰å›é€€
                        i = max(end - ovl + 1, start + 1)  # ç¡®ä¿è‡³å°‘å‰è¿›ä¸€ä¸ªå¥å­
                else:
                    i += 1

            return chunks

        def smart_segment_text(raw_text: str) -> List[dict]:
            """è¯­ä¹‰å®‰å…¨åˆ†ç‰‡å…¥å£"""
            protected, holders = _protect_blocks_and_links(raw_text)
            sentences = _split_sentences_zh_en(protected)
            chunks = _pack_sentences_to_chunks(sentences)

            for c in chunks:
                c["text"] = _restore_placeholders(c["text"], holders)

            return chunks

        # === å…¶ä»–å·¥å…·å‡½æ•° ===
        def get_segmenter_client():
            api_key = self.valves.SEGMENTER_API_KEY or self.valves.MOONSHOT_API_KEY
            base_url = self.valves.SEGMENTER_BASE_URL or self.valves.MOONSHOT_BASE_URL
            if not api_key:
                raise ValueError("LLMéœ€è¦APIå¯†é’¥")
            if (
                self.segmenter_client is None
                or self.segmenter_client.api_key != api_key
                or getattr(self.segmenter_client, "base_url_stored", None) != base_url
            ):
                self.segmenter_client = OpenAI(base_url=base_url, api_key=api_key)
                self.segmenter_client.base_url_stored = base_url
            return self.segmenter_client

        async def llm_call(
            messages: list, temperature: float = None, max_tokens: int = 4000
        ) -> str:
            """è°ƒç”¨LLMï¼ˆä¿®å¤ç‰ˆï¼šé‡è¯•+çº¿ç¨‹æ± ï¼‰"""
            client = get_segmenter_client()
            temp = (
                temperature
                if temperature is not None
                else self.valves.SUMMARY_TEMPERATURE
            )

            last_err = None
            for attempt in range(self.valves.LLM_RETRIES + 1):
                try:
                    resp = await asyncio.to_thread(
                        client.chat.completions.create,
                        model=self.valves.SEGMENTER_MODEL,
                        messages=messages,
                        temperature=temp,
                        max_tokens=max_tokens,
                    )
                    return resp.choices[0].message.content
                except Exception as e:
                    last_err = e
                    if attempt < self.valves.LLM_RETRIES:
                        await asyncio.sleep(
                            self.valves.LLM_BACKOFF_BASE_SEC * (attempt + 1)
                        )

            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {last_err}")

        def is_wikipedia(u: str) -> bool:
            try:
                return "wikipedia.org" in (urlparse(u).netloc or "").lower()
            except Exception:
                return False

        def next_run_id(tool: str) -> str:
            self.run_seq += 1
            return f"{tool}-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{self.run_seq}"

        def take_text(text: str, max_chars: int) -> str:
            if text is None:
                return ""
            return text if (max_chars is None or max_chars <= 0) else text[:max_chars]

        def split_text_chunks(text: str, size: int) -> List[str]:
            if text is None:
                return [""]
            if size is None or size <= 0:
                return [text]
            return [text[i : i + size] for i in range(0, len(text), size)]

        async def emit_citation_data(r: Dict, __event_emitter__, run_id: str, idx: int):
            if not (__event_emitter__ and self.valves.CITATION_LINKS):
                return

            full_doc = r.get("content") or ""
            doc_for_emit = take_text(full_doc, self.valves.CITATION_DOC_MAX_CHARS)
            chunks = split_text_chunks(doc_for_emit, self.valves.CITATION_CHUNK_SIZE)

            base_title = (r.get("title") or "") or (r.get("url") or "Source")
            base_url = (r.get("url") or "").strip()

            for ci, chunk in enumerate(chunks, 1):
                if self.valves.UNIQUE_REFERENCE_NAMES:
                    src_name = f"{base_title} | {base_url} | {run_id}#{idx}-{ci}-{uuid4().hex[:6]}"
                else:
                    src_name = base_url or base_title

                payload = {
                    "type": "citation",
                    "data": {
                        "document": [chunk],
                        "metadata": [
                            {
                                "title": base_title,
                                "date_accessed": datetime.now().isoformat(),
                            }
                        ],
                        "source": {
                            "name": src_name,
                            "url": base_url or "",
                            "type": r.get("source_type", "webpage"),
                        },
                    },
                }
                await __event_emitter__(payload)

                if self.valves.PERSIST_CITATIONS:
                    self.citations_history.append(payload)
                    if len(self.citations_history) > self.valves.PERSIST_CITATIONS_MAX:
                        self.citations_history = self.citations_history[
                            -self.valves.PERSIST_CITATIONS_MAX :
                        ]

        # æ ¸å¿ƒåŠŸèƒ½ï¼šä¿®å¤ç‰ˆMap-Reduceæ™ºèƒ½æ‘˜è¦
        async def extract_targeted_summaries_enhanced(
            content: str,
            user_request: str,
            url: str,
            page_title: str,
            max_summaries: int = None,
        ) -> List[Dict]:
            """ä¿®å¤ç‰ˆï¼šå¹¶å‘map + å¥å£®JSONè§£æ"""
            if max_summaries is None:
                max_summaries = int(
                    self.valves.REDUCE_SUMMARY_LIMIT
                    or self.valves.SUMMARY_COUNT_PER_PAGE
                )

            def cleanup(text: str) -> str:
                t = re.sub(r"\n{4,}", "\n\n", text)
                t = re.sub(r"[ \t]{3,}", " ", t)
                t = re.sub(r"\[\d+\]", "", t)
                return t.strip()

            cleaned = cleanup(content)
            if not cleaned:
                return []

            chunks = smart_segment_text(cleaned)
            if not chunks:
                return []

            if len(chunks) > int(self.valves.MAX_TOTAL_CHUNKS):
                chunks = chunks[: int(self.valves.MAX_TOTAL_CHUNKS)]

            debug_log(f"è¯­ä¹‰å®‰å…¨åˆ†ç‰‡å®Œæˆï¼š{len(chunks)} ç‰‡")

            # åŠ¨æ€å†³å®šæ¯ç‰‡æ¡æ•°
            target_total = max_summaries * 2
            per_chunk = max(3, min(10, int(round(target_total / max(1, len(chunks))))))

            sem = asyncio.Semaphore(self.valves.LLM_MAX_CONCURRENCY)
            map_summaries: List[Dict] = []

            def _extract_json_array(text: str) -> List[dict]:
                """å¥å£®çš„JSONæ•°ç»„æå–"""
                if not text:
                    return []

                t = text.strip()
                if t.startswith("```"):
                    t = re.sub(r"^```(?:json)?|```$", "", t, flags=re.I | re.M).strip()

                try:
                    obj = json.loads(t)
                    return obj if isinstance(obj, list) else []
                except Exception:
                    s, e = t.find("["), t.rfind("]")
                    if s != -1 and e != -1 and e > s:
                        try:
                            obj = json.loads(t[s : e + 1])
                            return obj if isinstance(obj, list) else []
                        except Exception:
                            return []
                    return []

            async def _map_one(idx: int, c: dict):
                """å•ä¸ªåˆ†ç‰‡çš„æ‘˜è¦æå–"""
                if self.valves.ENABLE_DETAILED_EXTRACTION:
                    detail_instruction = """
**è¯¦ç»†ä¿¡æ¯æå–è¦æ±‚ï¼š**
- æå–å…·ä½“çš„æ•°å­—ã€æ—¥æœŸã€äººåã€åœ°åã€ä¸“ä¸šæœ¯è¯­
- ä¿ç•™é‡è¦çš„å®šä¹‰ã€è§£é‡Šã€å…¬å¼ã€æ¦‚å¿µ
- åŒ…å«å†å²èƒŒæ™¯ã€å‘å±•è¿‡ç¨‹ã€å½±å“æ„ä¹‰
- æ•æ‰ä¸åŒè§‚ç‚¹ã€äº‰è®®ã€ç ”ç©¶ç°çŠ¶
- è®°å½•é‡è¦çš„å¼•ç”¨ã€å‚è€ƒèµ„æ–™ã€ç›¸å…³é“¾æ¥"""
                else:
                    detail_instruction = ""

                if self.valves.ENCOURAGE_COMPREHENSIVE:
                    comprehensive_instruction = f"""
**å…¨é¢è¦†ç›–ç­–ç•¥ï¼š**
- ä»å¤šä¸ªè§’åº¦åˆ†æå†…å®¹ï¼ˆæŠ€æœ¯ã€å†å²ã€æ–‡åŒ–ã€ç¤¾ä¼šç­‰ï¼‰
- ä¼˜å…ˆæå–èƒ½ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜çš„ä¿¡æ¯
- åŒ…å«èƒŒæ™¯çŸ¥è¯†å’Œç›¸å…³æ¦‚å¿µ
- æå–{per_chunk}æ¡ä¸åŒæ–¹é¢çš„æ‘˜è¦ï¼Œé¿å…é‡å¤
- ç¡®ä¿ä¿¡æ¯å®Œæ•´æ€§å’Œå‡†ç¡®æ€§"""
                else:
                    comprehensive_instruction = f"æå–{per_chunk}æ¡é«˜è´¨é‡æ‘˜è¦"

                sys_prompt = f"""ä½ æ˜¯ä¸“ä¸šä¿¡æ¯æå–ä¸“å®¶ã€‚è¯·åŸºäºç»™å®šç‰‡æ®µå†…å®¹ï¼Œå›´ç»•ç”¨æˆ·éœ€æ±‚è¿›è¡Œå…¨é¢æ·±åº¦çš„ä¿¡æ¯æå–ï¼Œè¾“å‡ºJSONæ•°ç»„æ ¼å¼ã€‚

{detail_instruction}

{comprehensive_instruction}

**è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š**
- æ¯æ¡æ‘˜è¦ {self.valves.SUMMARY_MIN_CHARS}â€“{self.valves.SUMMARY_MAX_CHARS} å­—ç¬¦
- å­—æ®µï¼šsummary(è¯¦ç»†æ‘˜è¦å…¨æ–‡)ã€position(ä½ç½®æè¿°)ã€key_points(5-8ä¸ªå…³é”®è¯)ã€covers_aspects(3-5ä¸ªæ¶µç›–æ–¹é¢)ã€relevance(0-1)ã€importance(0-1)ã€details(é‡è¦ç»†èŠ‚æ•°ç»„)
- è‹¥ç‰‡æ®µä¸ç›¸å…³ï¼Œè¿”å› []
- ç¦æ­¢ç¼–é€ ä¿¡æ¯ï¼Œä¸¥æ ¼åŸºäºåŸæ–‡
- ä¼˜å…ˆé€‰æ‹©ä¿¡æ¯å¯†åº¦é«˜çš„å†…å®¹

ä»…è¾“å‡ºJSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

                user_prompt = f"""ç”¨æˆ·éœ€æ±‚ï¼š{user_request}
é¡µé¢æ ‡é¢˜ï¼š{page_title}
åˆ†ç‰‡ {idx+1}/{len(chunks)} å†…å®¹ï¼š
{c['text']}

è¯·è¿›è¡Œå…¨é¢æ·±åº¦çš„ä¿¡æ¯æå–ï¼ˆ{per_chunk}æ¡ï¼‰ï¼š"""

                try:
                    async with sem:
                        resp = await llm_call(
                            [
                                {"role": "system", "content": sys_prompt},
                                {"role": "user", "content": user_prompt},
                            ],
                            temperature=self.valves.SUMMARY_TEMPERATURE,
                            max_tokens=4000,
                        )

                    arr = _extract_json_array(resp)
                    out = []
                    for item in arr:
                        if not isinstance(item, dict):
                            continue
                        s = (item.get("summary") or "").strip()
                        if len(s) < int(self.valves.SUMMARY_MIN_CHARS):
                            continue

                        out.append(
                            {
                                "content": s[: int(self.valves.SUMMARY_MAX_CHARS)],
                                "title": f"{page_title} Â· è¯¦ç»†æ‘˜è¦",
                                "url": url,
                                "relevance": float(item.get("relevance", 0.7)),
                                "importance": float(item.get("importance", 0.7)),
                                "position": item.get("position", f"åˆ†ç‰‡{idx+1}"),
                                "key_points": item.get("key_points", []),
                                "covers_aspects": item.get("covers_aspects", []),
                                "details": item.get("details", []),
                                "extract_method": "concurrent_map",
                                "source_type": "LLMæ™ºèƒ½æ‘˜è¦ï¼ˆå¹¶å‘mapï¼‰",
                                "chunk_index": idx,
                            }
                        )
                    return out
                except Exception as e:
                    debug_log(f"åˆ†ç‰‡ {idx+1} å¹¶å‘æ‘˜è¦å¤±è´¥ï¼š{e}")
                    return []

            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰åˆ†ç‰‡çš„æ‘˜è¦æå–
            tasks = [_map_one(idx, c) for idx, c in enumerate(chunks)]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for r in results:
                if isinstance(r, list):
                    map_summaries.extend(r)
                else:
                    debug_log("map å­ä»»åŠ¡å¼‚å¸¸", r)

            if not map_summaries:
                return []

            debug_log(f"å¹¶å‘mapæ‘˜è¦å®Œæˆï¼šå…± {len(map_summaries)} æ¡ï¼Œè¿›å…¥ reduce æ±‡æ€»")

            # reduceé˜¶æ®µ
            draft_items = []
            for m in map_summaries:
                aspects = ", ".join(m.get("covers_aspects", []))
                draft_items.append(
                    f"- [{m.get('position','')}] {aspects}: {m['content']}"
                )

            draft_text = "\n".join(draft_items)[:30000]

            reduce_sys = f"""ä½ æ˜¯é«˜çº§ä¿¡æ¯èšåˆå™¨ã€‚ç»™å®šè‹¥å¹²"åˆ†ç‰‡çº§è¯¦ç»†æ‘˜è¦"ï¼Œè¯·è¿›è¡Œæ™ºèƒ½æ•´åˆï¼š

**èšåˆç­–ç•¥ï¼š**
1) åˆå¹¶é«˜é‡å¤åº¦æ¡ç›®ï¼Œä¿ç•™ä¿¡æ¯æ›´å®Œæ•´/æ›´å…·ä½“çš„ä¸€æ¡
2) ä¼˜å…ˆè¦†ç›–"ç”¨æˆ·é—®é¢˜çš„å…³é”®æ–¹é¢"ï¼Œå…¶æ¬¡è¡¥å…¨èƒŒæ™¯/è„‰ç»œ
3) ä¼˜å…ˆä¿¡æ¯å¯†åº¦é«˜ã€åŒ…å«æ•°å­—/å®šä¹‰/å› æœ/å¯¹æ¯”çš„æ¡ç›®
4) é€‰å‡ºæœ€æœ‰ä»·å€¼çš„ {max_summaries} æ¡ï¼Œé¿å…ä¸»é¢˜é‡å¤
5) ä¿æŒåŸæ–‡äº‹å®ï¼Œä¸è¦ç¼–é€ ï¼›å¦‚ä¸ç¡®å®šåˆ™ä¸¢å¼ƒ

**è¾“å‡ºè¦æ±‚ï¼š**
- JSONæ•°ç»„æ ¼å¼ï¼Œå­—æ®µï¼šsummaryã€positionã€key_pointsã€covers_aspectsã€relevanceã€importanceã€details
- æ¯æ¡æ‘˜è¦ä¿æŒè¯¦ç»†å’Œå®Œæ•´
- æŒ‰é‡è¦æ€§å’Œç›¸å…³æ€§æ’åº
- ç¡®ä¿ä¸é—æ¼å…³é”®ä¿¡æ¯

ç¦æ­¢ç¼–é€ ï¼Œå¿…é¡»åŸºäºæ‰€ç»™æ‘˜è¦ã€‚ä»…è¾“å‡ºJSONæ•°ç»„ã€‚"""

            try:
                reduce_resp = await llm_call(
                    [
                        {"role": "system", "content": reduce_sys},
                        {
                            "role": "user",
                            "content": f"ç”¨æˆ·éœ€æ±‚ï¼š{user_request}\nåˆ†ç‰‡æ‘˜è¦åˆ—è¡¨ï¼š\n{draft_text}",
                        },
                    ],
                    temperature=0.05,
                    max_tokens=5000,
                )

                reduced = _extract_json_array(reduce_resp)
                if reduced:
                    final = []
                    for it in reduced[:max_summaries]:
                        s = (it.get("summary") or "").strip()
                        if not s:
                            continue

                        final.append(
                            {
                                "content": s[: int(self.valves.SUMMARY_MAX_CHARS)],
                                "title": f"{page_title} Â· ç»¼åˆæ‘˜è¦",
                                "url": url,
                                "relevance": float(it.get("relevance", 0.8)),
                                "importance": float(it.get("importance", 0.8)),
                                "position": it.get("position", ""),
                                "key_points": it.get("key_points", []),
                                "covers_aspects": it.get("covers_aspects", []),
                                "details": it.get("details", []),
                                "extract_method": "concurrent_reduce",
                                "source_type": "LLMæ™ºèƒ½æ‘˜è¦ï¼ˆå¹¶å‘reduceï¼‰",
                            }
                        )

                    if final:
                        debug_log(f"å¹¶å‘reduceæ±‡æ€»æˆåŠŸï¼Œæœ€ç»ˆ {len(final)} æ¡æ‘˜è¦")
                        return final
            except Exception as e:
                debug_log(f"å¹¶å‘reduceæ±‡æ€»å¤±è´¥ï¼Œå›é€€ map ç›´å‡ºï¼š{e}")

            # å›é€€ç­–ç•¥
            map_summaries.sort(
                key=lambda x: (x.get("relevance", 0) + x.get("importance", 0)) / 2,
                reverse=True,
            )
            debug_log(f"å›é€€åˆ°å¹¶å‘mapæ‘˜è¦å‰ {max_summaries} æ¡")
            return map_summaries[:max_summaries]

        # RAGå‡½æ•°
        async def batch_embeddings(texts: List[str]) -> List[Optional[List[float]]]:
            if not texts:
                return []
            if not (self.valves.ENABLE_RAG_ENHANCEMENT and self.valves.ARK_API_KEY):
                return [None for _ in texts]

            headers = {
                "Authorization": f"Bearer {self.valves.ARK_API_KEY}",
                "Content-Type": "application/json",
            }

            try:
                payload = {
                    "model": self.valves.EMBEDDING_MODEL,
                    "input": [t[:4000] for t in texts],
                    "encoding_format": "float",
                }

                resp = await asyncio.to_thread(
                    requests.post,
                    f"{self.valves.EMBEDDING_BASE_URL}/embeddings",
                    headers=headers,
                    json=payload,
                    timeout=60,
                )
                resp.raise_for_status()
                data = resp.json()

                vecs = [d["embedding"] for d in data.get("data", [])]
                if len(vecs) != len(texts):
                    fallback = []
                    for t in texts:
                        v = await get_single_embedding(t)
                        fallback.append(v)
                    return fallback

                return vecs
            except Exception:
                out = []
                for t in texts:
                    v = await get_single_embedding(t)
                    out.append(v)
                return out

        async def get_single_embedding(text: str) -> Optional[List[float]]:
            if not self.valves.ENABLE_RAG_ENHANCEMENT or not self.valves.ARK_API_KEY:
                return None

            text_hash = hashlib.sha256(text.encode("utf-8")).hexdigest()
            if text_hash in self.embedding_cache:
                return self.embedding_cache[text_hash]

            try:
                headers = {
                    "Authorization": f"Bearer {self.valves.ARK_API_KEY}",
                    "Content-Type": "application/json",
                }

                clean_text = text.strip()[:4000]
                if not clean_text:
                    return None

                payload = {
                    "model": self.valves.EMBEDDING_MODEL,
                    "input": [clean_text],
                    "encoding_format": "float",
                }

                response = await asyncio.to_thread(
                    requests.post,
                    f"{self.valves.EMBEDDING_BASE_URL}/embeddings",
                    headers=headers,
                    json=payload,
                    timeout=30,
                )
                response.raise_for_status()
                data = response.json()

                if "data" in data and len(data["data"]) > 0:
                    embedding = data["data"][0]["embedding"]
                    self.embedding_cache[text_hash] = embedding
                    return embedding
                else:
                    return None
            except Exception:
                return None

        def cos_similarity(a: List[float], b: List[float]) -> float:
            try:
                va = np.array(a)
                vb = np.array(b)
                return float(np.dot(va, vb) / (np.linalg.norm(va) * np.linalg.norm(vb)))
            except Exception:
                return 0.0

        async def internal_answerability_evaluation(
            content: str, user_request: str
        ) -> float:
            """å¯å›ç­”æ€§è¯„ä¼°"""
            if not self.valves.ENABLE_ANSWERABILITY:
                return 0.6

            try:
                content = content[:3000] if len(content) > 3000 else content
                system_prompt = """ä½ æ˜¯æ–‡æœ¬ç›¸å…³æ€§è¯„ä¼°ä¸“å®¶ã€‚è¯„åˆ†æ ‡å‡†ï¼ˆå®½æ¾æ¨¡å¼ï¼‰ï¼š
- 1.0: å®Œç¾å›ç­”é—®é¢˜
- 0.8: å¤§éƒ¨åˆ†å›ç­”é—®é¢˜  
- 0.6: æœ‰ä¸€å®šç›¸å…³æ€§
- 0.4: é—´æ¥ç›¸å…³
- 0.2: ç›¸å…³æ€§è¾ƒä½
- 0.0: å®Œå…¨æ— å…³

ä»…è¿”å›æ•°å­—ã€‚"""

                user_prompt = f"""ç”¨æˆ·é—®é¢˜: {user_request}
æ–‡æœ¬å†…å®¹: {content}

å¯å›ç­”æ€§åˆ†æ•°:"""

                response = await llm_call(
                    [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt},
                    ],
                    temperature=0.05,
                )

                number_match = re.search(r"0\.\d+|1\.0|0|1", response.strip())
                if number_match:
                    score = float(number_match.group())
                    return max(0.0, min(1.0, score))
                else:
                    return 0.6
            except Exception as e:
                return 0.6

        run_id = next_run_id("web-scrape")
        if self.valves.PERSIST_CITATIONS and __event_emitter__:
            for old in self.citations_history:
                await __event_emitter__(old)

        def debug_log(message: str, error=None):
            if self.valves.DEBUG_MODE:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"[DEBUG {timestamp}] {message}")
                if error:
                    print(f"[DEBUG ERROR] {str(error)}")
                    print(f"[DEBUG TRACEBACK] {traceback.format_exc()}")

        async def emit_status(
            description: str, done: bool, action: str, urls_list: List[str]
        ):
            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "done": done,
                            "action": f"{action}:{run_id}",
                            "description": description,
                            "urls": urls_list,
                        },
                    }
                )

        try:
            debug_log(f"å¼€å§‹æ™ºèƒ½ç½‘é¡µè¯»å–ï¼ŒURLæ•°é‡: {len(urls)}")
            await emit_status(
                f"ğŸ§  æ­£åœ¨è¿›è¡Œå¹¶å‘æ™ºèƒ½æ‘˜è¦æå– - {len(urls)} ä¸ªç½‘é¡µ",
                False,
                "web_search",
                urls,
            )

            async def process_url(url):
                jina_url = f"https://r.jina.ai/{url}"
                headers = {
                    "X-No-Cache": "true",
                    "X-With-Links-Summary": "true",
                    "Authorization": f"Bearer {self.valves.JINA_API_KEY}",
                }

                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.get(
                            jina_url, headers=headers, timeout=90
                        ) as response:
                            response.raise_for_status()
                            content = await response.text()

                    if not content or content.strip() == "":
                        return {
                            "content": "",
                            "title": url,
                            "url": url,
                            "error": "è¿”å›å†…å®¹ä¸ºç©º",
                            "status": "empty",
                        }

                    debug_log(f"æˆåŠŸè¯»å–URL {url}ï¼Œå†…å®¹é•¿åº¦: {len(content)}")

                    return {
                        "content": content,
                        "title": f"ç½‘é¡µå†…å®¹ - {url.split('/')[2] if '/' in url else url}",
                        "url": url,
                        "site_name": url.split("/")[2] if "/" in url else url,
                        "date_published": datetime.now().strftime("%Y-%m-%d"),
                        "source_type": "ç½‘é¡µè¯»å–",
                        "status": "success",
                    }
                except Exception as e:
                    error_message = f"è¯»å–ç½‘é¡µ {url} æ—¶å‡ºé”™: {str(e)}"
                    debug_log(f"å¤„ç†URLå¤±è´¥: {url}", e)
                    return {
                        "content": "",
                        "title": url,
                        "url": url,
                        "error": error_message,
                        "status": "error",
                    }

            tasks = [process_url(url) for url in urls]
            results = await asyncio.gather(*tasks)

            successful_results = []
            error_results = []

            for result in results:
                if result.get("status") == "success" and result.get("content"):
                    successful_results.append(result)
                else:
                    error_results.append(result)

            debug_log(
                f"å¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {len(successful_results)}, å¤±è´¥: {len(error_results)}"
            )

            if not successful_results:
                return json.dumps(
                    {
                        "request": user_request,
                        "error": "æ‰€æœ‰ç½‘é¡µè¯»å–éƒ½å¤±è´¥",
                        "stats": {"kept": 0, "failed": len(urls)},
                        "summaries": [],
                        "errors": error_results,
                    },
                    ensure_ascii=False,
                    indent=2,
                )

            # æ™ºèƒ½æ‘˜è¦æå–æµç¨‹
            if self.valves.ENABLE_SMART_SUMMARY:
                await emit_status(
                    f"ğŸ§  å¯¹ {len(successful_results)} ä¸ªé¡µé¢è¿›è¡Œå¹¶å‘æ‘˜è¦æå–",
                    False,
                    "smart_summary",
                    urls,
                )

                all_summaries = []
                for i, page in enumerate(successful_results):
                    content = page.get("content", "")
                    url = page.get("url", "")
                    title = page.get("title", "")

                    debug_log(f"ä¸ºé¡µé¢ {i+1}/{len(successful_results)} æå–æ‘˜è¦: {url}")

                    try:
                        summaries = await extract_targeted_summaries_enhanced(
                            content=content,
                            user_request=user_request,
                            url=url,
                            page_title=title,
                            max_summaries=int(self.valves.REDUCE_SUMMARY_LIMIT),
                        )

                        debug_log(f"é¡µé¢ {url} æå–åˆ° {len(summaries)} æ¡æ‘˜è¦")
                        all_summaries.extend(summaries)

                        await emit_status(
                            f"ğŸ“„ å·²å¤„ç† {i+1}/{len(successful_results)} ä¸ªé¡µé¢ï¼Œç´¯è®¡ {len(all_summaries)} æ¡æ‘˜è¦",
                            False,
                            "smart_summary",
                            urls,
                        )

                    except Exception as e:
                        debug_log(f"é¡µé¢ {url} æ‘˜è¦æå–å¤±è´¥: {e}")
                        # å›é€€å¤„ç†
                        content_chunks = [
                            content[i : i + self.valves.SUMMARY_MAX_CHARS]
                            for i in range(
                                0, len(content), self.valves.SUMMARY_MAX_CHARS
                            )[:3]
                        ]

                        for j, chunk in enumerate(content_chunks):
                            basic_summary = {
                                "content": chunk,
                                "title": f"{title} Â· åŸºç¡€æ‘˜è¦{j+1}",
                                "url": url,
                                "relevance": 0.4,
                                "importance": 0.4,
                                "position": f"æ‘˜è¦æå–å¤±è´¥-ç‰‡æ®µ{j+1}",
                                "key_points": [],
                                "covers_aspects": [],
                                "details": [],
                                "extract_method": "basic_fallback_concurrent",
                                "source_type": "åŸºç¡€æ‘˜è¦(å›é€€)",
                            }
                            all_summaries.append(basic_summary)

                debug_log(f"å¹¶å‘æ™ºèƒ½æ‘˜è¦æå–å®Œæˆï¼Œæ€»è®¡ {len(all_summaries)} æ¡æ‘˜è¦")

                if not all_summaries:
                    # æœ€ç»ˆå›é€€
                    all_summaries = []
                    for page in successful_results:
                        content = page.get("content", "")
                        content_chunks = [
                            content[i : i + 2000]
                            for i in range(0, len(content), 1500)[:5]
                        ]

                        for j, chunk in enumerate(content_chunks):
                            all_summaries.append(
                                {
                                    "content": chunk,
                                    "title": f"{page.get('title', '')} Â· åŸå§‹ç‰‡æ®µ{j+1}",
                                    "url": page.get("url", ""),
                                    "relevance": 0.5,
                                    "importance": 0.5,
                                    "position": f"åŸå§‹å†…å®¹ç‰‡æ®µ{j+1}",
                                    "key_points": [],
                                    "covers_aspects": [],
                                    "details": [],
                                    "extract_method": "multi_segment_fallback",
                                    "source_type": "åŸå§‹å†…å®¹ï¼ˆå¤šæ®µï¼‰",
                                }
                            )

                # RAGå¤„ç†
                if self.valves.ENABLE_RAG_ENHANCEMENT and all_summaries:
                    await emit_status(
                        f"ğŸ¯ RAGå‘é‡åŒ– {len(all_summaries)} æ¡æ‘˜è¦",
                        False,
                        "rag_enhancement",
                        urls,
                    )

                    summary_texts = [s["content"] for s in all_summaries]
                    query_vec = (await batch_embeddings([user_request]))[0]
                    summary_vecs = await batch_embeddings(summary_texts)

                    for i, summary in enumerate(all_summaries):
                        if (
                            i < len(summary_vecs)
                            and summary_vecs[i] is not None
                            and query_vec is not None
                        ):
                            similarity = cos_similarity(query_vec, summary_vecs[i])
                            summary["rag_similarity"] = similarity
                        else:
                            summary["rag_similarity"] = summary.get("relevance", 0.6)

                    all_summaries.sort(
                        key=lambda x: x.get("rag_similarity", 0), reverse=True
                    )

                # è¯­ä¹‰é‡æ’åº
                if (
                    self.valves.ENABLE_SEMANTIC_RERANK
                    and self.valves.BOCHA_API_KEY
                    and all_summaries
                ):
                    await emit_status(
                        f"ğŸ¯ è¯­ä¹‰é‡æ’åº {len(all_summaries)} æ¡æ‘˜è¦",
                        False,
                        "rerank",
                        urls,
                    )

                    try:
                        headers = {
                            "Authorization": f"Bearer {self.valves.BOCHA_API_KEY}",
                            "Content-Type": "application/json",
                        }

                        documents = [s["content"][:4000] for s in all_summaries]
                        payload = {
                            "model": self.valves.RERANK_MODEL,
                            "query": user_request,
                            "documents": documents,
                            "top_n": min(self.valves.RERANK_TOP_N, len(documents)),
                            "return_documents": False,
                        }

                        resp = await asyncio.to_thread(
                            requests.post,
                            self.valves.RERANK_ENDPOINT,
                            headers=headers,
                            json=payload,
                            timeout=60,
                        )
                        resp.raise_for_status()
                        data = resp.json()

                        if "data" in data and "results" in data["data"]:
                            rerank_results_data = data["data"]["results"]
                            reranked_summaries = []

                            for rerank_item in rerank_results_data:
                                index = rerank_item.get("index", 0)
                                relevance_score = rerank_item.get(
                                    "relevance_score", 0.0
                                )

                                if 0 <= index < len(all_summaries):
                                    summary = all_summaries[index].copy()
                                    summary["rerank_score"] = relevance_score
                                    reranked_summaries.append(summary)

                            all_summaries = reranked_summaries
                    except Exception as e:
                        debug_log(f"è¯­ä¹‰é‡æ’åºå¤±è´¥: {e}")

                # å¯å›ç­”æ€§è¯„åˆ†
                if self.valves.ENABLE_ANSWERABILITY and all_summaries:
                    await emit_status(f"ğŸ’¡ å¯å›ç­”æ€§è¯„åˆ†", False, "answerability", urls)

                    for i, summary in enumerate(all_summaries):
                        try:
                            answerability = await internal_answerability_evaluation(
                                summary["content"], user_request
                            )
                            summary["answerability"] = answerability
                        except Exception as e:
                            summary["answerability"] = 0.6

                # ç»¼åˆåˆ†æ•°è®¡ç®—
                for summary in all_summaries:
                    rag_similarity = float(summary.get("rag_similarity", 0.0))
                    rerank_score = float(summary.get("rerank_score", 0.0))
                    answerability = float(summary.get("answerability", 0.6))
                    llm_relevance = float(summary.get("relevance", 0.6))

                    final_score = (
                        self.valves.RAG_WEIGHT * rag_similarity
                        + self.valves.RERANK_WEIGHT * rerank_score
                        + self.valves.ANSWERABILITY_WEIGHT * answerability
                        + self.valves.SUMMARY_RELEVANCE_WEIGHT * llm_relevance
                    )
                    summary["final_score"] = final_score

                all_summaries.sort(key=lambda x: x.get("final_score", 0), reverse=True)

                # é˜ˆå€¼è¿‡æ»¤
                if (
                    self.valves.ENABLE_RAG_ENHANCEMENT
                    and self.valves.EMIT_ONLY_RAG_PASS
                ):
                    threshold = self.valves.SIMILARITY_THRESHOLD

                    if any(is_wikipedia(r["url"]) for r in successful_results):
                        threshold = max(0.03, threshold * 0.4)
                        debug_log(f"æ£€æµ‹åˆ°ç»´åŸºç™¾ç§‘ï¼Œæ”¾å®½é˜ˆå€¼åˆ°: {threshold}")

                    filtered_summaries = [
                        s
                        for s in all_summaries
                        if s.get("rag_similarity", 0) >= threshold
                        or s.get("final_score", 0) >= 0.3
                    ]

                    debug_log(
                        f"é˜ˆå€¼è¿‡æ»¤ï¼š{len(all_summaries)} -> {len(filtered_summaries)} æ¡æ‘˜è¦"
                    )
                    all_summaries = filtered_summaries

                final_summaries = all_summaries[: self.valves.RERANK_TOP_N]
                debug_log(f"æœ€ç»ˆä¿ç•™ {len(final_summaries)} æ¡æ‘˜è¦")

                # å‘é€å¼•ç”¨
                for idx, summary in enumerate(final_summaries):
                    await emit_citation_data(summary, __event_emitter__, run_id, idx)

                await emit_status(
                    f"ğŸ‰ æ™ºèƒ½æ‘˜è¦å®Œæˆï¼æå–äº† {len(final_summaries)} æ¡æ‘˜è¦",
                    True,
                    "web_search",
                    urls,
                )

                # æ„å»ºè¿”å›ä½“
                results_data = []
                for summary in final_summaries:
                    item = {
                        "title": summary.get("title") or "",
                        "url": summary.get("url"),
                        "rag_similarity": float(summary.get("rag_similarity", 0.0)),
                        "rerank_score": float(summary.get("rerank_score", 0.0)),
                        "answerability": float(summary.get("answerability", 0.6)),
                        "final_score": float(summary.get("final_score", 0.0)),
                        "key_points": summary.get("key_points", []),
                        "covers_aspects": summary.get("covers_aspects", []),
                        "details": summary.get("details", []),
                        "extract_method": summary.get("extract_method", ""),
                        "snippet": take_text(summary.get("content", ""), 300),
                    }
                    if self.valves.RETURN_CONTENT_IN_RESULTS:
                        item["content"] = take_text(
                            summary.get("content", ""),
                            self.valves.RETURN_CONTENT_MAX_CHARS,
                        )
                    results_data.append(item)

                return json.dumps(
                    {
                        "request": user_request,
                        "stats": {
                            "pages_fetched": len(successful_results),
                            "summaries_final": len(final_summaries),
                            "version": "concurrent_fixed_v3.8.2",
                            "chunking_strategy": "å¥å­/é“¾æ¥/è¡¨æ ¼æ„ŸçŸ¥ï¼ˆä¿®å¤é‡å ï¼‰",
                            "summarization_strategy": "å¹¶å‘Map-Reduce",
                        },
                        "summaries": results_data,
                        "errors": error_results,
                    },
                    ensure_ascii=False,
                    indent=2,
                )

            else:
                # æ™ºèƒ½æ‘˜è¦æœªå¯ç”¨
                for idx, r in enumerate(successful_results):
                    await emit_citation_data(r, __event_emitter__, run_id, idx)

                results_data = []
                for r in successful_results:
                    result_item = {
                        "title": (r.get("title") or ""),
                        "url": r.get("url"),
                        "snippet": take_text(r.get("content", ""), 300),
                    }
                    if self.valves.RETURN_CONTENT_IN_RESULTS:
                        result_item["content"] = take_text(
                            r.get("content", ""), self.valves.RETURN_CONTENT_MAX_CHARS
                        )
                    results_data.append(result_item)

                return json.dumps(
                    {
                        "request": user_request,
                        "stats": {
                            "kept": len(successful_results),
                            "failed": len(error_results),
                        },
                        "results": results_data,
                        "errors": error_results,
                    },
                    ensure_ascii=False,
                    indent=2,
                )

        except Exception as e:
            debug_log("æ™ºèƒ½ç½‘é¡µè¯»å–å¤±è´¥", e)
            return json.dumps(
                {
                    "request": user_request,
                    "error": str(e),
                    "stats": {"kept": 0, "failed": len(urls)},
                    "summaries": [],
                    "errors": [{"url": url, "error": "å¤„ç†å¤±è´¥"} for url in urls],
                },
                ensure_ascii=False,
                indent=2,
            )

    # ======================== Rawç½‘é¡µè¯»å–åŠŸèƒ½ ========================
    async def web_scrape_raw(
        self,
        urls: List[str],
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ Rawç½‘é¡µè¯»å–å·¥å…·"""

        def next_run_id(tool: str) -> str:
            self.run_seq += 1
            return f"{tool}-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{self.run_seq}"

        def take_text(text: str, max_chars: int) -> str:
            if text is None:
                return ""
            return text if (max_chars is None or max_chars <= 0) else text[:max_chars]

        def split_text_chunks(text: str, size: int) -> List[str]:
            if text is None:
                return [""]
            if size is None or size <= 0:
                return [text]
            return [text[i : i + size] for i in range(0, len(text), size)]

        async def emit_citation_data(r: Dict, __event_emitter__, run_id: str, idx: int):
            if not (__event_emitter__ and self.valves.CITATION_LINKS):
                return

            full_doc = r.get("content") or ""
            doc_for_emit = take_text(full_doc, self.valves.CITATION_DOC_MAX_CHARS)
            chunks = split_text_chunks(doc_for_emit, self.valves.CITATION_CHUNK_SIZE)

            base_title = (r.get("title") or "") or (r.get("url") or "Source")
            base_url = (r.get("url") or "").strip()

            for ci, chunk in enumerate(chunks, 1):
                if self.valves.UNIQUE_REFERENCE_NAMES:
                    src_name = f"{base_title} | {base_url} | {run_id}#{idx}-{ci}-{uuid4().hex[:6]}"
                else:
                    src_name = base_url or base_title

                payload = {
                    "type": "citation",
                    "data": {
                        "document": [chunk],
                        "metadata": [
                            {
                                "title": base_title,
                                "date_accessed": datetime.now().isoformat(),
                            }
                        ],
                        "source": {
                            "name": src_name,
                            "url": base_url or "",
                            "type": r.get("source_type", "webpage"),
                        },
                    },
                }
                await __event_emitter__(payload)

                if self.valves.PERSIST_CITATIONS:
                    self.citations_history.append(payload)
                    if len(self.citations_history) > self.valves.PERSIST_CITATIONS_MAX:
                        self.citations_history = self.citations_history[
                            -self.valves.PERSIST_CITATIONS_MAX :
                        ]

        run_id = next_run_id("web-scrape-raw")
        if self.valves.PERSIST_CITATIONS and __event_emitter__:
            for old in self.citations_history:
                await __event_emitter__(old)

        def debug_log(message: str, error=None):
            if self.valves.DEBUG_MODE:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"[DEBUG {timestamp}] {message}")
                if error:
                    print(f"[DEBUG ERROR] {str(error)}")
                    print(f"[DEBUG TRACEBACK] {traceback.format_exc()}")

        async def emit_status(
            description: str, done: bool, action: str, urls_list: List[str]
        ):
            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "done": done,
                            "action": f"{action}:{run_id}",
                            "description": description,
                            "urls": urls_list,
                        },
                    }
                )

        try:
            debug_log(f"å¼€å§‹Rawç½‘é¡µè¯»å–ï¼ŒURLæ•°é‡: {len(urls)}")
            await emit_status(
                f"ğŸŒ æ­£åœ¨Rawè¯»å– {len(urls)} ä¸ªç½‘é¡µ", False, "web_search", urls
            )

            async def process_url(url):
                jina_url = f"https://r.jina.ai/{url}"
                headers = {
                    "X-No-Cache": "true",
                    "X-With-Images-Summary": "true",
                    "X-With-Links-Summary": "true",
                    "Authorization": f"Bearer {self.valves.JINA_API_KEY}",
                }

                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.get(
                            jina_url, headers=headers, timeout=60
                        ) as response:
                            response.raise_for_status()
                            content = await response.text()

                    if not content or content.strip() == "":
                        return {
                            "content": "",
                            "title": url,
                            "url": url,
                            "error": "è¿”å›å†…å®¹ä¸ºç©º",
                            "status": "empty",
                        }

                    debug_log(f"æˆåŠŸè¯»å–URL {url}ï¼Œå†…å®¹é•¿åº¦: {len(content)}")

                    return {
                        "content": content,
                        "title": f"ç½‘é¡µå†…å®¹ - {url.split('/')[2] if '/' in url else url}",
                        "url": url,
                        "status": "success",
                    }
                except Exception as e:
                    error_message = f"è¯»å–ç½‘é¡µ {url} æ—¶å‡ºé”™: {str(e)}"
                    debug_log(f"å¤„ç†URLå¤±è´¥: {url}", e)
                    return {
                        "content": "",
                        "title": url,
                        "url": url,
                        "error": error_message,
                        "status": "error",
                    }

            tasks = [process_url(url) for url in urls]
            results = await asyncio.gather(*tasks)

            successful_results = []
            error_results = []

            for result in results:
                if result.get("status") == "success" and result.get("content"):
                    successful_results.append(result)
                else:
                    error_results.append(result)

            for idx, r in enumerate(successful_results):
                await emit_citation_data(r, __event_emitter__, run_id, idx)

            await emit_status(
                f"ğŸ‰ Rawè¯»å–å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(successful_results)} ä¸ª",
                True,
                "web_search",
                urls,
            )

            if self.valves.RAW_OUTPUT_FORMAT.lower() == "json":
                results_data = []
                for r in successful_results:
                    result_item = {
                        "title": (r.get("title") or ""),
                        "url": r.get("url"),
                        "content": take_text(
                            r.get("content", ""), self.valves.RETURN_CONTENT_MAX_CHARS
                        ),
                    }
                    results_data.append(result_item)

                return json.dumps(
                    {"results": results_data, "errors": error_results},
                    ensure_ascii=False,
                    indent=2,
                )
            else:
                final_results = []
                for result in successful_results:
                    content = take_text(
                        result.get("content", ""), self.valves.RETURN_CONTENT_MAX_CHARS
                    )
                    final_results.append(
                        f"""URL: {result['url']}
æ ‡é¢˜: {result.get('title', '')}
å†…å®¹: {content}
"""
                    )

                for result in error_results:
                    final_results.append(
                        f"""URL: {result['url']}
é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}
"""
                    )

                final_result = "\n".join(final_results)
                if not final_result.strip():
                    final_result = "æ‰€æœ‰ç½‘é¡µè¯»å–å‡å¤±è´¥ã€‚"

                result_text = f"""Rawç½‘é¡µè¯»å–ç»“æœ:
ğŸ“Š æ€»URLæ•°: {len(urls)}
âœ… æˆåŠŸè¯»å–: {len(successful_results)}
âŒ å¤±è´¥è¯»å–: {len(error_results)}

åŸå§‹ç½‘é¡µå†…å®¹:
{final_result}"""
                return result_text

        except Exception as e:
            debug_log("Rawç½‘é¡µè¯»å–å¤±è´¥", e)
            if self.valves.RAW_OUTPUT_FORMAT.lower() == "json":
                return json.dumps(
                    {
                        "error": str(e),
                        "results": [],
                        "errors": [{"url": url, "error": "å¤„ç†å¤±è´¥"} for url in urls],
                    },
                    ensure_ascii=False,
                    indent=2,
                )
            else:
                return f"""âŒ Rawç½‘é¡µè¯»å–å‡ºç°é”™è¯¯: {str(e)}
è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®ã€‚"""

    # ======================== AIæ™ºèƒ½æœç´¢ ========================
    async def search_ai_intelligent(
        self,
        query: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸ¤– é«˜çº§AIæ™ºèƒ½æœç´¢å·¥å…·"""

        def next_run_id(tool: str) -> str:
            self.run_seq += 1
            return f"{tool}-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{self.run_seq}"

        def take_text(text: str, max_chars: int) -> str:
            if text is None:
                return ""
            return text if (max_chars is None or max_chars <= 0) else text[:max_chars]

        def split_text_chunks(text: str, size: int) -> List[str]:
            if text is None:
                return [""]
            if size is None or size <= 0:
                return [text]
            return [text[i : i + size] for i in range(0, len(text), size)]

        async def emit_citation_data(r: Dict, __event_emitter__, run_id: str, idx: int):
            if not (__event_emitter__ and self.valves.CITATION_LINKS):
                return

            full_doc = r.get("content") or ""
            doc_for_emit = take_text(full_doc, self.valves.CITATION_DOC_MAX_CHARS)
            chunks = split_text_chunks(doc_for_emit, self.valves.CITATION_CHUNK_SIZE)

            base_title = (r.get("title") or "") or (r.get("url") or "Source")
            base_url = (r.get("url") or "").strip()

            for ci, chunk in enumerate(chunks, 1):
                if self.valves.UNIQUE_REFERENCE_NAMES:
                    src_name = f"{base_title} | {base_url} | {run_id}#{idx}-{ci}-{uuid4().hex[:6]}"
                else:
                    src_name = base_url or base_title

                payload = {
                    "type": "citation",
                    "data": {
                        "document": [chunk],
                        "metadata": [
                            {
                                "title": base_title,
                                "date_accessed": datetime.now().isoformat(),
                            }
                        ],
                        "source": {
                            "name": src_name,
                            "url": base_url or "",
                            "type": r.get("source_type", "webpage"),
                        },
                    },
                }
                await __event_emitter__(payload)

                if self.valves.PERSIST_CITATIONS:
                    self.citations_history.append(payload)
                    if len(self.citations_history) > self.valves.PERSIST_CITATIONS_MAX:
                        self.citations_history = self.citations_history[
                            -self.valves.PERSIST_CITATIONS_MAX :
                        ]

        run_id = next_run_id("ai-search")
        if self.valves.PERSIST_CITATIONS and __event_emitter__:
            for old in self.citations_history:
                await __event_emitter__(old)

        def debug_log(message: str, error=None):
            if self.valves.DEBUG_MODE:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"[DEBUG {timestamp}] {message}")
                if error:
                    print(f"[DEBUG ERROR] {str(error)}")
                    print(f"[DEBUG TRACEBACK] {traceback.format_exc()}")

        async def emit_status(
            description: str, status: str = "in_progress", done: bool = False
        ):
            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "status": status,
                            "description": description,
                            "done": done,
                            "action": f"ai_search:{run_id}",
                        },
                    }
                )

        try:
            debug_log(f"å¼€å§‹AIæ™ºèƒ½æœç´¢: {query}")
            await emit_status(f"ğŸ¤– æ­£åœ¨è¿›è¡Œé«˜çº§AIæ™ºèƒ½æœç´¢: {query}")

            headers = {
                "Authorization": f"Bearer {self.valves.BOCHA_API_KEY}",
                "Content-Type": "application/json",
            }

            payload = {
                "query": query,
                "freshness": self.valves.FRESHNESS,
                "answer": True,
                "stream": False,
                "count": self.valves.AI_SEARCH_COUNT,
            }

            await emit_status("â³ è¿æ¥AIæœç´¢æœåŠ¡å™¨...")
            resp = requests.post(
                self.valves.AI_SEARCH_ENDPOINT,
                headers=headers,
                json=payload,
                timeout=120,
            )
            resp.raise_for_status()
            data = resp.json()

            source_context_list = []
            ai_answers = []
            follow_up_questions = []

            await emit_status("ğŸ§  AIæ­£åœ¨åˆ†ææœç´¢ç»“æœ...")

            if "messages" in data:
                for msg in data["messages"]:
                    msg_role = msg.get("role", "")
                    msg_type = msg.get("type", "")
                    content_type = msg.get("content_type", "")
                    content = msg.get("content", "")

                    if (
                        msg_role == "assistant"
                        and msg_type == "source"
                        and content_type == "webpage"
                    ):
                        try:
                            content_obj = json.loads(content)
                            if "value" in content_obj and isinstance(
                                content_obj["value"], list
                            ):
                                await emit_status(
                                    f"ğŸ“„ å¤„ç† {len(content_obj['value'])} ä¸ªAIæœç´¢ç»“æœ..."
                                )

                                for i, item in enumerate(content_obj["value"]):
                                    search_content = item.get(
                                        "summary", ""
                                    ) or item.get("snippet", "")
                                    if not search_content:
                                        continue

                                    result_item = {
                                        "content": search_content,
                                        "title": item.get("name", ""),
                                        "url": item.get("url", ""),
                                        "site_name": item.get("siteName", ""),
                                        "date_published": item.get("datePublished", ""),
                                        "source_type": "é«˜çº§AIæ™ºèƒ½æœç´¢",
                                    }
                                    source_context_list.append(result_item)
                        except json.JSONDecodeError as e:
                            debug_log("è§£æAIæœç´¢ç»“æœJSONå¤±è´¥", e)

                    elif (
                        msg_role == "assistant"
                        and msg_type == "answer"
                        and content_type == "text"
                    ):
                        ai_answers.append(f"ğŸ¤– {content}")
                        await emit_status(f"âœ¨ AIç”Ÿæˆäº†ç¬¬ {len(ai_answers)} ä¸ªå›ç­”...")

                    elif (
                        msg_role == "assistant"
                        and msg_type == "follow_up"
                        and content_type == "text"
                    ):
                        follow_up_questions.append(f"ğŸ’­ {content}")
                        await emit_status(
                            f"ğŸ’¡ AIå»ºè®®äº†ç¬¬ {len(follow_up_questions)} ä¸ªè¿½é—®..."
                        )

            # å‘é€å¼•ç”¨
            for idx, r in enumerate(source_context_list):
                await emit_citation_data(r, __event_emitter__, run_id, idx)

            results_data = []
            for r in source_context_list:
                result_item = {
                    "title": (r.get("title") or ""),
                    "url": r.get("url"),
                    "snippet": take_text(r.get("content", ""), 300),
                }
                if self.valves.RETURN_CONTENT_IN_RESULTS:
                    result_item["content"] = take_text(
                        r.get("content", ""), self.valves.RETURN_CONTENT_MAX_CHARS
                    )
                results_data.append(result_item)

            result = {
                "search_results": results_data,
                "ai_answers": ai_answers,
                "follow_up_questions": follow_up_questions,
                "summary": {
                    "total_results": len(source_context_list),
                    "ai_answers_count": len(ai_answers),
                    "follow_up_count": len(follow_up_questions),
                    "search_type": "ğŸ¤– é«˜çº§AIæ™ºèƒ½æœç´¢",
                    "timestamp": datetime.now().isoformat(),
                },
            }

            await emit_status(
                status="complete",
                description=f"ğŸ‰ AIæ™ºèƒ½æœç´¢å®Œæˆï¼{len(source_context_list)} ä¸ªç»“æœï¼Œ{len(ai_answers)} ä¸ªAIç­”æ¡ˆ",
                done=True,
            )

            return json.dumps(result, ensure_ascii=False, indent=2)

        except Exception as e:
            debug_log("AIæ™ºèƒ½æœç´¢å¤±è´¥", e)
            error_details = {
                "error": str(e),
                "type": "âŒ AIæ™ºèƒ½æœç´¢é”™è¯¯",
                "debug_info": (
                    traceback.format_exc() if self.valves.DEBUG_MODE else None
                ),
            }
            await emit_status(
                status="error", description=f"âŒ AIæœç´¢å‡ºé”™: {str(e)}", done=True
            )
            return json.dumps(error_details, ensure_ascii=False, indent=2)


# ======================== Functionç±» - æš´éœ²å·¥å…·å‡½æ•° ========================
class Function:
    def __init__(self):
        self.tools = Tools()

    # Kimi AIåŸºç¡€æœç´¢
    async def kimi_ai_search(
        self,
        search_query: str,
        context: str = "",
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ™ Kimi AIåŸºç¡€æœç´¢ - é€šç”¨æœç´¢åŠŸèƒ½"""
        return await self.tools.kimi_ai_search(search_query, context, __event_emitter__)

    # ä¸“ä¸šä¸­æ–‡ç½‘é¡µæœç´¢
    async def search_chinese_web(
        self,
        query: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸ‡¨ğŸ‡³ ä¸“ä¸šä¸­æ–‡ç½‘é¡µæœç´¢å·¥å…·"""
        return await self.tools.search_chinese_web(query, __event_emitter__)

    # ä¸“ä¸šè‹±æ–‡ç½‘é¡µæœç´¢
    async def search_english_web(
        self,
        query: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ ä¸“ä¸šè‹±æ–‡ç½‘é¡µæœç´¢å·¥å…·"""
        return await self.tools.search_english_web(query, __event_emitter__)

    # æ™ºèƒ½ç½‘é¡µè¯»å–ï¼ˆæ”¯æŒLLMæ‘˜è¦ï¼‰
    async def web_scrape(
        self,
        urls: List[str],
        user_request: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ æ™ºèƒ½ç½‘é¡µè¯»å–å·¥å…·"""
        return await self.tools.web_scrape(urls, user_request, __event_emitter__)

    # Rawç½‘é¡µè¯»å–ï¼ˆä¸åšå¤„ç†ï¼‰
    async def web_scrape_raw(
        self,
        urls: List[str],
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ Rawç½‘é¡µè¯»å–å·¥å…·"""
        return await self.tools.web_scrape_raw(urls, __event_emitter__)

    # é«˜çº§AIæ™ºèƒ½æœç´¢
    async def search_ai_intelligent(
        self,
        query: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸ¤– é«˜çº§AIæ™ºèƒ½æœç´¢å·¥å…·"""
        return await self.tools.search_ai_intelligent(query, __event_emitter__)
