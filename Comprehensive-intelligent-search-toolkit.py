"""
title: ğŸ” ç»¼åˆæ™ºèƒ½æœç´¢å·¥å…·é›† - Kimi AI + Bocha + RAGä¼˜åŒ– + LLMæ™ºèƒ½æ‘˜è¦ + é“¾æ¥å™ªå£°æ²»ç† (å®Œæ•´ä¿®å¤ç‰ˆ)
author: JiangNanGenius
Github: https://github.com/JiangNanGenius
description: é›†æˆKimi AIåŸºç¡€æœç´¢ã€Bochaä¸“ä¸šæœç´¢ã€ç½‘é¡µè¯»å–ï¼Œæ”¯æŒLLMæ™ºèƒ½æ‘˜è¦æå–ã€RAGå‘é‡åŒ–ã€è¯­ä¹‰é‡æ’åºçš„æ™ºèƒ½æœç´¢å·¥å…·é›†ï¼Œå¼ºåŒ–é“¾æ¥å™ªå£°æ²»ç†å’Œä¼˜é›…å›é€€ï¼Œä¿®å¤è¯­æ³•é”™è¯¯å’Œåˆ†ç‰‡é‡å é—®é¢˜ï¼Œå®ç°å¹¶å‘LLMè°ƒç”¨
required_open_webui_version: 0.4.0
requirements: openai>=1.0.0, requests, beautifulsoup4, numpy, aiohttp
version: 3.9.3
license: MIT
"""

import os
import requests
import json
import asyncio
import aiohttp
import numpy as np
import hashlib
from pydantic import BaseModel, Field
from typing import Callable, Any, Optional, List, Dict
from urllib.parse import urlparse
import unicodedata
import re
from bs4 import BeautifulSoup
from openai import OpenAI
from datetime import datetime
import traceback
from uuid import uuid4
from collections import defaultdict


class Tools:
    class Valves(BaseModel):
        # è°ƒè¯•é…ç½®
        DEBUG_MODE: bool = Field(
            default=False, description="ğŸ› è°ƒè¯•æ¨¡å¼å¼€å…³ - æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯"
        )

        # Kimi AI é…ç½®
        MOONSHOT_API_KEY: str = Field(
            default="", description="ğŸŒ™ Moonshot APIå¯†é’¥ (ç”¨äºKimi AIåŸºç¡€æœç´¢åŠŸèƒ½)"
        )
        MOONSHOT_BASE_URL: str = Field(
            default="https://api.moonshot.cn/v1", description="ğŸŒ™ Moonshot APIåŸºç¡€URL"
        )
        KIMI_MODEL: str = Field(
            default="moonshot-v1-auto", description="ğŸ¤– Kimiä½¿ç”¨çš„æ¨¡å‹"
        )
        KIMI_TEMPERATURE: float = Field(default=0.3, description="ğŸŒ¡ï¸ Kimiæ¨¡å‹æ¸©åº¦å‚æ•°")

        # Bocha é…ç½®
        BOCHA_API_KEY: str = Field(
            default="YOUR_BOCHA_API_KEY",
            description="ğŸ”‘ Bocha AI APIå¯†é’¥ (ç”¨äºä¸“ä¸šä¸­æ–‡æœç´¢å’ŒAIæœç´¢)",
        )
        LANGSEARCH_API_KEY: str = Field(
            default="YOUR_LANGSEARCH_API_KEY",
            description="ğŸ—ï¸ LangSearch APIå¯†é’¥ (ç”¨äºä¸“ä¸šè‹±æ–‡æœç´¢)",
        )

        # è±†åŒ…å‘é‡åŒ–é…ç½®
        ARK_API_KEY: str = Field(
            default="YOUR_ARK_API_KEY",
            description="ğŸ¯ è±†åŒ…ARK APIå¯†é’¥ (ç”¨äºæ–‡æœ¬å‘é‡åŒ–)",
        )
        EMBEDDING_BASE_URL: str = Field(
            default="https://ark.cn-beijing.volces.com/api/v3",
            description="ğŸ¯ è±†åŒ…å‘é‡åŒ–APIåŸºç¡€URL",
        )
        EMBEDDING_MODEL: str = Field(
            default="doubao-embedding-large-text-250515",
            description="ğŸ“Š å‘é‡åŒ–æ¨¡å‹åç§°",
        )

        # LLMæ™ºèƒ½æ‘˜è¦é…ç½®
        ENABLE_SMART_SUMMARY: bool = Field(
            default=True, description="ğŸ§  æ˜¯å¦å¯ç”¨LLMæ™ºèƒ½æ‘˜è¦æå–"
        )
        SUMMARY_MIN_CHARS: int = Field(
            default=200, description="ğŸ“ å•æ¡æ‘˜è¦æœ€å°å­—ç¬¦æ•°ï¼ˆç»™LLMå‚è€ƒï¼‰"
        )
        SUMMARY_MAX_CHARS: int = Field(
            default=800, description="ğŸ“ å•æ¡æ‘˜è¦æœ€å¤§å­—ç¬¦æ•°ï¼ˆç»™LLMå‚è€ƒï¼‰"
        )
        SUMMARY_TEMPERATURE: float = Field(
            default=0.2, description="ğŸŒ¡ï¸ æ‘˜è¦æå–æ¸©åº¦å‚æ•°"
        )
        SUMMARY_COUNT_PER_PAGE: int = Field(default=20, description="ğŸ“„ æ¯é¡µæ‘˜è¦æ•°é‡")

        # è¯­ä¹‰å®‰å…¨åˆ†ç‰‡é…ç½®
        TARGET_CHUNK_CHARS: int = Field(
            default=2800, description="ğŸ¯ ç›®æ ‡åˆ†ç‰‡å¤§å°ï¼ˆå­—ç¬¦ï¼‰"
        )
        MAX_CHUNK_CHARS: int = Field(default=3500, description="â›” å•ç‰‡æœ€å¤§å­—ç¬¦")
        OVERLAP_SENTENCES: int = Field(default=3, description="ğŸ”— ç›¸é‚»åˆ†ç‰‡çš„å¥å­é‡å æ•°")
        MAX_TOTAL_CHUNKS: int = Field(default=32, description="ğŸ“š æ¯é¡µæœ€å¤šå¤„ç†çš„åˆ†ç‰‡æ•°")

        # å¹¶å‘æ§åˆ¶é…ç½®
        LLM_MAX_CONCURRENCY: int = Field(
            default=5, description="ğŸ•Šï¸ åˆ†ç‰‡å¹¶å‘æ‘˜è¦çš„æœ€å¤§å¹¶å‘æ•°"
        )
        LLM_RETRIES: int = Field(default=2, description="ğŸ” LLMè°ƒç”¨å¤±è´¥çš„é‡è¯•æ¬¡æ•°")
        LLM_BACKOFF_BASE_SEC: float = Field(
            default=1.2, description="â³ é‡è¯•é€€é¿åŸºæ•°ï¼ˆç§’ï¼‰"
        )
        LLM_REQUEST_TIMEOUT_SEC: float = Field(
            default=45.0, description="â±ï¸ å•æ¬¡LLMè°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"
        )

        # åˆ†ç‰‡ä¿æŠ¤ç­–ç•¥
        PRESERVE_TABLES: bool = Field(
            default=False, description="ğŸ“Š åˆ†ç‰‡æ—¶æ•´å—ä¿ç•™è¡¨æ ¼"
        )
        FLATTEN_TABLES: bool = Field(
            default=True, description="ğŸ“‹ å°†Markdownè¡¨æ ¼è½¬ä¸ºæ¡ç›®åˆ—è¡¨ï¼Œä¾¿äºæ¨¡å‹æç‚¼"
        )
        PRESERVE_CODEBLOCKS: bool = Field(
            default=True, description="ğŸ§© åˆ†ç‰‡æ—¶æ•´å—ä¿ç•™ä»£ç å—"
        )
        PRESERVE_LINKS: bool = Field(
            default=True, description="ğŸ”— åˆ†ç‰‡æ—¶ä¿è¯é“¾æ¥ä¸è¢«æ‰“æ•£"
        )
        DENOISE_LINK_SECTIONS: bool = Field(
            default=True, description="ğŸ§¹ ç§»é™¤çº¯é“¾æ¥/å¯¼èˆªæ®µè½"
        )

        # æ‘˜è¦ç­–ç•¥
        MAP_SUMMARY_PER_CHUNK: int = Field(
            default=3, description="ğŸ§­ map é˜¶æ®µï¼šæ¯ä¸ªåˆ†ç‰‡è¦æå–çš„æ‘˜è¦æ¡æ•°"
        )
        REDUCE_SUMMARY_LIMIT: int = Field(
            default=20, description="ğŸ§° reduce é˜¶æ®µï¼šæ•´é¡µä¿ç•™çš„æ‘˜è¦æ¡æ•°ä¸Šé™"
        )
        ENABLE_DETAILED_EXTRACTION: bool = Field(
            default=True, description="ğŸ“ æ˜¯å¦å¯ç”¨è¯¦ç»†ä¿¡æ¯æå–æ¨¡å¼"
        )
        ENCOURAGE_COMPREHENSIVE: bool = Field(
            default=True, description="ğŸ¯ æ˜¯å¦é¼“åŠ±å…¨é¢è¦†ç›–å„ä¸ªæ–¹é¢"
        )

        # LLMé…ç½®
        SEGMENTER_API_KEY: str = Field(
            default="", description="ğŸ”§ LLM APIå¯†é’¥ (ç•™ç©ºåˆ™ä½¿ç”¨Moonshotå¯†é’¥)"
        )
        SEGMENTER_BASE_URL: str = Field(
            default="", description="ğŸ”§ LLM APIåŸºç¡€URL (ç•™ç©ºåˆ™ä½¿ç”¨Moonshot URL)"
        )
        SEGMENTER_MODEL: str = Field(
            default="moonshot-v1-auto", description="ğŸ¤– LLMä½¿ç”¨çš„æ¨¡å‹"
        )
        SEGMENTER_TEMPERATURE: float = Field(default=0.1, description="ğŸŒ¡ï¸ LLMæ¸©åº¦å‚æ•°")

        # è¯„åˆ†æƒé‡é…ç½® - ç®€åŒ–ä¸ºåªæœ‰RAGå’Œrerank
        RERANK_WEIGHT: float = Field(default=0.6, description="âš–ï¸ é‡æ’åºåˆ†æ•°æƒé‡")
        RAG_WEIGHT: float = Field(default=0.4, description="âš–ï¸ RAGç›¸ä¼¼åº¦æƒé‡")

        # æœç´¢ç«¯ç‚¹é…ç½®
        CHINESE_WEB_SEARCH_ENDPOINT: str = Field(
            default="https://api.bochaai.com/v1/web-search",
            description="ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç½‘é¡µæœç´¢APIç«¯ç‚¹",
        )
        ENGLISH_WEB_SEARCH_ENDPOINT: str = Field(
            default="https://api.langsearch.com/v1/web-search",
            description="ğŸŒ è‹±æ–‡ç½‘é¡µæœç´¢APIç«¯ç‚¹",
        )
        AI_SEARCH_ENDPOINT: str = Field(
            default="https://api.bochaai.com/v1/ai-search",
            description="ğŸ¤– AIæ™ºèƒ½æœç´¢APIç«¯ç‚¹",
        )
        RERANK_ENDPOINT: str = Field(
            default="https://api.bochaai.com/v1/rerank",
            description="ğŸ¯ è¯­ä¹‰é‡æ’åºAPIç«¯ç‚¹",
        )

        # RAGå’Œé‡æ’åºé…ç½®
        ENABLE_RAG_ENHANCEMENT: bool = Field(
            default=True, description="ğŸ§  æ˜¯å¦å¯ç”¨RAGå‘é‡åŒ–ä¼˜åŒ–"
        )
        ENABLE_SEMANTIC_RERANK: bool = Field(
            default=True, description="ğŸ¯ æ˜¯å¦å¯ç”¨è¯­ä¹‰é‡æ’åº"
        )
        RERANK_MODEL: str = Field(default="gte-rerank", description="ğŸ¯ é‡æ’åºæ¨¡å‹åç§°")
        SIMILARITY_THRESHOLD: float = Field(
            default=0.08, description="ğŸ“Š RAGç›¸ä¼¼åº¦é˜ˆå€¼"
        )
        EMIT_ONLY_RAG_PASS: bool = Field(
            default=True, description="ğŸ¯ ä»…è¿”å›é€šè¿‡é˜ˆå€¼çš„RAGç»“æœ"
        )
        RERANK_TOP_N: int = Field(default=25, description="ğŸ¯ é‡æ’åºè¿”å›ç»“æœæ•°é‡")

        # å†…å®¹è¿”å›æ§åˆ¶ - ç®€åŒ–ä¸ºåªè¿”å›æ‘˜è¦
        RETURN_CONTENT_IN_RESULTS: bool = Field(
            default=False, description="ğŸ“„ æ˜¯å¦åœ¨ç»“æœJSONä¸­æºå¸¦contentå­—æ®µ"
        )
        RETURN_CONTENT_MAX_CHARS: int = Field(
            default=-1, description="ğŸ“ è¿”å›contentçš„æœ€å¤§å­—ç¬¦æ•°ï¼Œ<=0è¡¨ç¤ºä¸æˆªæ–­"
        )
        CITATION_DOC_MAX_CHARS: int = Field(
            default=6400, description="ğŸ“‹ å¼•ç”¨ä¸­æ–‡æ¡£æœ€å¤§å­—ç¬¦æ•°ï¼Œ<=0è¡¨ç¤ºä¸æˆªæ–­"
        )
        CITATION_CHUNK_SIZE: int = Field(
            default=1600, description="ğŸ”— å¼•ç”¨åˆ†ç‰‡å¤§å°ï¼Œ<=0è¡¨ç¤ºä¸åˆ†ç‰‡"
        )
        UNIQUE_REFERENCE_NAMES: bool = Field(
            default=True, description="ğŸ¯ å¼•ç”¨åå”¯ä¸€ï¼Œé¿å…UIåˆå¹¶/æŠ˜å "
        )
        PERSIST_CITATIONS: bool = Field(
            default=True, description="ğŸ’¾ å¤šæ¬¡è°ƒç”¨æ—¶ä¿ç•™å¹¶é‡å‘å†å²å¼•ç”¨"
        )
        PERSIST_CITATIONS_MAX: int = Field(
            default=100, description="ğŸ“š å†å²å¼•ç”¨æœ€å¤šä¿å­˜æ¡æ•°"
        )
        RAW_OUTPUT_FORMAT: str = Field(
            default="json", description="ğŸ“„ rawè¾“å‡ºæ ¼å¼: jsonæˆ–text"
        )

        # é€šç”¨é…ç½®
        CHINESE_SEARCH_COUNT: int = Field(default=15, description="ğŸ‡¨ğŸ‡³ ä¸­æ–‡æœç´¢ç»“æœæ•°é‡")
        ENGLISH_SEARCH_COUNT: int = Field(default=15, description="ğŸŒ è‹±æ–‡æœç´¢ç»“æœæ•°é‡")
        AI_SEARCH_COUNT: int = Field(default=15, description="ğŸ¤– AIæœç´¢ç»“æœæ•°é‡")
        CITATION_LINKS: bool = Field(
            default=True, description="ğŸ”— æ˜¯å¦å‘é€å¼•ç”¨é“¾æ¥å’Œå…ƒæ•°æ®"
        )
        FRESHNESS: str = Field(
            default="noLimit",
            description="â° æœç´¢æ—¶é—´èŒƒå›´ (noLimit, oneDay, oneWeek, oneMonth, oneYear)",
        )
        MAX_RETRIES: int = Field(default=3, description="ğŸ”„ æœ€å¤§é‡è¯•æ¬¡æ•°")

        # Jinaç½‘é¡µè¯»å–é…ç½®
        JINA_API_KEY: str = Field(
            default="jina_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
            description="ğŸŒ Jina APIå¯†é’¥ (ç”¨äºç½‘é¡µè¯»å–)",
        )

        # Kimiè”ç½‘æœç´¢é…ç½®
        KIMI_FORCE_SEARCH: bool = Field(
            default=True, description="ğŸŒ™ å¼ºåˆ¶Kimiè¿›è¡Œè”ç½‘æœç´¢"
        )
        KIMI_SEARCH_MAX_RETRIES: int = Field(
            default=3, description="ğŸ”„ Kimiè”ç½‘æœç´¢æœ€å¤§é‡è¯•æ¬¡æ•°"
        )

    def __init__(self):
        self.valves = self.Valves()
        self.kimi_client = None
        self.segmenter_client = None
        self.embedding_cache = {}
        self.citation = False
        self.run_seq = 0
        self.citations_history = []

    # ======================== Kimi AI æœç´¢ï¼ˆä¿®å¤ç‰ˆï¼šå¼ºåˆ¶è”ç½‘ï¼‰ ========================
    async def kimi_ai_search(
        self,
        search_query: str,
        context: str = "",
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ™ Kimi AIåŸºç¡€æœç´¢ï¼ˆä¿®å¤ç‰ˆï¼šå¼ºåˆ¶è”ç½‘ï¼‰"""

        # === å†…åµŒå·¥å…·å‡½æ•° ===
        def get_kimi_client():
            if (
                self.kimi_client is None
                or self.kimi_client.api_key != self.valves.MOONSHOT_API_KEY
            ):
                if not self.valves.MOONSHOT_API_KEY:
                    raise ValueError("Moonshot APIå¯†é’¥æ˜¯å¿…éœ€çš„")
                self.kimi_client = OpenAI(
                    base_url=self.valves.MOONSHOT_BASE_URL,
                    api_key=self.valves.MOONSHOT_API_KEY,
                )
            return self.kimi_client

        def next_run_id(tool: str) -> str:
            self.run_seq += 1
            return f"{tool}-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{self.run_seq}"

        def take_text(text: str, max_chars: int) -> str:
            if text is None:
                return ""
            if len(text) <= max_chars or max_chars <= 0:
                return text
            cut = text[:max_chars]
            # å°è¯•åœ¨æœ€è¿‘çš„å¥è¯»ç¬¦å¤„æˆªæ–­
            p = max(
                cut.rfind("ã€‚"),
                cut.rfind("ï¼"),
                cut.rfind("ï¼Ÿ"),
                cut.rfind("."),
                cut.rfind(";"),
            )
            if p >= max_chars * 0.6:  # ä»…åœ¨è¾ƒé åæ‰ä½¿ç”¨
                return cut[: p + 1] + " â€¦"
            return cut + " â€¦"

        def split_text_chunks(text: str, size: int) -> List[str]:
            if text is None:
                return [""]
            if size is None or size <= 0:
                return [text]
            return [text[i : i + size] for i in range(0, len(text), size)]

        async def emit_citation_data(r: Dict, __event_emitter__, run_id: str, idx: int):
            if not (__event_emitter__ and self.valves.CITATION_LINKS):
                return

            full_doc = r.get("content") or ""
            doc_for_emit = take_text(full_doc, self.valves.CITATION_DOC_MAX_CHARS)
            chunks = split_text_chunks(doc_for_emit, self.valves.CITATION_CHUNK_SIZE)
            base_title = (r.get("title") or "") or (r.get("url") or "Source")
            base_url = (r.get("url") or "").strip()

            for ci, chunk in enumerate(chunks, 1):
                if self.valves.UNIQUE_REFERENCE_NAMES:
                    src_name = f"{base_title} | {base_url} | {run_id}#{idx}-{ci}-{uuid4().hex[:6]}"
                else:
                    src_name = base_url or base_title

                payload = {
                    "type": "citation",
                    "data": {
                        "document": [chunk],
                        "metadata": [
                            {
                                "title": base_title,
                                "date_accessed": datetime.now().isoformat(),
                            }
                        ],
                        "source": {
                            "name": src_name,
                            "url": base_url or "",
                            "type": r.get("source_type", "webpage"),
                        },
                    },
                }
                await __event_emitter__(payload)

                if self.valves.PERSIST_CITATIONS:
                    self.citations_history.append(payload)
                    if len(self.citations_history) > self.valves.PERSIST_CITATIONS_MAX:
                        self.citations_history = self.citations_history[
                            -self.valves.PERSIST_CITATIONS_MAX :
                        ]

        # è·å–run_idå¹¶é‡æ”¾å†å²å¼•ç”¨
        run_id = next_run_id("kimi")
        if self.valves.PERSIST_CITATIONS and __event_emitter__:
            for old in self.citations_history:
                await __event_emitter__(old)

        def debug_log(message: str, error=None):
            if self.valves.DEBUG_MODE:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"[DEBUG {timestamp}] {message}")
                if error:
                    print(f"[DEBUG ERROR] {str(error)}")
                    print(f"[DEBUG TRACEBACK] {traceback.format_exc()}")

        async def emit_status(
            description: str, status: str = "in_progress", done: bool = False
        ):
            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "status": status,
                            "description": description,
                            "done": done,
                            "action": f"kimi_search:{run_id}",
                        },
                    }
                )

        def search_impl(arguments: Dict[str, Any]) -> Dict[str, Any]:
            """
            æ ¹æ®Kimiæ–‡æ¡£ï¼Œä½¿ç”¨å†…ç½®$web_searchæ—¶ï¼Œåªéœ€è¦åŸå°ä¸åŠ¨è¿”å›argumentså³å¯
            """
            debug_log(f"Kimi $web_search å‚æ•°: {arguments}")
            return arguments

        async def chat_with_tool_calls(messages: list) -> tuple:
            """
            ä½¿ç”¨å·¥å…·è°ƒç”¨æ–¹å¼ä¸Kimiäº¤äº’ï¼Œå¼ºåˆ¶è¿›è¡Œè”ç½‘æœç´¢
            è¿”å›: (final_response, search_used, search_results)
            """
            client = get_kimi_client()

            # ä½¿ç”¨æ”¯æŒæ›´å¤§ä¸Šä¸‹æ–‡çš„æ¨¡å‹
            model_to_use = "moonshot-v1-auto"
            if "kimi" in self.valves.KIMI_MODEL.lower():
                model_to_use = self.valves.KIMI_MODEL
            elif "moonshot" in self.valves.KIMI_MODEL.lower():
                model_to_use = self.valves.KIMI_MODEL
            else:
                # é»˜è®¤ä½¿ç”¨autoæ¨¡å‹
                model_to_use = "moonshot-v1-auto"

            debug_log(f"ä½¿ç”¨æ¨¡å‹: {model_to_use}")

            finish_reason = None
            search_used = False
            search_results = []
            final_content = ""

            # æ·»åŠ æœç´¢å·¥å…·å£°æ˜
            tools = [
                {
                    "type": "builtin_function",
                    "function": {
                        "name": "$web_search",
                    },
                }
            ]

            max_iterations = 5  # é˜²æ­¢æ— é™å¾ªç¯
            iteration = 0

            while (
                finish_reason is None or finish_reason == "tool_calls"
            ) and iteration < max_iterations:
                iteration += 1
                debug_log(f"Kimiå·¥å…·è°ƒç”¨è¿­ä»£ {iteration}")

                try:
                    completion = await asyncio.to_thread(
                        client.chat.completions.create,
                        model=model_to_use,
                        messages=messages,
                        temperature=self.valves.KIMI_TEMPERATURE,
                        tools=tools,
                        timeout=60,
                    )

                    choice = completion.choices[0]
                    finish_reason = choice.finish_reason

                    debug_log(f"Kimiå“åº”finish_reason: {finish_reason}")

                    if finish_reason == "tool_calls":
                        search_used = True
                        await emit_status("ğŸ” Kimiæ­£åœ¨è¿›è¡Œè”ç½‘æœç´¢...")

                        # æ·»åŠ assistantæ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
                        messages.append(choice.message)

                        # å¤„ç†å·¥å…·è°ƒç”¨
                        for tool_call in choice.message.tool_calls:
                            tool_call_name = tool_call.function.name
                            tool_call_arguments = json.loads(
                                tool_call.function.arguments
                            )

                            debug_log(f"å·¥å…·è°ƒç”¨: {tool_call_name}")
                            debug_log(f"å·¥å…·å‚æ•°: {tool_call_arguments}")

                            if tool_call_name == "$web_search":
                                # æ£€æŸ¥tokensæ¶ˆè€—ä¿¡æ¯
                                if "usage" in tool_call_arguments:
                                    search_tokens = tool_call_arguments["usage"].get(
                                        "total_tokens", 0
                                    )
                                    debug_log(f"æœç´¢å†…å®¹tokensæ¶ˆè€—: {search_tokens}")

                                tool_result = search_impl(tool_call_arguments)

                                # è®°å½•æœç´¢ç»“æœç”¨äºåç»­è§£æ
                                search_results.append(
                                    {
                                        "arguments": tool_call_arguments,
                                        "result": tool_result,
                                    }
                                )
                            else:
                                tool_result = f"Error: æ— æ³•æ‰¾åˆ°å·¥å…· '{tool_call_name}'"

                            # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯
                            messages.append(
                                {
                                    "role": "tool",
                                    "tool_call_id": tool_call.id,
                                    "name": tool_call_name,
                                    "content": json.dumps(
                                        tool_result, ensure_ascii=False
                                    ),
                                }
                            )

                    elif finish_reason == "stop":
                        final_content = choice.message.content or ""
                        if completion.usage:
                            debug_log(
                                f"æœ€ç»ˆtokensæ¶ˆè€—: prompt={completion.usage.prompt_tokens}, completion={completion.usage.completion_tokens}, total={completion.usage.total_tokens}"
                            )
                        break

                except Exception as e:
                    debug_log(f"Kimiå·¥å…·è°ƒç”¨å¼‚å¸¸: {e}")
                    raise e

            return final_content, search_used, search_results

        def parse_kimi_response(content: str, search_results: List[Dict]) -> tuple:
            """
            è§£æKimiçš„å“åº”å†…å®¹ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯
            è¿”å›: (parsed_results, sources)
            """
            debug_log(f"è§£æKimiå“åº”å†…å®¹: {content[:300]}...")

            # ä»æœç´¢ç»“æœä¸­æå–URLä¿¡æ¯
            urls_from_search = []
            for sr in search_results:
                args = sr.get("arguments", {})
                if "urls" in args:
                    urls_from_search.extend(args["urls"])
                elif "url" in args:
                    urls_from_search.append(args["url"])

            # ä»å†…å®¹ä¸­æå–é“¾æ¥
            url_pattern = r'https?://[^\s\)\]\}ï¼Œã€‚ï¼›ï¼ï¼Ÿ"\']*'
            urls_from_content = re.findall(url_pattern, content)

            # åˆå¹¶æ‰€æœ‰URL
            all_urls = list(set(urls_from_search + urls_from_content))
            debug_log(f"æå–åˆ°çš„URLs: {all_urls}")

            # å°è¯•æŒ‰æ®µè½åˆ†å‰²å†…å®¹
            paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]

            parsed_results = []

            # å¦‚æœå†…å®¹æœ‰æ˜æ˜¾çš„ç»“æ„åŒ–ä¿¡æ¯
            if any(
                marker in content for marker in ["1.", "2.", "ä¸€ã€", "äºŒã€", "##", "**"]
            ):
                # å°è¯•è§£æç»“æ„åŒ–å†…å®¹
                sections = re.split(r"\n(?=\d+\.|\w+ã€|##|\*\*)", content)
                for i, section in enumerate(sections):
                    section = section.strip()
                    if not section:
                        continue

                    # æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œï¼‰
                    lines = section.split("\n")
                    title = lines[0].strip("*#").strip()
                    content_text = (
                        "\n".join(lines[1:]).strip() if len(lines) > 1 else section
                    )

                    # ä¸ºæ¯ä¸ªæ®µè½åˆ†é…URL
                    section_url = ""
                    if i < len(all_urls):
                        section_url = all_urls[i]
                    elif all_urls:
                        section_url = all_urls[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªURLä½œä¸ºé»˜è®¤

                    parsed_results.append(
                        {
                            "title": title or f"æœç´¢ç»“æœ {i+1}",
                            "content": content_text or section,
                            "url": section_url,
                            "site_name": (
                                section_url.split("/")[2]
                                if section_url and "/" in section_url
                                else "Kimiæœç´¢"
                            ),
                            "date_published": datetime.now().strftime("%Y-%m-%d"),
                            "source_type": "Kimi AIè”ç½‘æœç´¢",
                        }
                    )
            else:
                # æ²¡æœ‰æ˜æ˜¾ç»“æ„ï¼Œå°†æ•´ä¸ªå†…å®¹ä½œä¸ºä¸€ä¸ªç»“æœ
                main_url = all_urls[0] if all_urls else ""
                parsed_results.append(
                    {
                        "title": f"å…³äº {search_query}",
                        "content": content,
                        "url": main_url,
                        "site_name": (
                            main_url.split("/")[2]
                            if main_url and "/" in main_url
                            else "Kimiæœç´¢"
                        ),
                        "date_published": datetime.now().strftime("%Y-%m-%d"),
                        "source_type": "Kimi AIè”ç½‘æœç´¢",
                    }
                )

            debug_log(f"è§£æå®Œæˆï¼Œå¾—åˆ° {len(parsed_results)} ä¸ªç»“æœ")
            return parsed_results, all_urls

        try:
            debug_log(f"å¼€å§‹Kimi AIæœç´¢: {search_query}, ä¸Šä¸‹æ–‡: {context}")

            # æ„å»ºæœç´¢æç¤º
            if context:
                search_prompt = f"è¯·æœç´¢å…³äº'{search_query}'çš„æœ€æ–°ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨'{context}'èƒŒæ™¯ä¸‹çš„ç›¸å…³å†…å®¹ã€‚è¯·ç¡®ä¿è¿›è¡Œå®æ—¶æœç´¢ä»¥è·å–å‡†ç¡®å’Œæœ€æ–°çš„ä¿¡æ¯ã€‚"
                await emit_status(
                    f"ğŸŒ™ å¼€å§‹Kimi AIè”ç½‘æœç´¢: {search_query} (èƒŒæ™¯: {context})"
                )
            else:
                search_prompt = f"è¯·æœç´¢å…³äº'{search_query}'çš„æœ€æ–°ä¿¡æ¯ã€‚è¯·ç¡®ä¿è¿›è¡Œå®æ—¶è”ç½‘æœç´¢ä»¥è·å–å‡†ç¡®å’Œæœ€æ–°çš„ä¿¡æ¯ï¼Œå¹¶æä¾›è¯¦ç»†çš„æœç´¢ç»“æœã€‚"
                await emit_status(f"ğŸŒ™ å¼€å§‹Kimi AIè”ç½‘æœç´¢: {search_query}")

            # å¼ºè°ƒè”ç½‘æœç´¢çš„ç³»ç»Ÿæç¤º
            system_prompt = """ä½ æ˜¯Kimi AIåŠ©æ‰‹ï¼Œå…·æœ‰å¼ºå¤§çš„å®æ—¶è”ç½‘æœç´¢èƒ½åŠ›ã€‚

é‡è¦æŒ‡ç¤ºï¼š
1. å¿…é¡»ä½¿ç”¨$web_searchå·¥å…·è¿›è¡Œå®æ—¶è”ç½‘æœç´¢
2. ä¸è¦ä»…ä¾èµ–è®­ç»ƒæ•°æ®ï¼Œå¿…é¡»è·å–æœ€æ–°ä¿¡æ¯
3. æœç´¢ç»“æœè¦åŒ…å«å…·ä½“çš„ç½‘ç«™é“¾æ¥å’Œæ¥æº
4. æŒ‰ç»“æ„åŒ–æ–¹å¼ç»„ç»‡æœç´¢ç»“æœï¼ŒåŒ…å«æ ‡é¢˜ã€å†…å®¹å’Œæ¥æºé“¾æ¥

è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸Šè¦æ±‚ï¼Œç¡®ä¿è¿›è¡ŒçœŸå®çš„è”ç½‘æœç´¢ã€‚"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": search_prompt},
            ]

            # é‡è¯•æœºåˆ¶ç¡®ä¿è”ç½‘æœç´¢
            search_success = False
            final_content = ""
            all_search_results = []

            for attempt in range(self.valves.KIMI_SEARCH_MAX_RETRIES):
                try:
                    debug_log(
                        f"Kimiæœç´¢å°è¯• {attempt + 1}/{self.valves.KIMI_SEARCH_MAX_RETRIES}"
                    )

                    content, search_used, search_results = await chat_with_tool_calls(
                        messages.copy()
                    )

                    if search_used:
                        debug_log("âœ… ç¡®è®¤Kimiè¿›è¡Œäº†è”ç½‘æœç´¢")
                        search_success = True
                        final_content = content
                        all_search_results = search_results
                        break
                    else:
                        debug_log(
                            f"âš ï¸ ç¬¬{attempt + 1}æ¬¡å°è¯•ï¼šKimiæœªè¿›è¡Œè”ç½‘æœç´¢ï¼Œå‡†å¤‡é‡è¯•"
                        )
                        if attempt < self.valves.KIMI_SEARCH_MAX_RETRIES - 1:
                            # ä¿®æ”¹æç¤ºè¯ï¼Œæ›´å¼ºè°ƒè”ç½‘æœç´¢
                            messages = [
                                {
                                    "role": "system",
                                    "content": system_prompt
                                    + "\n\nç‰¹åˆ«å¼ºè°ƒï¼šä½ å¿…é¡»ä½¿ç”¨$web_searchå·¥å…·è¿›è¡Œè”ç½‘æœç´¢ï¼Œä¸å¾—ä»…ä½¿ç”¨å·²æœ‰çŸ¥è¯†å›ç­”ã€‚",
                                },
                                {
                                    "role": "user",
                                    "content": f"è¯·ç«‹å³ä½¿ç”¨è”ç½‘æœç´¢åŠŸèƒ½æŸ¥æ‰¾å…³äº'{search_query}'çš„æœ€æ–°ä¿¡æ¯ã€‚è¿™æ˜¯ç¬¬{attempt + 2}æ¬¡è¯·æ±‚ï¼Œè¯·åŠ¡å¿…è¿›è¡Œå®æ—¶æœç´¢ã€‚",
                                },
                            ]
                            await asyncio.sleep(1)  # çŸ­æš‚å»¶è¿Ÿ

                except Exception as e:
                    debug_log(f"Kimiæœç´¢å°è¯• {attempt + 1} å¤±è´¥: {e}")
                    if attempt == self.valves.KIMI_SEARCH_MAX_RETRIES - 1:
                        raise e
                    await asyncio.sleep(2)

            if not search_success:
                # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½æœªè¿›è¡Œè”ç½‘æœç´¢ï¼Œè¿”å›è­¦å‘Š
                debug_log("âŒ æ‰€æœ‰é‡è¯•éƒ½æœªèƒ½è§¦å‘Kimiè”ç½‘æœç´¢")
                await emit_status(
                    "âš ï¸ æœªèƒ½è§¦å‘è”ç½‘æœç´¢ï¼Œè¿”å›åŸºç¡€å›ç­”", status="warning", done=True
                )

                error_result = {
                    "summary": {
                        "total_results": 0,
                        "total_sources": 0,
                        "search_query": search_query,
                        "context": context,
                        "search_type": "ğŸŒ™ Kimi AIåŸºç¡€æœç´¢",
                        "timestamp": datetime.now().isoformat(),
                        "status": "warning",
                        "message": "æœªèƒ½è§¦å‘è”ç½‘æœç´¢åŠŸèƒ½",
                    },
                    "results": [],
                }
                return json.dumps(error_result, ensure_ascii=False, indent=2)

            await emit_status("âœ… è”ç½‘æœç´¢å®Œæˆï¼Œæ­£åœ¨å¤„ç†ç»“æœ...")

            # è§£ææœç´¢ç»“æœ
            parsed_results, sources = parse_kimi_response(
                final_content, all_search_results
            )

            # å‘é€å¼•ç”¨æ•°æ®
            for idx, r in enumerate(parsed_results):
                await emit_citation_data(r, __event_emitter__, run_id, idx)

            # æ„å»ºè¿”å›æ•°æ®
            results_data = []
            for r in parsed_results:
                result_item = {
                    "title": r.get("title", ""),
                    "url": r.get("url", ""),
                    "snippet": take_text(r.get("content", ""), 300),
                }
                if self.valves.RETURN_CONTENT_IN_RESULTS:
                    result_item["content"] = take_text(
                        r.get("content", ""),
                        self.valves.RETURN_CONTENT_MAX_CHARS,
                    )
                results_data.append(result_item)

            result = {
                "summary": {
                    "total_results": len(parsed_results),
                    "total_sources": len(sources),
                    "search_query": search_query,
                    "context": context,
                    "search_type": "ğŸŒ™ Kimi AIè”ç½‘æœç´¢",
                    "timestamp": datetime.now().isoformat(),
                    "search_verified": search_success,
                },
                "results": results_data,
            }

            await emit_status("ğŸ‰ Kimi AIè”ç½‘æœç´¢å®Œæˆï¼", status="complete", done=True)
            return json.dumps(result, ensure_ascii=False, indent=2)

        except Exception as e:
            debug_log("Kimi AIæœç´¢å¤±è´¥", e)
            await emit_status(
                f"âŒ Kimi AIæœç´¢å¤±è´¥: {str(e)}", status="error", done=True
            )

            error_result = {
                "summary": {
                    "total_results": 0,
                    "total_sources": 0,
                    "search_query": search_query,
                    "context": context,
                    "search_type": "ğŸŒ™ Kimi AIè”ç½‘æœç´¢",
                    "timestamp": datetime.now().isoformat(),
                    "status": "error",
                    "error": str(e),
                },
                "results": [],
            }
            return json.dumps(error_result, ensure_ascii=False, indent=2)

    # ======================== ä¸“ä¸šä¸­æ–‡æœç´¢ ========================
    async def search_chinese_web(
        self,
        query: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸ‡¨ğŸ‡³ ä¸“ä¸šä¸­æ–‡ç½‘é¡µæœç´¢å·¥å…·"""

        def next_run_id(tool: str) -> str:
            self.run_seq += 1
            return f"{tool}-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{self.run_seq}"

        def take_text(text: str, max_chars: int) -> str:
            if text is None:
                return ""
            if len(text) <= max_chars or max_chars <= 0:
                return text
            cut = text[:max_chars]
            # å°è¯•åœ¨æœ€è¿‘çš„å¥è¯»ç¬¦å¤„æˆªæ–­
            p = max(
                cut.rfind("ã€‚"),
                cut.rfind("ï¼"),
                cut.rfind("ï¼Ÿ"),
                cut.rfind("."),
                cut.rfind(";"),
            )
            if p >= max_chars * 0.6:  # ä»…åœ¨è¾ƒé åæ‰ä½¿ç”¨
                return cut[: p + 1] + " â€¦"
            return cut + " â€¦"

        def split_text_chunks(text: str, size: int) -> List[str]:
            if text is None:
                return [""]
            if size is None or size <= 0:
                return [text]
            return [text[i : i + size] for i in range(0, len(text), size)]

        async def emit_citation_data(r: Dict, __event_emitter__, run_id: str, idx: int):
            if not (__event_emitter__ and self.valves.CITATION_LINKS):
                return

            full_doc = r.get("content") or ""
            doc_for_emit = take_text(full_doc, self.valves.CITATION_DOC_MAX_CHARS)
            chunks = split_text_chunks(doc_for_emit, self.valves.CITATION_CHUNK_SIZE)
            base_title = (r.get("title") or "") or (r.get("url") or "Source")
            base_url = (r.get("url") or "").strip()

            for ci, chunk in enumerate(chunks, 1):
                if self.valves.UNIQUE_REFERENCE_NAMES:
                    src_name = f"{base_title} | {base_url} | {run_id}#{idx}-{ci}-{uuid4().hex[:6]}"
                else:
                    src_name = base_url or base_title

                payload = {
                    "type": "citation",
                    "data": {
                        "document": [chunk],
                        "metadata": [
                            {
                                "title": base_title,
                                "date_accessed": datetime.now().isoformat(),
                            }
                        ],
                        "source": {
                            "name": src_name,
                            "url": base_url or "",
                            "type": r.get("source_type", "webpage"),
                        },
                    },
                }
                await __event_emitter__(payload)

                if self.valves.PERSIST_CITATIONS:
                    self.citations_history.append(payload)
                    if len(self.citations_history) > self.valves.PERSIST_CITATIONS_MAX:
                        self.citations_history = self.citations_history[
                            -self.valves.PERSIST_CITATIONS_MAX :
                        ]

        async def get_text_embedding(text: str) -> Optional[List[float]]:
            if not self.valves.ENABLE_RAG_ENHANCEMENT or not self.valves.ARK_API_KEY:
                return None

            text_hash = hashlib.sha256(text.encode("utf-8")).hexdigest()
            if text_hash in self.embedding_cache:
                return self.embedding_cache[text_hash]

            try:
                headers = {
                    "Authorization": f"Bearer {self.valves.ARK_API_KEY}",
                    "Content-Type": "application/json",
                }

                clean_text = text.strip()[:4000]
                if not clean_text:
                    return None

                payload = {
                    "model": self.valves.EMBEDDING_MODEL,
                    "input": [clean_text],
                    "encoding_format": "float",
                }

                response = requests.post(
                    f"{self.valves.EMBEDDING_BASE_URL}/embeddings",
                    headers=headers,
                    json=payload,
                    timeout=30,
                )
                response.raise_for_status()
                data = response.json()

                if "data" in data and len(data["data"]) > 0:
                    embedding = data["data"][0]["embedding"]
                    self.embedding_cache[text_hash] = embedding
                    return embedding
                else:
                    return None

            except Exception:
                return None

        def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:
            try:
                v1 = np.array(vec1)
                v2 = np.array(vec2)
                return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))
            except Exception:
                return 0.0

        run_id = next_run_id("zh-web")
        if self.valves.PERSIST_CITATIONS and __event_emitter__:
            for old in self.citations_history:
                await __event_emitter__(old)

        def debug_log(message: str, error=None):
            if self.valves.DEBUG_MODE:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"[DEBUG {timestamp}] {message}")
                if error:
                    print(f"[DEBUG ERROR] {str(error)}")
                    print(f"[DEBUG TRACEBACK] {traceback.format_exc()}")

        async def emit_status(
            description: str, status: str = "in_progress", done: bool = False
        ):
            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "status": status,
                            "description": description,
                            "done": done,
                            "action": f"zh_search:{run_id}",
                        },
                    }
                )

        # æ„å›¾æ‹†è§£ + å¤šè§†è§’ç›¸ä¼¼åº¦
        def plan_aspects(user_request: str):
            """è½»é‡è§„åˆ’å™¨ï¼šæ ¹æ®æŸ¥è¯¢æ„å›¾æ‹†è§£æœç´¢è§†è§’"""
            buckets = []
            if re.search(r"æ„ä¹‰|å«ä¹‰|è±¡å¾|æœ¬è´¨|å“²å­¦", user_request):
                buckets += [
                    "æ•°å­¦å®šä¹‰ ä¸ å•ä½å…ƒ",
                    "æ–‡åŒ–/å“²å­¦è±¡å¾",
                    "åº”ç”¨åœºæ™¯ ä¸ å½’ä¸€åŒ–/è®¡é‡",
                    "è¯­è¨€å­¦/è¯æº",
                ]
            else:
                buckets += ["æ ¸å¿ƒå®šä¹‰", "æ€§è´¨/å®šç†", "å†å²ä¸ç¬¦å·", "åº”ç”¨ä¸å·¥ç¨‹"]
            return buckets[:4]

        async def batch_embeddings(texts: List[str]) -> List[Optional[List[float]]]:
            """æ‰¹é‡å‘é‡åŒ–"""
            if not texts:
                return []
            if not (self.valves.ENABLE_RAG_ENHANCEMENT and self.valves.ARK_API_KEY):
                return [None for _ in texts]

            headers = {
                "Authorization": f"Bearer {self.valves.ARK_API_KEY}",
                "Content-Type": "application/json",
            }

            try:
                payload = {
                    "model": self.valves.EMBEDDING_MODEL,
                    "input": [t[:4000] for t in texts],
                    "encoding_format": "float",
                }

                resp = await asyncio.to_thread(
                    requests.post,
                    f"{self.valves.EMBEDDING_BASE_URL}/embeddings",
                    headers=headers,
                    json=payload,
                    timeout=60,
                )
                resp.raise_for_status()
                data = resp.json()

                vecs = [d["embedding"] for d in data.get("data", [])]
                if len(vecs) != len(texts):
                    fallback = []
                    for t in texts:
                        v = await get_text_embedding(t)
                        fallback.append(v)
                    return fallback

                return vecs

            except Exception:
                out = []
                for t in texts:
                    v = await get_text_embedding(t)
                    out.append(v)
                return out

        async def enhance_results_with_rag(results: List[Dict]) -> List[Dict]:
            if not self.valves.ENABLE_RAG_ENHANCEMENT or not results:
                debug_log("RAGæœªå¯ç”¨æˆ–ç»“æœä¸ºç©º")
                return results

            try:
                await emit_status(f"ğŸ§  æ­£åœ¨è¿›è¡ŒRAGå‘é‡åŒ–ä¼˜åŒ– ({len(results)} ä¸ªç»“æœ)")
                debug_log(f"å¼€å§‹RAGä¼˜åŒ–ï¼ŒæŸ¥è¯¢: {query}, ç»“æœæ•°: {len(results)}")

                # å¤šè§†è§’ç›¸ä¼¼åº¦èåˆ
                aspects = plan_aspects(query)
                all_texts = [query] + aspects
                all_vecs = await batch_embeddings(all_texts)

                query_vec = all_vecs[0] if all_vecs else None
                aspect_vecs = all_vecs[1:] if len(all_vecs) > 1 else []

                def fuse_similarity(doc_vec):
                    """èåˆæŸ¥è¯¢å’Œå„æ–¹é¢å­æŸ¥è¯¢çš„ç›¸ä¼¼åº¦"""
                    sims = []
                    if query_vec is not None and doc_vec is not None:
                        sims.append(calculate_similarity(query_vec, doc_vec))
                    for av in aspect_vecs:
                        if av is not None and doc_vec is not None:
                            sims.append(calculate_similarity(av, doc_vec))
                    return max(sims) if sims else 0.0

                if not query_vec:
                    debug_log("æŸ¥è¯¢å‘é‡åŒ–å¤±è´¥ï¼Œè¿”å›åŸç»“æœ")
                    return results

                enhanced_results = []
                for i, result in enumerate(results):
                    content = result.get("content", "")
                    if not content:
                        debug_log(f"ç»“æœ {i} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                        continue

                    content_embedding = await get_text_embedding(content)
                    if content_embedding:
                        similarity = fuse_similarity(content_embedding)
                        result["rag_similarity"] = similarity
                        result["rag_enhanced"] = True
                        debug_log(f"ç»“æœ {i} ç›¸ä¼¼åº¦: {similarity:.3f}")
                        enhanced_results.append(result)
                    else:
                        result["rag_similarity"] = 0.0
                        result["rag_enhanced"] = False
                        enhanced_results.append(result)
                        debug_log(f"ç»“æœ {i} å‘é‡åŒ–å¤±è´¥ï¼Œä¿ç•™åŸç»“æœ")

                enhanced_results.sort(
                    key=lambda x: x.get("rag_similarity", 0), reverse=True
                )

                debug_log(f"RAGä¼˜åŒ–å®Œæˆï¼Œä¿ç•™ {len(enhanced_results)} ä¸ªç»“æœ")
                await emit_status(f"âœ… RAGä¼˜åŒ–å®Œæˆ")
                return enhanced_results

            except Exception as e:
                debug_log("RAGä¼˜åŒ–å¤±è´¥", e)
                return results

        async def rerank_results(results: List[Dict]) -> List[Dict]:
            if (
                not self.valves.ENABLE_SEMANTIC_RERANK
                or not results
                or not self.valves.BOCHA_API_KEY
            ):
                debug_log("è¯­ä¹‰é‡æ’åºæœªå¯ç”¨æˆ–é…ç½®ä¸å®Œæ•´")
                return results

            try:
                await emit_status(f"ğŸ¯ æ­£åœ¨è¿›è¡Œè¯­ä¹‰é‡æ’åº ({len(results)} ä¸ªç»“æœ)")
                debug_log(f"å¼€å§‹è¯­ä¹‰é‡æ’åºï¼ŒæŸ¥è¯¢: {query}")

                # æ„é€  documents æ—¶å»ºç«‹æ˜ å°„
                documents = []
                doc_to_result_idx = []

                for i, result in enumerate(results):
                    content = (result.get("content") or "")[:4000]
                    if content:
                        documents.append(content)
                        doc_to_result_idx.append(i)

                if not documents:
                    debug_log("æ²¡æœ‰æœ‰æ•ˆæ–‡æ¡£ï¼Œè·³è¿‡é‡æ’åº")
                    return results

                headers = {
                    "Authorization": f"Bearer {self.valves.BOCHA_API_KEY}",
                    "Content-Type": "application/json",
                }

                payload = {
                    "model": self.valves.RERANK_MODEL,
                    "query": query,
                    "documents": documents,
                    "top_n": min(self.valves.RERANK_TOP_N, len(documents)),
                    "return_documents": False,
                }

                response = requests.post(
                    self.valves.RERANK_ENDPOINT,
                    headers=headers,
                    json=payload,
                    timeout=60,
                )
                response.raise_for_status()
                data = response.json()

                if "data" in data and "results" in data["data"]:
                    rerank_results_data = data["data"]["results"]
                    reranked_results = []

                    # å›å¡«æ—¶ä½¿ç”¨æ˜ å°„
                    for rerank_item in rerank_results_data:
                        doc_idx = rerank_item.get("index", 0)
                        relevance_score = rerank_item.get("relevance_score", 0.0)

                        if 0 <= doc_idx < len(doc_to_result_idx):
                            orig_idx = doc_to_result_idx[doc_idx]
                            result = results[orig_idx].copy()
                            result["rerank_score"] = relevance_score
                            result["rerank_enhanced"] = True
                            reranked_results.append(result)

                    debug_log(f"é‡æ’åºå®Œæˆï¼Œè¿”å› {len(reranked_results)} ä¸ªç»“æœ")
                    await emit_status(f"âœ… è¯­ä¹‰é‡æ’åºå®Œæˆ")
                    return reranked_results
                else:
                    debug_log("é‡æ’åºå“åº”æ ¼å¼å¼‚å¸¸")
                    return results

            except Exception as e:
                debug_log("è¯­ä¹‰é‡æ’åºå¤±è´¥", e)
                return results

        try:
            debug_log(f"å¼€å§‹ä¸­æ–‡ç½‘é¡µæœç´¢: {query}")
            await emit_status(f"ğŸ” æ­£åœ¨è¿›è¡Œä¸“ä¸šä¸­æ–‡ç½‘é¡µæœç´¢: {query}")

            headers = {
                "Authorization": f"Bearer {self.valves.BOCHA_API_KEY}",
                "Content-Type": "application/json",
            }

            payload = {
                "query": query,
                "freshness": self.valves.FRESHNESS,
                "summary": True,
                "count": self.valves.CHINESE_SEARCH_COUNT,
            }

            await emit_status("â³ æ­£åœ¨è¿æ¥ä¸“ä¸šä¸­æ–‡æœç´¢æœåŠ¡å™¨...")

            resp = requests.post(
                self.valves.CHINESE_WEB_SEARCH_ENDPOINT,
                headers=headers,
                json=payload,
                timeout=120,
            )
            resp.raise_for_status()
            data = resp.json()

            source_context_list = []
            len_raw = 0

            if "data" in data and "webPages" in data["data"]:
                web_pages = data["data"]["webPages"]
                if "value" in web_pages and isinstance(web_pages["value"], list):
                    len_raw = len(web_pages["value"])
                    await emit_status(f"ğŸ“„ æ­£åœ¨å¤„ç† {len_raw} ä¸ªä¸­æ–‡ç½‘é¡µç»“æœ...")

                    for i, item in enumerate(web_pages["value"]):
                        url = item.get("url", "")
                        snippet = item.get("snippet", "")
                        summary = item.get("summary", "")
                        name = item.get("name", "")
                        site_name = item.get("siteName", "")
                        date_published = item.get("datePublished", "")

                        content = summary or snippet
                        if not content:
                            debug_log(f"ä¸­æ–‡æœç´¢ç»“æœ {i} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                            continue

                        result_item = {
                            "content": content,
                            "title": name,
                            "url": url,
                            "site_name": site_name if site_name else "",
                            "date_published": date_published if date_published else "",
                            "source_type": "ä¸“ä¸šä¸­æ–‡ç½‘é¡µ",
                        }
                        source_context_list.append(result_item)

            debug_log(f"åŸå§‹ä¸­æ–‡æœç´¢ç»“æœæ•°: {len(source_context_list)}")

            if self.valves.ENABLE_RAG_ENHANCEMENT:
                source_context_list = await enhance_results_with_rag(
                    source_context_list
                )

            if self.valves.ENABLE_SEMANTIC_RERANK:
                source_context_list = await rerank_results(source_context_list)

            # è¯„åˆ†å½’ä¸€åŒ–ä¸é¡ºåº
            def zminmax(vals):
                if not vals:
                    return []
                lo, hi = min(vals), max(vals)
                if hi - lo < 1e-6:
                    return [0.5 for _ in vals]
                return [(v - lo) / (hi - lo) for v in vals]

            if source_context_list:
                rags = [float(x.get("rag_similarity", 0)) for x in source_context_list]
                rers = [float(x.get("rerank_score", 0)) for x in source_context_list]

                rags_n, rers_n = map(zminmax, (rags, rers))

                for i, s in enumerate(source_context_list):
                    s["final_score"] = (
                        self.valves.RAG_WEIGHT * rags_n[i]
                        + self.valves.RERANK_WEIGHT * rers_n[i]
                    )

                # å…ˆæ’åºå–å‰Nï¼Œå†æŒ‰é˜ˆå€¼åšè½»è¿‡æ»¤
                source_context_list.sort(
                    key=lambda x: x.get("final_score", 0), reverse=True
                )
                source_context_list = source_context_list[: self.valves.RERANK_TOP_N]

                if (
                    self.valves.ENABLE_RAG_ENHANCEMENT
                    and self.valves.EMIT_ONLY_RAG_PASS
                ):
                    thr = max(
                        0.03, float(self.valves.SIMILARITY_THRESHOLD) * 0.6
                    )  # æ”¾å®½
                    source_context_list = [
                        s
                        for s in source_context_list
                        if float(s.get("rag_similarity", 0)) >= thr
                        or s["final_score"] >= 0.35
                    ]

            for idx, r in enumerate(source_context_list):
                await emit_citation_data(r, __event_emitter__, run_id, idx)

            await emit_status(
                status="complete",
                description=f"ğŸ‰ ä¸­æ–‡ç½‘é¡µæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(source_context_list)} ä¸ªç»“æœ",
                done=True,
            )

            results_data = []
            for r in source_context_list:
                result_item = {
                    "title": (r.get("title") or ""),
                    "url": r.get("url"),
                    "rag_similarity": float(r.get("rag_similarity") or 0.0),
                    "rerank_score": float(r.get("rerank_score") or 0.0),
                    "final_score": float(r.get("final_score") or 0.0),
                    "snippet": take_text(r.get("content", ""), 450),
                }
                if self.valves.RETURN_CONTENT_IN_RESULTS:
                    result_item["content"] = take_text(
                        r.get("content", ""), self.valves.RETURN_CONTENT_MAX_CHARS
                    )
                results_data.append(result_item)

            return json.dumps(
                {
                    "summary": {
                        "rag_threshold": self.valves.SIMILARITY_THRESHOLD,
                        "kept": len(source_context_list),
                        "total": len_raw,
                        "search_type": "ğŸ‡¨ğŸ‡³ ä¸“ä¸šä¸­æ–‡ç½‘é¡µ",
                    },
                    "results": results_data,
                },
                ensure_ascii=False,
                indent=2,
            )

        except Exception as e:
            debug_log("ä¸­æ–‡ç½‘é¡µæœç´¢å¤±è´¥", e)
            error_details = {
                "error": str(e),
                "type": "âŒ ä¸“ä¸šä¸­æ–‡ç½‘é¡µæœç´¢é”™è¯¯",
                "debug_info": (
                    traceback.format_exc() if self.valves.DEBUG_MODE else None
                ),
            }
            await emit_status(
                status="error", description=f"âŒ æœç´¢å‡ºé”™: {str(e)}", done=True
            )
            return json.dumps(error_details, ensure_ascii=False, indent=2)

    # ======================== ä¸“ä¸šè‹±æ–‡æœç´¢ ========================
    async def search_english_web(
        self,
        query: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ ä¸“ä¸šè‹±æ–‡ç½‘é¡µæœç´¢å·¥å…·"""

        def next_run_id(tool: str) -> str:
            self.run_seq += 1
            return f"{tool}-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{self.run_seq}"

        def take_text(text: str, max_chars: int) -> str:
            if text is None:
                return ""
            if len(text) <= max_chars or max_chars <= 0:
                return text
            cut = text[:max_chars]
            # å°è¯•åœ¨æœ€è¿‘çš„å¥è¯»ç¬¦å¤„æˆªæ–­
            p = max(
                cut.rfind("ã€‚"),
                cut.rfind("ï¼"),
                cut.rfind("ï¼Ÿ"),
                cut.rfind("."),
                cut.rfind(";"),
            )
            if p >= max_chars * 0.6:  # ä»…åœ¨è¾ƒé åæ‰ä½¿ç”¨
                return cut[: p + 1] + " â€¦"
            return cut + " â€¦"

        def split_text_chunks(text: str, size: int) -> List[str]:
            if text is None:
                return [""]
            if size is None or size <= 0:
                return [text]
            return [text[i : i + size] for i in range(0, len(text), size)]

        async def emit_citation_data(r: Dict, __event_emitter__, run_id: str, idx: int):
            if not (__event_emitter__ and self.valves.CITATION_LINKS):
                return

            full_doc = r.get("content") or ""
            doc_for_emit = take_text(full_doc, self.valves.CITATION_DOC_MAX_CHARS)
            chunks = split_text_chunks(doc_for_emit, self.valves.CITATION_CHUNK_SIZE)
            base_title = (r.get("title") or "") or (r.get("url") or "Source")
            base_url = (r.get("url") or "").strip()

            for ci, chunk in enumerate(chunks, 1):
                if self.valves.UNIQUE_REFERENCE_NAMES:
                    src_name = f"{base_title} | {base_url} | {run_id}#{idx}-{ci}-{uuid4().hex[:6]}"
                else:
                    src_name = base_url or base_title

                payload = {
                    "type": "citation",
                    "data": {
                        "document": [chunk],
                        "metadata": [
                            {
                                "title": base_title,
                                "date_accessed": datetime.now().isoformat(),
                            }
                        ],
                        "source": {
                            "name": src_name,
                            "url": base_url or "",
                            "type": r.get("source_type", "webpage"),
                        },
                    },
                }
                await __event_emitter__(payload)

                if self.valves.PERSIST_CITATIONS:
                    self.citations_history.append(payload)
                    if len(self.citations_history) > self.valves.PERSIST_CITATIONS_MAX:
                        self.citations_history = self.citations_history[
                            -self.valves.PERSIST_CITATIONS_MAX :
                        ]

        async def get_text_embedding(text: str) -> Optional[List[float]]:
            if not self.valves.ENABLE_RAG_ENHANCEMENT or not self.valves.ARK_API_KEY:
                return None

            text_hash = hashlib.sha256(text.encode("utf-8")).hexdigest()
            if text_hash in self.embedding_cache:
                return self.embedding_cache[text_hash]

            try:
                headers = {
                    "Authorization": f"Bearer {self.valves.ARK_API_KEY}",
                    "Content-Type": "application/json",
                }

                clean_text = text.strip()[:4000]
                if not clean_text:
                    return None

                payload = {
                    "model": self.valves.EMBEDDING_MODEL,
                    "input": [clean_text],
                    "encoding_format": "float",
                }

                response = requests.post(
                    f"{self.valves.EMBEDDING_BASE_URL}/embeddings",
                    headers=headers,
                    json=payload,
                    timeout=30,
                )
                response.raise_for_status()
                data = response.json()

                if "data" in data and len(data["data"]) > 0:
                    embedding = data["data"][0]["embedding"]
                    self.embedding_cache[text_hash] = embedding
                    return embedding
                else:
                    return None

            except Exception:
                return None

        def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:
            try:
                v1 = np.array(vec1)
                v2 = np.array(vec2)
                return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))
            except Exception:
                return 0.0

        run_id = next_run_id("en-web")
        if self.valves.PERSIST_CITATIONS and __event_emitter__:
            for old in self.citations_history:
                await __event_emitter__(old)

        def debug_log(message: str, error=None):
            if self.valves.DEBUG_MODE:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"[DEBUG {timestamp}] {message}")
                if error:
                    print(f"[DEBUG ERROR] {str(error)}")
                    print(f"[DEBUG TRACEBACK] {traceback.format_exc()}")

        async def emit_status(
            description: str, status: str = "in_progress", done: bool = False
        ):
            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "status": status,
                            "description": description,
                            "done": done,
                            "action": f"en_search:{run_id}",
                        },
                    }
                )

        # æ„å›¾æ‹†è§£ + å¤šè§†è§’ç›¸ä¼¼åº¦
        def plan_aspects(user_request: str):
            """è½»é‡è§„åˆ’å™¨ï¼šæ ¹æ®æŸ¥è¯¢æ„å›¾æ‹†è§£æœç´¢è§†è§’"""
            buckets = []
            if re.search(
                r"meaning|significance|symbolism|essence|philosophy", user_request, re.I
            ):
                buckets += [
                    "mathematical definition",
                    "cultural/philosophical symbolism",
                    "application scenarios",
                    "linguistic/etymology",
                ]
            else:
                buckets += [
                    "core definition",
                    "properties/theorems",
                    "history and symbols",
                    "applications and engineering",
                ]
            return buckets[:4]

        async def batch_embeddings(texts: List[str]) -> List[Optional[List[float]]]:
            """æ‰¹é‡å‘é‡åŒ–"""
            if not texts:
                return []
            if not (self.valves.ENABLE_RAG_ENHANCEMENT and self.valves.ARK_API_KEY):
                return [None for _ in texts]

            headers = {
                "Authorization": f"Bearer {self.valves.ARK_API_KEY}",
                "Content-Type": "application/json",
            }

            try:
                payload = {
                    "model": self.valves.EMBEDDING_MODEL,
                    "input": [t[:4000] for t in texts],
                    "encoding_format": "float",
                }

                resp = await asyncio.to_thread(
                    requests.post,
                    f"{self.valves.EMBEDDING_BASE_URL}/embeddings",
                    headers=headers,
                    json=payload,
                    timeout=60,
                )
                resp.raise_for_status()
                data = resp.json()

                vecs = [d["embedding"] for d in data.get("data", [])]
                if len(vecs) != len(texts):
                    fallback = []
                    for t in texts:
                        v = await get_text_embedding(t)
                        fallback.append(v)
                    return fallback

                return vecs

            except Exception:
                out = []
                for t in texts:
                    v = await get_text_embedding(t)
                    out.append(v)
                return out

        async def enhance_results_with_rag(results: List[Dict]) -> List[Dict]:
            if not self.valves.ENABLE_RAG_ENHANCEMENT or not results:
                return results

            try:
                await emit_status(f"ğŸ§  RAGä¼˜åŒ– ({len(results)} ä¸ªç»“æœ)")

                # å¤šè§†è§’ç›¸ä¼¼åº¦èåˆ
                aspects = plan_aspects(query)
                all_texts = [query] + aspects
                all_vecs = await batch_embeddings(all_texts)

                query_vec = all_vecs[0] if all_vecs else None
                aspect_vecs = all_vecs[1:] if len(all_vecs) > 1 else []

                def fuse_similarity(doc_vec):
                    """èåˆæŸ¥è¯¢å’Œå„æ–¹é¢å­æŸ¥è¯¢çš„ç›¸ä¼¼åº¦"""
                    sims = []
                    if query_vec is not None and doc_vec is not None:
                        sims.append(calculate_similarity(query_vec, doc_vec))
                    for av in aspect_vecs:
                        if av is not None and doc_vec is not None:
                            sims.append(calculate_similarity(av, doc_vec))
                    return max(sims) if sims else 0.0

                if not query_vec:
                    return results

                enhanced_results = []
                for i, result in enumerate(results):
                    content = result.get("content", "")
                    if not content:
                        continue

                    content_embedding = await get_text_embedding(content)
                    if content_embedding:
                        similarity = fuse_similarity(content_embedding)
                        result["rag_similarity"] = similarity
                        result["rag_enhanced"] = True
                        enhanced_results.append(result)
                    else:
                        result["rag_similarity"] = 0.0
                        result["rag_enhanced"] = False
                        enhanced_results.append(result)

                enhanced_results.sort(
                    key=lambda x: x.get("rag_similarity", 0), reverse=True
                )

                await emit_status(f"âœ… RAGä¼˜åŒ–å®Œæˆ")
                return enhanced_results

            except Exception as e:
                debug_log("RAGä¼˜åŒ–å¤±è´¥", e)
                return results

        async def rerank_results(results: List[Dict]) -> List[Dict]:
            if (
                not self.valves.ENABLE_SEMANTIC_RERANK
                or not results
                or not self.valves.BOCHA_API_KEY
            ):
                return results

            try:
                await emit_status(f"ğŸ¯ è¯­ä¹‰é‡æ’åº ({len(results)} ä¸ªç»“æœ)")

                # æ„é€  documents æ—¶å»ºç«‹æ˜ å°„
                documents = []
                doc_to_result_idx = []

                for i, result in enumerate(results):
                    content = (result.get("content") or "")[:4000]
                    if content:
                        documents.append(content)
                        doc_to_result_idx.append(i)

                if not documents:
                    return results

                headers = {
                    "Authorization": f"Bearer {self.valves.BOCHA_API_KEY}",
                    "Content-Type": "application/json",
                }

                payload = {
                    "model": self.valves.RERANK_MODEL,
                    "query": query,
                    "documents": documents,
                    "top_n": min(self.valves.RERANK_TOP_N, len(documents)),
                    "return_documents": False,
                }

                response = requests.post(
                    self.valves.RERANK_ENDPOINT,
                    headers=headers,
                    json=payload,
                    timeout=60,
                )
                response.raise_for_status()
                data = response.json()

                if "data" in data and "results" in data["data"]:
                    rerank_results_data = data["data"]["results"]
                    reranked_results = []

                    # å›å¡«æ—¶ä½¿ç”¨æ˜ å°„
                    for rerank_item in rerank_results_data:
                        doc_idx = rerank_item.get("index", 0)
                        relevance_score = rerank_item.get("relevance_score", 0.0)

                        if 0 <= doc_idx < len(doc_to_result_idx):
                            orig_idx = doc_to_result_idx[doc_idx]
                            result = results[orig_idx].copy()
                            result["rerank_score"] = relevance_score
                            result["rerank_enhanced"] = True
                            reranked_results.append(result)

                    await emit_status(f"âœ… è¯­ä¹‰é‡æ’åºå®Œæˆ")
                    return reranked_results
                else:
                    return results

            except Exception as e:
                debug_log("è¯­ä¹‰é‡æ’åºå¤±è´¥", e)
                return results

        try:
            await emit_status(f"ğŸ” è‹±æ–‡ç½‘é¡µæœç´¢: {query}")

            headers = {
                "Authorization": f"Bearer {self.valves.LANGSEARCH_API_KEY}",
                "Content-Type": "application/json",
            }

            payload = {
                "query": query,
                "freshness": self.valves.FRESHNESS,
                "summary": True,
                "count": self.valves.ENGLISH_SEARCH_COUNT,
            }

            await emit_status("â³ è¿æ¥è‹±æ–‡æœç´¢æœåŠ¡å™¨...")

            resp = requests.post(
                self.valves.ENGLISH_WEB_SEARCH_ENDPOINT,
                headers=headers,
                json=payload,
                timeout=120,
            )
            resp.raise_for_status()
            data = resp.json()

            source_context_list = []
            len_raw = 0

            if "data" in data and "webPages" in data["data"]:
                web_pages = data["data"]["webPages"]
                if "value" in web_pages and isinstance(web_pages["value"], list):
                    len_raw = len(web_pages["value"])
                    await emit_status(f"ğŸ“„ å¤„ç† {len_raw} ä¸ªè‹±æ–‡ç»“æœ...")

                    for i, item in enumerate(web_pages["value"]):
                        content = item.get("summary", "") or item.get("snippet", "")
                        if not content:
                            continue

                        result_item = {
                            "content": content,
                            "title": item.get("name", ""),
                            "url": item.get("url", ""),
                            "site_name": item.get("siteName", ""),
                            "date_published": item.get("datePublished", ""),
                            "source_type": "ä¸“ä¸šè‹±æ–‡ç½‘é¡µ",
                        }
                        source_context_list.append(result_item)

            if self.valves.ENABLE_RAG_ENHANCEMENT:
                source_context_list = await enhance_results_with_rag(
                    source_context_list
                )

            if self.valves.ENABLE_SEMANTIC_RERANK:
                source_context_list = await rerank_results(source_context_list)

            # è¯„åˆ†å½’ä¸€åŒ–ä¸é¡ºåº
            def zminmax(vals):
                if not vals:
                    return []
                lo, hi = min(vals), max(vals)
                if hi - lo < 1e-6:
                    return [0.5 for _ in vals]
                return [(v - lo) / (hi - lo) for v in vals]

            if source_context_list:
                rags = [float(x.get("rag_similarity", 0)) for x in source_context_list]
                rers = [float(x.get("rerank_score", 0)) for x in source_context_list]

                rags_n, rers_n = map(zminmax, (rags, rers))

                for i, s in enumerate(source_context_list):
                    s["final_score"] = (
                        self.valves.RAG_WEIGHT * rags_n[i]
                        + self.valves.RERANK_WEIGHT * rers_n[i]
                    )

                # å…ˆæ’åºå–å‰Nï¼Œå†æŒ‰é˜ˆå€¼åšè½»è¿‡æ»¤
                source_context_list.sort(
                    key=lambda x: x.get("final_score", 0), reverse=True
                )
                source_context_list = source_context_list[: self.valves.RERANK_TOP_N]

                if (
                    self.valves.ENABLE_RAG_ENHANCEMENT
                    and self.valves.EMIT_ONLY_RAG_PASS
                ):
                    thr = max(
                        0.03, float(self.valves.SIMILARITY_THRESHOLD) * 0.6
                    )  # æ”¾å®½
                    source_context_list = [
                        s
                        for s in source_context_list
                        if float(s.get("rag_similarity", 0)) >= thr
                        or s["final_score"] >= 0.35
                    ]

            for idx, r in enumerate(source_context_list):
                await emit_citation_data(r, __event_emitter__, run_id, idx)

            await emit_status(
                status="complete",
                description=f"ğŸ‰ è‹±æ–‡ç½‘é¡µæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(source_context_list)} ä¸ªç»“æœ",
                done=True,
            )

            results_data = []
            for r in source_context_list:
                result_item = {
                    "title": (r.get("title") or ""),
                    "url": r.get("url"),
                    "rag_similarity": float(r.get("rag_similarity") or 0.0),
                    "rerank_score": float(r.get("rerank_score") or 0.0),
                    "final_score": float(r.get("final_score") or 0.0),
                    "snippet": take_text(r.get("content", ""), 450),
                }
                if self.valves.RETURN_CONTENT_IN_RESULTS:
                    result_item["content"] = take_text(
                        r.get("content", ""), self.valves.RETURN_CONTENT_MAX_CHARS
                    )
                results_data.append(result_item)

            return json.dumps(
                {
                    "summary": {
                        "rag_threshold": self.valves.SIMILARITY_THRESHOLD,
                        "kept": len(source_context_list),
                        "total": len_raw,
                        "search_type": "ğŸŒ ä¸“ä¸šè‹±æ–‡ç½‘é¡µ",
                    },
                    "results": results_data,
                },
                ensure_ascii=False,
                indent=2,
            )

        except Exception as e:
            debug_log("è‹±æ–‡ç½‘é¡µæœç´¢å¤±è´¥", e)
            error_details = {
                "error": str(e),
                "type": "âŒ è‹±æ–‡ç½‘é¡µæœç´¢é”™è¯¯",
                "debug_info": (
                    traceback.format_exc() if self.valves.DEBUG_MODE else None
                ),
            }
            await emit_status(
                status="error", description=f"âŒ æœç´¢å‡ºé”™: {str(e)}", done=True
            )
            return json.dumps(error_details, ensure_ascii=False, indent=2)

    # ======================== æ™ºèƒ½ç½‘é¡µè¯»å–åŠŸèƒ½ï¼ˆä¿®å¤ç‰ˆï¼‰ ========================
    async def web_scrape(
        self,
        urls: List[str],
        user_request: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ æ™ºèƒ½ç½‘é¡µè¯»å–å·¥å…· (ä¿®å¤ç‰ˆ)"""

        # === è¡¨æ ¼æ‰å¹³åŒ–å·¥å…·å‡½æ•° ===
        def _flatten_md_tables(text: str) -> str:
            """å°†Markdownè¡¨æ ¼è½¬ä¸ºæ¡ç›®åˆ—è¡¨"""
            lines = text.splitlines()
            out = []
            i = 0
            while i < len(lines):
                line = lines[i]
                if line.strip().startswith("|") and "|" in line:
                    # æ”¶é›†æ•´å—è¡¨æ ¼
                    tbl = [line]
                    i += 1
                    while i < len(lines) and (
                        lines[i].strip().startswith("|")
                        or re.match(r"^\s*[:\-\|\s]+$", lines[i])
                    ):
                        tbl.append(lines[i])
                        i += 1

                    # è§£æè¡¨å¤´å’Œè¡Œ
                    if len(tbl) >= 3:  # è‡³å°‘è¦æœ‰è¡¨å¤´ã€åˆ†å‰²çº¿ã€æ•°æ®è¡Œ
                        header = [h.strip() for h in tbl[0].strip("| ").split("|")]
                        for r in tbl[2:]:  # è·³è¿‡å¯¹é½è¡Œ
                            cells = [c.strip() for c in r.strip("| ").split("|")]
                            if len(cells) == len(header):
                                # è½¬ä¸ºè¦ç‚¹è¡Œ
                                kv = [
                                    f"{header[j]}ï¼š{cells[j]}"
                                    for j in range(len(header))
                                    if cells[j]
                                ]
                                if kv:
                                    out.append("â€¢ " + "ï¼›".join(kv))
                    else:
                        # è¡¨æ ¼æ ¼å¼ä¸å®Œæ•´ï¼Œä¿æŒåŸæ ·
                        out.extend(tbl)
                else:
                    out.append(line)
                    i += 1
            return "\n".join(out)

        # === å†…åµŒè¯­ä¹‰å®‰å…¨åˆ†ç‰‡å·¥å…·å‡½æ•° ===
        def _protect_blocks_and_links(text: str):
            """ä¿æŠ¤ä»£ç å—ã€è¡¨æ ¼ã€é“¾æ¥"""
            holders = {"code": {}, "tables": {}, "md": {}, "url": {}}

            if self.valves.PRESERVE_CODEBLOCKS:
                code_pat = re.compile(r"```.*?```", re.S)

                def _code_sub(m):
                    key = f"âŸ¦CODE{len(holders['code'])}âŸ§"
                    holders["code"][key] = m.group(0)
                    return key

                text = code_pat.sub(_code_sub, text)

            if self.valves.PRESERVE_TABLES and not self.valves.FLATTEN_TABLES:
                lines = text.splitlines()
                out, i = [], 0
                while i < len(lines):
                    line = lines[i]
                    if line.strip().startswith("|") and "|" in line:
                        start = i
                        i += 1
                        while i < len(lines) and (
                            lines[i].strip().startswith("|")
                            or re.match(r"^\s*[:\-\|\s]+$", lines[i])
                        ):
                            i += 1
                        block = "\n".join(lines[start:i])
                        key = f"âŸ¦TBL{len(holders['tables'])}âŸ§"
                        holders["tables"][key] = block
                        out.append(key)
                    else:
                        out.append(line)
                        i += 1
                text = "\n".join(out)

            if self.valves.PRESERVE_LINKS:

                def _md_sub(m):
                    key = f"âŸ¦MD{len(holders['md'])}âŸ§"
                    holders["md"][key] = m.group(0)
                    return key

                text = re.sub(r"\[[^\]]+\]\([^)]+\)", _md_sub, text)

                def _url_sub(m):
                    key = f"âŸ¦URL{len(holders['url'])}âŸ§"
                    holders["url"][key] = m.group(0)
                    return key

                text = re.sub(r"https?://[^\s\)\]]+", _url_sub, text)

            return text, holders

        def _restore_placeholders(text: str, holders: dict) -> str:
            """æ¢å¤å ä½ç¬¦"""
            for k, v in holders.get("url", {}).items():
                text = text.replace(k, v)
            for k, v in holders.get("md", {}).items():
                text = text.replace(k, v)
            for k, v in holders.get("tables", {}).items():
                text = text.replace(k, v)
            for k, v in holders.get("code", {}).items():
                text = text.replace(k, v)
            return text

        def _split_sentences_zh_en(text: str) -> List[str]:
            """ä¸­è‹±æ··åˆå¥å­åˆ‡åˆ†"""
            text = re.sub(r"\n{3,}", "\n\n", text)
            text = re.sub(r"[ \t]{2,}", " ", text)
            text = text.replace("\n\n", "âŸ¦PARAâŸ§")
            text = re.sub(r"([ã€‚ï¼ï¼Ÿï¼›â€¦])", r"\1âŸ¦SPLITâŸ§", text)
            text = re.sub(r"([.!?;])(\s+)(?=[A-Z0-9\"'])", r"\1âŸ¦SPLITâŸ§", text)
            text = text.replace("âŸ¦PARAâŸ§", "âŸ¦SPLITâŸ§")

            parts = [p.strip() for p in text.split("âŸ¦SPLITâŸ§") if p.strip()]

            if self.valves.DENOISE_LINK_SECTIONS:
                cleaned = []
                for s in parts:
                    tokens = s.split()
                    link_like = sum(
                        1
                        for t in tokens
                        if t.startswith("http")
                        or t.startswith("âŸ¦URL")
                        or t.startswith("âŸ¦MD")
                    )
                    if len(tokens) <= 4 and link_like >= max(2, int(len(tokens) * 0.8)):
                        continue
                    cleaned.append(s)
                parts = cleaned

            return parts

        def _pack_sentences_to_chunks(sentences: List[str]) -> List[dict]:
            """å°†å¥å­æ‰“åŒ…æˆåˆ†ç‰‡ï¼ˆä¿®å¤é‡å é€»è¾‘ï¼‰"""
            tgt = max(800, int(self.valves.TARGET_CHUNK_CHARS))
            hard = max(tgt, int(self.valves.MAX_CHUNK_CHARS))
            ovl = max(0, int(self.valves.OVERLAP_SENTENCES))

            chunks = []
            i = 0

            while i < len(sentences) and len(chunks) < int(
                self.valves.MAX_TOTAL_CHUNKS
            ):
                buf, size, start = [], 0, i

                while i < len(sentences):
                    s = sentences[i]
                    s_len = len(s) + 1

                    if size + s_len > tgt:
                        if not buf:
                            s_cut = s[:hard]
                            buf.append(s_cut)
                            sentences[i] = s[hard:]
                        break

                    buf.append(s)
                    size += s_len
                    i += 1

                if buf:
                    text = " ".join(buf).strip()
                    end = start + len(buf) - 1
                    chunks.append({"text": text, "start_sent": start, "end_sent": end})

                    # ä¿®å¤åˆ†ç‰‡é‡å é€»è¾‘ï¼šæ­£ç¡®çš„å›é€€
                    if i < len(sentences):  # åªæœ‰åœ¨è¿˜æœ‰å‰©ä½™å¥å­æ—¶æ‰å›é€€
                        i = max(end - ovl + 1, start + 1)  # ç¡®ä¿è‡³å°‘å‰è¿›ä¸€ä¸ªå¥å­
                else:
                    i += 1

            return chunks

        def smart_segment_text(raw_text: str) -> List[dict]:
            """è¯­ä¹‰å®‰å…¨åˆ†ç‰‡å…¥å£"""
            protected, holders = _protect_blocks_and_links(raw_text)
            sentences = _split_sentences_zh_en(protected)
            chunks = _pack_sentences_to_chunks(sentences)

            for c in chunks:
                c["text"] = _restore_placeholders(c["text"], holders)

            return chunks

        # === å…¶ä»–å·¥å…·å‡½æ•° ===
        def get_segmenter_client():
            api_key = self.valves.SEGMENTER_API_KEY or self.valves.MOONSHOT_API_KEY
            base_url = self.valves.SEGMENTER_BASE_URL or self.valves.MOONSHOT_BASE_URL

            if not api_key:
                raise ValueError("LLMéœ€è¦APIå¯†é’¥")

            if (
                self.segmenter_client is None
                or self.segmenter_client.api_key != api_key
                or getattr(self.segmenter_client, "base_url_stored", None) != base_url
            ):
                self.segmenter_client = OpenAI(base_url=base_url, api_key=api_key)
                self.segmenter_client.base_url_stored = base_url

            return self.segmenter_client

        # LLMè°ƒç”¨ç¡¬è¶…æ—¶
        async def llm_call(
            messages: list, temperature: float = None, max_tokens: int = 4000
        ) -> str:
            """è°ƒç”¨LLMï¼ˆä¿®å¤ç‰ˆï¼šé‡è¯•+çº¿ç¨‹æ± +è¶…æ—¶ï¼‰"""
            client = get_segmenter_client()
            temp = (
                temperature
                if temperature is not None
                else self.valves.SUMMARY_TEMPERATURE
            )

            last_err = None
            for attempt in range(self.valves.LLM_RETRIES + 1):
                try:
                    # æ·»åŠ ç¡¬è¶…æ—¶ä¿æŠ¤
                    resp = await asyncio.wait_for(
                        asyncio.to_thread(
                            client.chat.completions.create,
                            model=self.valves.SEGMENTER_MODEL,
                            messages=messages,
                            temperature=temp,
                            max_tokens=max_tokens,
                        ),
                        timeout=self.valves.LLM_REQUEST_TIMEOUT_SEC,
                    )
                    return resp.choices[0].message.content

                except Exception as e:
                    last_err = e
                    if attempt < self.valves.LLM_RETRIES:
                        await asyncio.sleep(
                            self.valves.LLM_BACKOFF_BASE_SEC * (attempt + 1)
                        )

            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {last_err}")

        def is_wikipedia(u: str) -> bool:
            try:
                return "wikipedia.org" in (urlparse(u).netloc or "").lower()
            except Exception:
                return False

        def next_run_id(tool: str) -> str:
            self.run_seq += 1
            return f"{tool}-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{self.run_seq}"

        def take_text(text: str, max_chars: int) -> str:
            if text is None:
                return ""
            if len(text) <= max_chars or max_chars <= 0:
                return text
            cut = text[:max_chars]
            # å°è¯•åœ¨æœ€è¿‘çš„å¥è¯»ç¬¦å¤„æˆªæ–­
            p = max(
                cut.rfind("ã€‚"),
                cut.rfind("ï¼"),
                cut.rfind("ï¼Ÿ"),
                cut.rfind("."),
                cut.rfind(";"),
            )
            if p >= max_chars * 0.6:  # ä»…åœ¨è¾ƒé åæ‰ä½¿ç”¨
                return cut[: p + 1] + " â€¦"
            return cut + " â€¦"

        def split_text_chunks(text: str, size: int) -> List[str]:
            if text is None:
                return [""]
            if size is None or size <= 0:
                return [text]
            return [text[i : i + size] for i in range(0, len(text), size)]

        async def emit_citation_data(r: Dict, __event_emitter__, run_id: str, idx: int):
            if not (__event_emitter__ and self.valves.CITATION_LINKS):
                return

            full_doc = r.get("content") or ""
            doc_for_emit = take_text(full_doc, self.valves.CITATION_DOC_MAX_CHARS)
            chunks = split_text_chunks(doc_for_emit, self.valves.CITATION_CHUNK_SIZE)
            base_title = (r.get("title") or "") or (r.get("url") or "Source")
            base_url = (r.get("url") or "").strip()

            for ci, chunk in enumerate(chunks, 1):
                if self.valves.UNIQUE_REFERENCE_NAMES:
                    src_name = f"{base_title} | {base_url} | {run_id}#{idx}-{ci}-{uuid4().hex[:6]}"
                else:
                    src_name = base_url or base_title

                payload = {
                    "type": "citation",
                    "data": {
                        "document": [chunk],
                        "metadata": [
                            {
                                "title": base_title,
                                "date_accessed": datetime.now().isoformat(),
                            }
                        ],
                        "source": {
                            "name": src_name,
                            "url": base_url or "",
                            "type": r.get("source_type", "webpage"),
                        },
                    },
                }
                await __event_emitter__(payload)

                if self.valves.PERSIST_CITATIONS:
                    self.citations_history.append(payload)
                    if len(self.citations_history) > self.valves.PERSIST_CITATIONS_MAX:
                        self.citations_history = self.citations_history[
                            -self.valves.PERSIST_CITATIONS_MAX :
                        ]

        # è¿›åº¦æ¡ç®¡ç†å™¨
        class ProgressManager:
            def __init__(self, total_steps: int):
                self.total_steps = total_steps
                self.current_step = 0

            async def update_step(self, description: str, __event_emitter__):
                self.current_step += 1
                percentage = int((self.current_step / self.total_steps) * 100)
                if __event_emitter__:
                    await __event_emitter__(
                        {
                            "type": "status",
                            "data": {
                                "status": "in_progress",
                                "description": f"[{percentage}%] {description}",
                                "done": False,
                                "progress": percentage,
                                "step": self.current_step,
                                "total_steps": self.total_steps,
                                "action": "web_scrape",
                            },
                        }
                    )

        # ä¿®å¤ç‰ˆæ‘˜è¦æå–å‡½æ•°
        async def extract_summaries_fixed(
            content: str,
            user_request: str,
            url: str,
            page_title: str,
            progress_mgr: ProgressManager,
        ) -> List[Dict]:
            """ä¿®å¤ç‰ˆæ‘˜è¦æå–ï¼šè§£å†³9ä¸ªåˆ†ç‰‡åªè¿”å›1ä¸ªç»“æœçš„é—®é¢˜"""

            def cleanup(text: str) -> str:
                t = re.sub(r"\n{4,}", "\n\n", text)
                t = re.sub(r"[ \t]{3,}", " ", t)
                t = re.sub(r"\[\d+\]", "", t)
                return t.strip()

            cleaned = cleanup(content)
            if not cleaned:
                return []

            # è¡¨æ ¼æ‰å¹³åŒ–
            if self.valves.FLATTEN_TABLES:
                cleaned = _flatten_md_tables(cleaned)

            chunks = smart_segment_text(cleaned)
            if not chunks:
                return []

            if len(chunks) > int(self.valves.MAX_TOTAL_CHUNKS):
                chunks = chunks[: int(self.valves.MAX_TOTAL_CHUNKS)]

            debug_log(f"åˆ†ç‰‡å®Œæˆï¼š{len(chunks)} ç‰‡")

            await progress_mgr.update_step(
                f"ğŸ“„ å¼€å§‹å¤„ç† {len(chunks)} ä¸ªåˆ†ç‰‡", __event_emitter__
            )

            # å¹¶å‘æ§åˆ¶
            sem = asyncio.Semaphore(self.valves.LLM_MAX_CONCURRENCY)
            per_chunk = max(2, min(4, int(self.valves.MAP_SUMMARY_PER_CHUNK)))

            def _extract_json_array(text: str, debug_chunk_idx: int = -1) -> List[dict]:
                """å¢å¼ºçš„JSONæ•°ç»„æå–ï¼Œå¸¦è°ƒè¯•ä¿¡æ¯"""
                if not text:
                    debug_log(f"åˆ†ç‰‡{debug_chunk_idx} JSONæå–ï¼šè¾“å…¥ä¸ºç©º")
                    return []

                t = text.strip()
                debug_log(f"åˆ†ç‰‡{debug_chunk_idx} LLMåŸå§‹å“åº”: {t[:200]}...")

                # æ¸…ç†ä»£ç å—æ ‡è®°
                if t.startswith("```"):
                    t = re.sub(r"^```(?:json)?|```$", "", t, flags=re.I | re.M).strip()

                # å°è¯•å®Œæ•´JSONè§£æ
                try:
                    obj = json.loads(t)
                    result = obj if isinstance(obj, list) else []
                    debug_log(
                        f"åˆ†ç‰‡{debug_chunk_idx} å®Œæ•´JSONè§£ææˆåŠŸï¼Œå¾—åˆ°{len(result)}ä¸ªé¡¹ç›®"
                    )
                    return result
                except Exception as e1:
                    debug_log(f"åˆ†ç‰‡{debug_chunk_idx} å®Œæ•´JSONè§£æå¤±è´¥: {e1}")

                # å°è¯•æå–JSONæ•°ç»„éƒ¨åˆ†
                s, e = t.find("["), t.rfind("]")
                if s != -1 and e != -1 and e > s:
                    try:
                        obj = json.loads(t[s : e + 1])
                        result = obj if isinstance(obj, list) else []
                        debug_log(
                            f"åˆ†ç‰‡{debug_chunk_idx} éƒ¨åˆ†JSONè§£ææˆåŠŸï¼Œå¾—åˆ°{len(result)}ä¸ªé¡¹ç›®"
                        )
                        return result
                    except Exception as e2:
                        debug_log(f"åˆ†ç‰‡{debug_chunk_idx} éƒ¨åˆ†JSONè§£æä¹Ÿå¤±è´¥: {e2}")

                debug_log(f"åˆ†ç‰‡{debug_chunk_idx} æ‰€æœ‰JSONè§£æéƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                return []

            # æœ¬åœ°å…œåº•å¡«å……å™¨
            STOPWORDS = set(
                list("çš„ä¸€æ˜¯åœ¨ä¸äº†æœ‰å’Œå°±ä¹Ÿè€ŒåŠä¸æˆ–è¢«äºæŠŠç­‰å…¶å¹¶ä¹‹ä¹‹äºä»¥ä¸º")
            ) | {
                "the",
                "a",
                "an",
                "and",
                "or",
                "of",
                "to",
                "in",
                "on",
                "for",
                "as",
                "with",
                "by",
                "is",
                "are",
                "was",
                "were",
            }

            def derive_key_points(text: str, topk=4):
                """æœ´ç´ è¯é¢‘å…³é”®è¯"""
                tokens = re.findall(r"[\u4e00-\u9fff]{2,}|[A-Za-z]{3,}", text)
                cnt = {}
                for t in tokens:
                    if t.lower() in STOPWORDS:
                        continue
                    cnt[t] = cnt.get(t, 0) + 1
                return [w for w, _ in sorted(cnt.items(), key=lambda x: -x[1])[:topk]]

            async def _extract_one_chunk(idx: int, c: dict):
                """å•ä¸ªåˆ†ç‰‡çš„æ‘˜è¦æå– - ä¿®å¤ç‰ˆ"""
                # æ›´æ¸…æ™°çš„ç³»ç»Ÿæç¤ºï¼Œå¼ºè°ƒè¾“å‡ºæ ¼å¼
                sys_prompt = f"""ä½ æ˜¯ä¸“ä¸šä¿¡æ¯æå–ä¸“å®¶ã€‚åŸºäºç»™å®šå†…å®¹ç‰‡æ®µï¼Œå›´ç»•ç”¨æˆ·éœ€æ±‚æå–{per_chunk}æ¡æ‘˜è¦ã€‚

**é‡è¦è¦æ±‚ï¼š**
1. å¿…é¡»è¾“å‡ºJSONæ•°ç»„æ ¼å¼ï¼š[{{"summary": "æ‘˜è¦å†…å®¹", "relevance": 0.8}}]
2. æ¯æ¡æ‘˜è¦æ§åˆ¶åœ¨{self.valves.SUMMARY_MIN_CHARS}-{self.valves.SUMMARY_MAX_CHARS}ä¸ªå­—ç¬¦
3. æ‘˜è¦è¦å®Œæ•´è¡¨è¾¾ä¸€ä¸ªè¦ç‚¹ï¼Œè¯­å¥å®Œæ•´é€šé¡º
4. relevanceä¸º0-1çš„ç›¸å…³åº¦åˆ†æ•°
5. å¦‚æœå†…å®¹ä¸ç›¸å…³æˆ–æ— æ³•æå–ï¼Œè¿”å›[]

**ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦ä»»ä½•é¢å¤–è¯´æ˜ã€‚**"""

                user_prompt = f"""ç”¨æˆ·éœ€æ±‚ï¼š{user_request}

åˆ†ç‰‡å†…å®¹ï¼š
{c['text'][:4000]}

è¯·ä¸¥æ ¼æŒ‰JSONæ•°ç»„æ ¼å¼è¾“å‡ºï¼š"""

                try:
                    async with sem:
                        resp = await llm_call(
                            [
                                {"role": "system", "content": sys_prompt},
                                {"role": "user", "content": user_prompt},
                            ],
                            temperature=self.valves.SUMMARY_TEMPERATURE,
                            max_tokens=3000,
                        )

                    # å¢å¼ºçš„JSONè§£æï¼Œå¸¦è°ƒè¯•ä¿¡æ¯
                    arr = _extract_json_array(resp, idx)
                    out = []

                    for item_idx, item in enumerate(arr):
                        if not isinstance(item, dict):
                            debug_log(
                                f"åˆ†ç‰‡{idx} ç¬¬{item_idx}é¡¹ä¸æ˜¯å­—å…¸ï¼Œè·³è¿‡: {type(item)}"
                            )
                            continue

                        s = (item.get("summary") or "").strip()
                        if not s:
                            debug_log(f"åˆ†ç‰‡{idx} ç¬¬{item_idx}é¡¹æ‘˜è¦ä¸ºç©ºï¼Œè·³è¿‡")
                            continue

                        # ä¿®å¤ï¼šæ›´å®½æ¾çš„å­—æ•°æ£€æŸ¥ï¼Œé¿å…è¿‡åº¦è¿‡æ»¤
                        if (
                            len(s) < int(self.valves.SUMMARY_MIN_CHARS) * 0.7
                        ):  # å…è®¸30%çš„å¼¹æ€§
                            debug_log(
                                f"åˆ†ç‰‡{idx} ç¬¬{item_idx}é¡¹è¿‡çŸ­({len(s)}å­—ç¬¦)ï¼Œè·³è¿‡"
                            )
                            continue

                        # æ¸©å’Œçš„é•¿åº¦å¤„ç†
                        if len(s) > int(self.valves.SUMMARY_MAX_CHARS):
                            cut_pos = int(self.valves.SUMMARY_MAX_CHARS)
                            sentence_end = max(
                                s.rfind("ã€‚", 0, cut_pos),
                                s.rfind("ï¼", 0, cut_pos),
                                s.rfind("ï¼Ÿ", 0, cut_pos),
                                s.rfind(".", 0, cut_pos),
                            )
                            if sentence_end > cut_pos * 0.7:
                                s = s[: sentence_end + 1]
                                debug_log(
                                    f"åˆ†ç‰‡{idx} ç¬¬{item_idx}é¡¹åœ¨å¥å­è¾¹ç•Œæˆªæ–­ä¸º{len(s)}å­—ç¬¦"
                                )
                            else:
                                s = s[:cut_pos] + "..."
                                debug_log(
                                    f"åˆ†ç‰‡{idx} ç¬¬{item_idx}é¡¹å¼ºåˆ¶æˆªæ–­ä¸º{len(s)}å­—ç¬¦"
                                )

                        # å…œåº•å¡«å……
                        kp = derive_key_points(s)
                        out.append(
                            {
                                "content": s,
                                "title": f"{page_title} Â· æ‘˜è¦",
                                "url": url,
                                "relevance": float(item.get("relevance", 0.7)),
                                "key_points": kp,
                                "extract_method": "fixed_concurrent",
                                "source_type": "LLMæ™ºèƒ½æ‘˜è¦",
                                "chunk_index": idx,
                            }
                        )

                    debug_log(f"åˆ†ç‰‡{idx} æˆåŠŸæå–{len(out)}æ¡æ‘˜è¦")
                    return out

                except Exception as e:
                    debug_log(f"åˆ†ç‰‡ {idx+1} æ‘˜è¦æå–å¼‚å¸¸ï¼š{e}")
                    # å¦‚æœJSONè§£æå®Œå…¨å¤±è´¥ï¼Œå°è¯•åŸºäºåŸå§‹å“åº”åˆ›å»ºæ‘˜è¦
                    try:
                        # å°†LLMå“åº”ä½œä¸ºå•æ¡æ‘˜è¦å¤„ç†
                        if (
                            resp
                            and len(resp.strip())
                            >= int(self.valves.SUMMARY_MIN_CHARS) * 0.5
                        ):
                            content = resp.strip()
                            if len(content) > int(self.valves.SUMMARY_MAX_CHARS):
                                cut_pos = int(self.valves.SUMMARY_MAX_CHARS)
                                sentence_end = max(
                                    content.rfind("ã€‚", 0, cut_pos),
                                    content.rfind(".", 0, cut_pos),
                                )
                                if sentence_end > cut_pos * 0.7:
                                    content = content[: sentence_end + 1]
                                else:
                                    content = content[:cut_pos] + "..."

                            fallback_item = {
                                "content": content,
                                "title": f"{page_title} Â· æ‘˜è¦",
                                "url": url,
                                "relevance": 0.6,
                                "key_points": derive_key_points(content),
                                "extract_method": "fallback_from_response",
                                "source_type": "LLMå“åº”å›é€€",
                                "chunk_index": idx,
                            }
                            debug_log(f"åˆ†ç‰‡{idx} ä½¿ç”¨å“åº”å›é€€åˆ›å»º1æ¡æ‘˜è¦")
                            return [fallback_item]
                    except:
                        pass

                    return []

            # å¹¶å‘æ‰§è¡Œï¼Œè¿½è¸ªæ¯ä¸ªåˆ†ç‰‡ç»“æœ
            tasks = [_extract_one_chunk(idx, c) for idx, c in enumerate(chunks)]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            all_summaries = []
            successful_chunks = 0
            failed_chunks = 0

            for i, r in enumerate(results):
                if isinstance(r, list):
                    all_summaries.extend(r)
                    if r:  # æœ‰ç»“æœ
                        successful_chunks += 1
                        debug_log(f"åˆ†ç‰‡{i}æˆåŠŸæå–{len(r)}æ¡æ‘˜è¦")
                    else:  # ç©ºç»“æœ
                        failed_chunks += 1
                        debug_log(f"åˆ†ç‰‡{i}æå–ç»“æœä¸ºç©º")
                else:
                    failed_chunks += 1
                    debug_log(f"åˆ†ç‰‡{i}å‡ºç°å¼‚å¸¸: {r}")

            debug_log(
                f"å¹¶å‘æ‘˜è¦æå–å®Œæˆï¼šæˆåŠŸ{successful_chunks}ä¸ªåˆ†ç‰‡ï¼Œå¤±è´¥{failed_chunks}ä¸ªåˆ†ç‰‡ï¼Œæ€»æ‘˜è¦{len(all_summaries)}æ¡"
            )

            # å¦‚æœæå–æ•ˆæœå¤ªå·®ï¼Œå¯åŠ¨å¼ºåŒ–å›é€€
            if len(all_summaries) < max(2, len(chunks) * 0.3):  # å¦‚æœæ‘˜è¦æ•°é‡å¤ªå°‘
                debug_log(f"æ‘˜è¦æå–æ•ˆæœä¸ä½³ï¼ˆ{len(all_summaries)}æ¡ï¼‰ï¼Œå¯åŠ¨å¼ºåŒ–å›é€€")
                for i, chunk in enumerate(chunks[:5]):  # æœ€å¤šå¤„ç†5ä¸ªåˆ†ç‰‡
                    try:
                        # ç›´æ¥å°†åˆ†ç‰‡å†…å®¹ä½œä¸ºæ‘˜è¦ï¼Œæ¸©å’Œå¤„ç†é•¿åº¦
                        fallback_content = chunk["text"]
                        if len(fallback_content) > int(self.valves.SUMMARY_MAX_CHARS):
                            cut_pos = int(self.valves.SUMMARY_MAX_CHARS)
                            sentence_end = max(
                                fallback_content.rfind("ã€‚", 0, cut_pos),
                                fallback_content.rfind(".", 0, cut_pos),
                            )
                            if sentence_end > cut_pos * 0.7:
                                fallback_content = fallback_content[: sentence_end + 1]
                            else:
                                fallback_content = fallback_content[:cut_pos] + "..."

                        fallback_summary = {
                            "content": fallback_content,
                            "title": f"{page_title} Â· åˆ†ç‰‡æ‘˜è¦{i+1}",
                            "url": url,
                            "relevance": 0.6,
                            "key_points": derive_key_points(fallback_content),
                            "extract_method": "enhanced_fallback",
                            "source_type": "å¼ºåŒ–å›é€€æ‘˜è¦",
                            "chunk_index": i,
                        }
                        all_summaries.append(fallback_summary)
                    except Exception as e:
                        debug_log(f"å¼ºåŒ–å›é€€åˆ†ç‰‡{i}ä¹Ÿå¤±è´¥: {e}")

            await progress_mgr.update_step(
                f"âœ… æ‘˜è¦æå–å®Œæˆï¼Œè·å¾— {len(all_summaries)} æ¡æ‘˜è¦", __event_emitter__
            )

            debug_log(f"æœ€ç»ˆæ‘˜è¦æå–å®Œæˆï¼šå…± {len(all_summaries)} æ¡")
            return all_summaries

        # RAGå‡½æ•°
        async def batch_embeddings(texts: List[str]) -> List[Optional[List[float]]]:
            if not texts:
                return []
            if not (self.valves.ENABLE_RAG_ENHANCEMENT and self.valves.ARK_API_KEY):
                return [None for _ in texts]

            headers = {
                "Authorization": f"Bearer {self.valves.ARK_API_KEY}",
                "Content-Type": "application/json",
            }

            try:
                payload = {
                    "model": self.valves.EMBEDDING_MODEL,
                    "input": [t[:4000] for t in texts],
                    "encoding_format": "float",
                }

                resp = await asyncio.to_thread(
                    requests.post,
                    f"{self.valves.EMBEDDING_BASE_URL}/embeddings",
                    headers=headers,
                    json=payload,
                    timeout=60,
                )
                resp.raise_for_status()
                data = resp.json()

                vecs = [d["embedding"] for d in data.get("data", [])]
                if len(vecs) != len(texts):
                    debug_log("æ‰¹é‡å‘é‡åŒ–é•¿åº¦ä¸åŒ¹é…ï¼Œå›é€€å•ä¸ªå¤„ç†")
                    fallback = []
                    for t in texts:
                        v = await get_single_embedding(t)
                        fallback.append(v)
                    return fallback

                return vecs

            except Exception as e:
                debug_log(f"æ‰¹é‡å‘é‡åŒ–å¤±è´¥ï¼š{e}")
                out = []
                for t in texts:
                    v = await get_single_embedding(t)
                    out.append(v)
                return out

        async def get_single_embedding(text: str) -> Optional[List[float]]:
            if not self.valves.ENABLE_RAG_ENHANCEMENT or not self.valves.ARK_API_KEY:
                return None

            text_hash = hashlib.sha256(text.encode("utf-8")).hexdigest()
            if text_hash in self.embedding_cache:
                return self.embedding_cache[text_hash]

            try:
                headers = {
                    "Authorization": f"Bearer {self.valves.ARK_API_KEY}",
                    "Content-Type": "application/json",
                }

                clean_text = text.strip()[:4000]
                if not clean_text:
                    return None

                payload = {
                    "model": self.valves.EMBEDDING_MODEL,
                    "input": [clean_text],
                    "encoding_format": "float",
                }

                response = await asyncio.to_thread(
                    requests.post,
                    f"{self.valves.EMBEDDING_BASE_URL}/embeddings",
                    headers=headers,
                    json=payload,
                    timeout=30,
                )
                response.raise_for_status()
                data = response.json()

                if "data" in data and len(data["data"]) > 0:
                    embedding = data["data"][0]["embedding"]
                    self.embedding_cache[text_hash] = embedding
                    return embedding
                else:
                    return None

            except Exception:
                return None

        def cos_similarity(a: List[float], b: List[float]) -> float:
            try:
                va = np.array(a)
                vb = np.array(b)
                return float(np.dot(va, vb) / (np.linalg.norm(va) * np.linalg.norm(vb)))
            except Exception:
                return 0.0

        # æ„å›¾æ‹†è§£ + å¤šè§†è§’ç›¸ä¼¼åº¦
        def plan_aspects(user_request: str):
            """è½»é‡è§„åˆ’å™¨ï¼šæ ¹æ®æŸ¥è¯¢æ„å›¾æ‹†è§£æœç´¢è§†è§’"""
            buckets = []
            if re.search(
                r"æ„ä¹‰|å«ä¹‰|è±¡å¾|æœ¬è´¨|å“²å­¦|meaning|significance", user_request, re.I
            ):
                buckets += [
                    "æ•°å­¦å®šä¹‰ ä¸ å•ä½å…ƒ",
                    "æ–‡åŒ–/å“²å­¦è±¡å¾",
                    "åº”ç”¨åœºæ™¯ ä¸ å½’ä¸€åŒ–/è®¡é‡",
                    "è¯­è¨€å­¦/è¯æº",
                ]
            else:
                buckets += ["æ ¸å¿ƒå®šä¹‰", "æ€§è´¨/å®šç†", "å†å²ä¸ç¬¦å·", "åº”ç”¨ä¸å·¥ç¨‹"]
            return buckets[:4]

        # å»é‡å·¥å…·
        def dedup_by_embedding(items, vecs, thr=0.88):
            """åŸºäºembeddingçš„å»é‡"""
            kept, kept_vecs = [], []
            for it, v in zip(items, vecs):
                if v is None:
                    kept.append(it)
                    kept_vecs.append(v)
                    continue
                if any(
                    cos_similarity(v, kv) >= thr for kv in kept_vecs if kv is not None
                ):
                    continue
                kept.append(it)
                kept_vecs.append(v)
            return kept

        run_id = next_run_id("web-scrape")
        if self.valves.PERSIST_CITATIONS and __event_emitter__:
            for old in self.citations_history:
                await __event_emitter__(old)

        def debug_log(message: str, error=None):
            if self.valves.DEBUG_MODE:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"[DEBUG {timestamp}] {message}")
                if error:
                    print(f"[DEBUG ERROR] {str(error)}")
                    print(f"[DEBUG TRACEBACK] {traceback.format_exc()}")

        # åˆå§‹åŒ–è¿›åº¦ç®¡ç†å™¨
        total_steps = 6  # è¯»å–ç½‘é¡µã€æ‘˜è¦æå–ã€RAGã€é‡æ’åºã€è¯„åˆ†ã€å®Œæˆ
        progress_mgr = ProgressManager(total_steps)

        try:
            debug_log(f"å¼€å§‹æ™ºèƒ½ç½‘é¡µè¯»å–ï¼ŒURLæ•°é‡: {len(urls)}")

            await progress_mgr.update_step(
                f"ğŸš€ å¼€å§‹å¤„ç† {len(urls)} ä¸ªç½‘é¡µ", __event_emitter__
            )

            async def process_url(url):
                jina_url = f"https://r.jina.ai/{url}"
                headers = {
                    "X-No-Cache": "true",
                    "X-With-Links-Summary": "true",
                    "Authorization": f"Bearer {self.valves.JINA_API_KEY}",
                }

                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.get(
                            jina_url, headers=headers, timeout=90
                        ) as response:
                            response.raise_for_status()
                            content = await response.text()

                    if not content or content.strip() == "":
                        return {
                            "content": "",
                            "title": url,
                            "url": url,
                            "error": "è¿”å›å†…å®¹ä¸ºç©º",
                            "status": "empty",
                        }

                    debug_log(f"æˆåŠŸè¯»å–URL {url}ï¼Œå†…å®¹é•¿åº¦: {len(content)}")

                    return {
                        "content": content,
                        "title": f"ç½‘é¡µå†…å®¹ - {url.split('/')[2] if '/' in url else url}",
                        "url": url,
                        "site_name": url.split("/")[2] if "/" in url else url,
                        "date_published": datetime.now().strftime("%Y-%m-%d"),
                        "source_type": "ç½‘é¡µè¯»å–",
                        "status": "success",
                    }

                except Exception as e:
                    error_message = f"è¯»å–ç½‘é¡µ {url} æ—¶å‡ºé”™: {str(e)}"
                    debug_log(f"å¤„ç†URLå¤±è´¥: {url}", e)
                    return {
                        "content": "",
                        "title": url,
                        "url": url,
                        "error": error_message,
                        "status": "error",
                    }

            tasks = [process_url(url) for url in urls]
            results = await asyncio.gather(*tasks)

            successful_results = []
            error_results = []

            for result in results:
                if result.get("status") == "success" and result.get("content"):
                    successful_results.append(result)
                else:
                    error_results.append(result)

            debug_log(
                f"å¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {len(successful_results)}, å¤±è´¥: {len(error_results)}"
            )

            await progress_mgr.update_step(
                f"ğŸ“– æˆåŠŸè¯»å– {len(successful_results)} ä¸ªç½‘é¡µ", __event_emitter__
            )

            if not successful_results:
                return json.dumps(
                    {
                        "request": user_request,
                        "error": "æ‰€æœ‰ç½‘é¡µè¯»å–éƒ½å¤±è´¥",
                        "summaries_count": 0,
                        "summaries": [],
                        "errors": error_results,
                    },
                    ensure_ascii=False,
                    indent=2,
                )

            # æ™ºèƒ½æ‘˜è¦æå–æµç¨‹
            if self.valves.ENABLE_SMART_SUMMARY:
                all_summaries = []

                for i, page in enumerate(successful_results):
                    content = page.get("content", "")
                    url = page.get("url", "")
                    title = page.get("title", "")

                    debug_log(f"ä¸ºé¡µé¢ {i+1}/{len(successful_results)} æå–æ‘˜è¦: {url}")

                    try:
                        summaries = await extract_summaries_fixed(
                            content=content,
                            user_request=user_request,
                            url=url,
                            page_title=title,
                            progress_mgr=progress_mgr,
                        )
                        debug_log(f"é¡µé¢ {url} æå–åˆ° {len(summaries)} æ¡æ‘˜è¦")
                        all_summaries.extend(summaries)
                    except Exception as e:
                        debug_log(f"é¡µé¢ {url} æ‘˜è¦æå–å¤±è´¥: {e}")

                debug_log(f"æ‰€æœ‰é¡µé¢æ‘˜è¦æå–å®Œæˆï¼Œæ€»è®¡ {len(all_summaries)} æ¡æ‘˜è¦")

                # RAGå¤„ç†
                if self.valves.ENABLE_RAG_ENHANCEMENT and all_summaries:
                    await progress_mgr.update_step(
                        f"ğŸ¯ RAGå‘é‡åŒ–å¤„ç† {len(all_summaries)} æ¡æ‘˜è¦",
                        __event_emitter__,
                    )

                    # å¤šè§†è§’ç›¸ä¼¼åº¦èåˆ
                    aspects = plan_aspects(user_request)
                    all_texts = (
                        [user_request] + aspects + [s["content"] for s in all_summaries]
                    )
                    all_vecs = await batch_embeddings(all_texts)

                    query_vec = all_vecs[0] if all_vecs else None
                    aspect_vecs = (
                        all_vecs[1 : len(aspects) + 1]
                        if len(all_vecs) > len(aspects)
                        else []
                    )
                    summary_vecs = (
                        all_vecs[len(aspects) + 1 :]
                        if len(all_vecs) > len(aspects) + 1
                        else []
                    )

                    def fuse_similarity(doc_vec):
                        """èåˆæŸ¥è¯¢å’Œå„æ–¹é¢å­æŸ¥è¯¢çš„ç›¸ä¼¼åº¦"""
                        sims = []
                        if query_vec is not None and doc_vec is not None:
                            sims.append(cos_similarity(query_vec, doc_vec))
                        for av in aspect_vecs:
                            if av is not None and doc_vec is not None:
                                sims.append(cos_similarity(av, doc_vec))
                        return max(sims) if sims else 0.0

                    for i, summary in enumerate(all_summaries):
                        if i < len(summary_vecs) and summary_vecs[i] is not None:
                            similarity = fuse_similarity(summary_vecs[i])
                            summary["rag_similarity"] = similarity
                        else:
                            summary["rag_similarity"] = summary.get("relevance", 0.6)

                    # å»é‡
                    all_summaries = dedup_by_embedding(
                        all_summaries, summary_vecs, thr=0.88
                    )

                    all_summaries.sort(
                        key=lambda x: x.get("rag_similarity", 0), reverse=True
                    )

                # è¯­ä¹‰é‡æ’åº
                if (
                    self.valves.ENABLE_SEMANTIC_RERANK
                    and self.valves.BOCHA_API_KEY
                    and all_summaries
                ):
                    await progress_mgr.update_step(
                        f"ğŸ¯ è¯­ä¹‰é‡æ’åº {len(all_summaries)} æ¡æ‘˜è¦", __event_emitter__
                    )

                    try:
                        headers = {
                            "Authorization": f"Bearer {self.valves.BOCHA_API_KEY}",
                            "Content-Type": "application/json",
                        }

                        # æ„é€  documents æ—¶å»ºç«‹æ˜ å°„
                        documents = []
                        doc_to_result_idx = []

                        for i, s in enumerate(all_summaries):
                            content = s["content"][:4000]
                            if content:
                                documents.append(content)
                                doc_to_result_idx.append(i)

                        if documents:
                            payload = {
                                "model": self.valves.RERANK_MODEL,
                                "query": user_request,
                                "documents": documents,
                                "top_n": min(self.valves.RERANK_TOP_N, len(documents)),
                                "return_documents": False,
                            }

                            resp = await asyncio.to_thread(
                                requests.post,
                                self.valves.RERANK_ENDPOINT,
                                headers=headers,
                                json=payload,
                                timeout=60,
                            )
                            resp.raise_for_status()
                            data = resp.json()

                            if "data" in data and "results" in data["data"]:
                                rerank_results_data = data["data"]["results"]
                                reranked_summaries = []

                                # å›å¡«æ—¶ä½¿ç”¨æ˜ å°„
                                for rerank_item in rerank_results_data:
                                    doc_idx = rerank_item.get("index", 0)
                                    relevance_score = rerank_item.get(
                                        "relevance_score", 0.0
                                    )

                                    if 0 <= doc_idx < len(doc_to_result_idx):
                                        orig_idx = doc_to_result_idx[doc_idx]
                                        summary = all_summaries[orig_idx].copy()
                                        summary["rerank_score"] = relevance_score
                                        reranked_summaries.append(summary)

                                all_summaries = reranked_summaries
                                debug_log(
                                    f"é‡æ’åºå®Œæˆï¼Œä¿ç•™ {len(all_summaries)} æ¡æ‘˜è¦"
                                )

                    except Exception as e:
                        debug_log(f"è¯­ä¹‰é‡æ’åºå¤±è´¥: {e}")

                # æœ€ç»ˆè¯„åˆ† - åªç”¨RAGå’Œrerank
                await progress_mgr.update_step(
                    "ğŸ† è®¡ç®—æœ€ç»ˆè¯„åˆ†å¹¶ç­›é€‰ç»“æœ", __event_emitter__
                )

                def zminmax(vals):
                    if not vals:
                        return []
                    lo, hi = min(vals), max(vals)
                    if hi - lo < 1e-6:
                        return [0.5 for _ in vals]
                    return [(v - lo) / (hi - lo) for v in vals]

                if all_summaries:
                    rags = [float(x.get("rag_similarity", 0)) for x in all_summaries]
                    rers = [float(x.get("rerank_score", 0)) for x in all_summaries]

                    rags_n, rers_n = map(zminmax, (rags, rers))

                    for i, s in enumerate(all_summaries):
                        s["final_score"] = (
                            self.valves.RAG_WEIGHT * rags_n[i]
                            + self.valves.RERANK_WEIGHT * rers_n[i]
                        )

                    # æ’åºå¹¶ç­›é€‰
                    all_summaries.sort(
                        key=lambda x: x.get("final_score", 0), reverse=True
                    )
                    final_summaries = all_summaries[: self.valves.RERANK_TOP_N]

                    # é˜ˆå€¼è¿‡æ»¤
                    if (
                        self.valves.ENABLE_RAG_ENHANCEMENT
                        and self.valves.EMIT_ONLY_RAG_PASS
                    ):
                        thr = max(0.03, float(self.valves.SIMILARITY_THRESHOLD) * 0.6)
                        if any(is_wikipedia(r["url"]) for r in successful_results):
                            thr = max(0.03, thr * 0.4)
                            debug_log(f"æ£€æµ‹åˆ°ç»´åŸºç™¾ç§‘ï¼Œæ”¾å®½é˜ˆå€¼åˆ°: {thr}")

                        final_summaries = [
                            s
                            for s in final_summaries
                            if float(s.get("rag_similarity", 0)) >= thr
                            or s["final_score"] >= 0.35
                        ]
                else:
                    final_summaries = []

                debug_log(f"æœ€ç»ˆä¿ç•™ {len(final_summaries)} æ¡æ‘˜è¦")

                # å‘é€å¼•ç”¨
                for idx, summary in enumerate(final_summaries):
                    await emit_citation_data(summary, __event_emitter__, run_id, idx)

                await progress_mgr.update_step("ğŸ‰ å¤„ç†å®Œæˆï¼", __event_emitter__)

                # æ„å»ºè¿”å›ä½“ - ç®€åŒ–statsä¿¡æ¯
                results_data = []
                for summary in final_summaries:
                    item = {
                        "title": summary.get("title") or "",
                        "url": summary.get("url"),
                        "rag_similarity": float(summary.get("rag_similarity", 0.0)),
                        "rerank_score": float(summary.get("rerank_score", 0.0)),
                        "final_score": float(summary.get("final_score", 0.0)),
                        "key_points": summary.get("key_points", []),
                        "snippet": summary.get(
                            "content", ""
                        ),  # ä½¿ç”¨å®Œæ•´æ‘˜è¦ä½œä¸ºsnippet
                    }
                    results_data.append(item)

                return json.dumps(
                    {
                        "summaries_count": len(final_summaries),
                        "summaries": results_data,
                    },
                    ensure_ascii=False,
                    indent=2,
                )

            else:
                # æ™ºèƒ½æ‘˜è¦æœªå¯ç”¨
                for idx, r in enumerate(successful_results):
                    await emit_citation_data(r, __event_emitter__, run_id, idx)

                results_data = []
                for r in successful_results:
                    result_item = {
                        "title": (r.get("title") or ""),
                        "url": r.get("url"),
                        "snippet": take_text(r.get("content", ""), 600),
                    }
                    results_data.append(result_item)

                return json.dumps(
                    {
                        "results_count": len(successful_results),
                        "results": results_data,
                        "errors": error_results,
                    },
                    ensure_ascii=False,
                    indent=2,
                )

        except Exception as e:
            debug_log("æ™ºèƒ½ç½‘é¡µè¯»å–å¤±è´¥", e)
            return json.dumps(
                {
                    "error": str(e),
                    "summaries_count": 0,
                    "summaries": [],
                    "errors": [{"url": url, "error": "å¤„ç†å¤±è´¥"} for url in urls],
                },
                ensure_ascii=False,
                indent=2,
            )

    # ======================== Rawç½‘é¡µè¯»å–åŠŸèƒ½ ========================
    async def web_scrape_raw(
        self,
        urls: List[str],
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ Rawç½‘é¡µè¯»å–å·¥å…·"""

        def next_run_id(tool: str) -> str:
            self.run_seq += 1
            return f"{tool}-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{self.run_seq}"

        def take_text(text: str, max_chars: int) -> str:
            if text is None:
                return ""
            if len(text) <= max_chars or max_chars <= 0:
                return text
            cut = text[:max_chars]
            # å°è¯•åœ¨æœ€è¿‘çš„å¥è¯»ç¬¦å¤„æˆªæ–­
            p = max(
                cut.rfind("ã€‚"),
                cut.rfind("ï¼"),
                cut.rfind("ï¼Ÿ"),
                cut.rfind("."),
                cut.rfind(";"),
            )
            if p >= max_chars * 0.6:  # ä»…åœ¨è¾ƒé åæ‰ä½¿ç”¨
                return cut[: p + 1] + " â€¦"
            return cut + " â€¦"

        def split_text_chunks(text: str, size: int) -> List[str]:
            if text is None:
                return [""]
            if size is None or size <= 0:
                return [text]
            return [text[i : i + size] for i in range(0, len(text), size)]

        async def emit_citation_data(r: Dict, __event_emitter__, run_id: str, idx: int):
            if not (__event_emitter__ and self.valves.CITATION_LINKS):
                return

            full_doc = r.get("content") or ""
            doc_for_emit = take_text(full_doc, self.valves.CITATION_DOC_MAX_CHARS)
            chunks = split_text_chunks(doc_for_emit, self.valves.CITATION_CHUNK_SIZE)
            base_title = (r.get("title") or "") or (r.get("url") or "Source")
            base_url = (r.get("url") or "").strip()

            for ci, chunk in enumerate(chunks, 1):
                if self.valves.UNIQUE_REFERENCE_NAMES:
                    src_name = f"{base_title} | {base_url} | {run_id}#{idx}-{ci}-{uuid4().hex[:6]}"
                else:
                    src_name = base_url or base_title

                payload = {
                    "type": "citation",
                    "data": {
                        "document": [chunk],
                        "metadata": [
                            {
                                "title": base_title,
                                "date_accessed": datetime.now().isoformat(),
                            }
                        ],
                        "source": {
                            "name": src_name,
                            "url": base_url or "",
                            "type": r.get("source_type", "webpage"),
                        },
                    },
                }
                await __event_emitter__(payload)

                if self.valves.PERSIST_CITATIONS:
                    self.citations_history.append(payload)
                    if len(self.citations_history) > self.valves.PERSIST_CITATIONS_MAX:
                        self.citations_history = self.citations_history[
                            -self.valves.PERSIST_CITATIONS_MAX :
                        ]

        run_id = next_run_id("web-scrape-raw")
        if self.valves.PERSIST_CITATIONS and __event_emitter__:
            for old in self.citations_history:
                await __event_emitter__(old)

        def debug_log(message: str, error=None):
            if self.valves.DEBUG_MODE:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"[DEBUG {timestamp}] {message}")
                if error:
                    print(f"[DEBUG ERROR] {str(error)}")
                    print(f"[DEBUG TRACEBACK] {traceback.format_exc()}")

        async def emit_status(
            description: str, done: bool, action: str, urls_list: List[str]
        ):
            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "done": done,
                            "action": f"{action}:{run_id}",
                            "description": description,
                            "urls": urls_list,
                        },
                    }
                )

        try:
            debug_log(f"å¼€å§‹Rawç½‘é¡µè¯»å–ï¼ŒURLæ•°é‡: {len(urls)}")

            await emit_status(
                f"ğŸŒ æ­£åœ¨Rawè¯»å– {len(urls)} ä¸ªç½‘é¡µ", False, "web_search", urls
            )

            async def process_url(url):
                jina_url = f"https://r.jina.ai/{url}"
                headers = {
                    "X-No-Cache": "true",
                    "X-With-Images-Summary": "true",
                    "X-With-Links-Summary": "true",
                    "Authorization": f"Bearer {self.valves.JINA_API_KEY}",
                }

                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.get(
                            jina_url, headers=headers, timeout=60
                        ) as response:
                            response.raise_for_status()
                            content = await response.text()

                    if not content or content.strip() == "":
                        return {
                            "content": "",
                            "title": url,
                            "url": url,
                            "error": "è¿”å›å†…å®¹ä¸ºç©º",
                            "status": "empty",
                        }

                    debug_log(f"æˆåŠŸè¯»å–URL {url}ï¼Œå†…å®¹é•¿åº¦: {len(content)}")

                    return {
                        "content": content,
                        "title": f"ç½‘é¡µå†…å®¹ - {url.split('/')[2] if '/' in url else url}",
                        "url": url,
                        "status": "success",
                    }

                except Exception as e:
                    error_message = f"è¯»å–ç½‘é¡µ {url} æ—¶å‡ºé”™: {str(e)}"
                    debug_log(f"å¤„ç†URLå¤±è´¥: {url}", e)
                    return {
                        "content": "",
                        "title": url,
                        "url": url,
                        "error": error_message,
                        "status": "error",
                    }

            tasks = [process_url(url) for url in urls]
            results = await asyncio.gather(*tasks)

            successful_results = []
            error_results = []

            for result in results:
                if result.get("status") == "success" and result.get("content"):
                    successful_results.append(result)
                else:
                    error_results.append(result)

            for idx, r in enumerate(successful_results):
                await emit_citation_data(r, __event_emitter__, run_id, idx)

            await emit_status(
                f"ğŸ‰ Rawè¯»å–å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(successful_results)} ä¸ª",
                True,
                "web_search",
                urls,
            )

            if self.valves.RAW_OUTPUT_FORMAT.lower() == "json":
                results_data = []
                for r in successful_results:
                    result_item = {
                        "title": (r.get("title") or ""),
                        "url": r.get("url"),
                        "content": take_text(
                            r.get("content", ""), self.valves.RETURN_CONTENT_MAX_CHARS
                        ),
                    }
                    results_data.append(result_item)

                return json.dumps(
                    {"results": results_data, "errors": error_results},
                    ensure_ascii=False,
                    indent=2,
                )
            else:
                final_results = []
                for result in successful_results:
                    content = take_text(
                        result.get("content", ""), self.valves.RETURN_CONTENT_MAX_CHARS
                    )
                    final_results.append(
                        f"""URL: {result['url']}
æ ‡é¢˜: {result.get('title', '')}
å†…å®¹: {content}
"""
                    )

                for result in error_results:
                    final_results.append(
                        f"""URL: {result['url']}
é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}
"""
                    )

                final_result = "\n".join(final_results)
                if not final_result.strip():
                    final_result = "æ‰€æœ‰ç½‘é¡µè¯»å–å‡å¤±è´¥ã€‚"

                result_text = f"""Rawç½‘é¡µè¯»å–ç»“æœ:

ğŸ“Š æ€»URLæ•°: {len(urls)}
âœ… æˆåŠŸè¯»å–: {len(successful_results)}
âŒ å¤±è´¥è¯»å–: {len(error_results)}

åŸå§‹ç½‘é¡µå†…å®¹:
{final_result}"""

                return result_text

        except Exception as e:
            debug_log("Rawç½‘é¡µè¯»å–å¤±è´¥", e)

            if self.valves.RAW_OUTPUT_FORMAT.lower() == "json":
                return json.dumps(
                    {
                        "error": str(e),
                        "results": [],
                        "errors": [{"url": url, "error": "å¤„ç†å¤±è´¥"} for url in urls],
                    },
                    ensure_ascii=False,
                    indent=2,
                )
            else:
                return f"""âŒ Rawç½‘é¡µè¯»å–å‡ºç°é”™è¯¯: {str(e)}

è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®ã€‚"""

    # ======================== AIæ™ºèƒ½æœç´¢ ========================
    async def search_ai_intelligent(
        self,
        query: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸ¤– é«˜çº§AIæ™ºèƒ½æœç´¢å·¥å…·"""

        def next_run_id(tool: str) -> str:
            self.run_seq += 1
            return f"{tool}-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{self.run_seq}"

        def take_text(text: str, max_chars: int) -> str:
            if text is None:
                return ""
            if len(text) <= max_chars or max_chars <= 0:
                return text
            cut = text[:max_chars]
            # å°è¯•åœ¨æœ€è¿‘çš„å¥è¯»ç¬¦å¤„æˆªæ–­
            p = max(
                cut.rfind("ã€‚"),
                cut.rfind("ï¼"),
                cut.rfind("ï¼Ÿ"),
                cut.rfind("."),
                cut.rfind(";"),
            )
            if p >= max_chars * 0.6:  # ä»…åœ¨è¾ƒé åæ‰ä½¿ç”¨
                return cut[: p + 1] + " â€¦"
            return cut + " â€¦"

        def split_text_chunks(text: str, size: int) -> List[str]:
            if text is None:
                return [""]
            if size is None or size <= 0:
                return [text]
            return [text[i : i + size] for i in range(0, len(text), size)]

        async def emit_citation_data(r: Dict, __event_emitter__, run_id: str, idx: int):
            if not (__event_emitter__ and self.valves.CITATION_LINKS):
                return

            full_doc = r.get("content") or ""
            doc_for_emit = take_text(full_doc, self.valves.CITATION_DOC_MAX_CHARS)
            chunks = split_text_chunks(doc_for_emit, self.valves.CITATION_CHUNK_SIZE)
            base_title = (r.get("title") or "") or (r.get("url") or "Source")
            base_url = (r.get("url") or "").strip()

            for ci, chunk in enumerate(chunks, 1):
                if self.valves.UNIQUE_REFERENCE_NAMES:
                    src_name = f"{base_title} | {base_url} | {run_id}#{idx}-{ci}-{uuid4().hex[:6]}"
                else:
                    src_name = base_url or base_title

                payload = {
                    "type": "citation",
                    "data": {
                        "document": [chunk],
                        "metadata": [
                            {
                                "title": base_title,
                                "date_accessed": datetime.now().isoformat(),
                            }
                        ],
                        "source": {
                            "name": src_name,
                            "url": base_url or "",
                            "type": r.get("source_type", "webpage"),
                        },
                    },
                }
                await __event_emitter__(payload)

                if self.valves.PERSIST_CITATIONS:
                    self.citations_history.append(payload)
                    if len(self.citations_history) > self.valves.PERSIST_CITATIONS_MAX:
                        self.citations_history = self.citations_history[
                            -self.valves.PERSIST_CITATIONS_MAX :
                        ]

        run_id = next_run_id("ai-search")
        if self.valves.PERSIST_CITATIONS and __event_emitter__:
            for old in self.citations_history:
                await __event_emitter__(old)

        def debug_log(message: str, error=None):
            if self.valves.DEBUG_MODE:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"[DEBUG {timestamp}] {message}")
                if error:
                    print(f"[DEBUG ERROR] {str(error)}")
                    print(f"[DEBUG TRACEBACK] {traceback.format_exc()}")

        async def emit_status(
            description: str, status: str = "in_progress", done: bool = False
        ):
            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "status": status,
                            "description": description,
                            "done": done,
                            "action": f"ai_search:{run_id}",
                        },
                    }
                )

        try:
            debug_log(f"å¼€å§‹AIæ™ºèƒ½æœç´¢: {query}")
            await emit_status(f"ğŸ¤– æ­£åœ¨è¿›è¡Œé«˜çº§AIæ™ºèƒ½æœç´¢: {query}")

            headers = {
                "Authorization": f"Bearer {self.valves.BOCHA_API_KEY}",
                "Content-Type": "application/json",
            }

            payload = {
                "query": query,
                "freshness": self.valves.FRESHNESS,
                "answer": True,
                "stream": False,
                "count": self.valves.AI_SEARCH_COUNT,
            }

            await emit_status("â³ è¿æ¥AIæœç´¢æœåŠ¡å™¨...")

            resp = requests.post(
                self.valves.AI_SEARCH_ENDPOINT,
                headers=headers,
                json=payload,
                timeout=120,
            )
            resp.raise_for_status()
            data = resp.json()

            source_context_list = []
            ai_answers = []
            follow_up_questions = []

            await emit_status("ğŸ§  AIæ­£åœ¨åˆ†ææœç´¢ç»“æœ...")

            if "messages" in data:
                for msg in data["messages"]:
                    msg_role = msg.get("role", "")
                    msg_type = msg.get("type", "")
                    content_type = msg.get("content_type", "")
                    content = msg.get("content", "")

                    if (
                        msg_role == "assistant"
                        and msg_type == "source"
                        and content_type == "webpage"
                    ):
                        try:
                            content_obj = json.loads(content)
                            if "value" in content_obj and isinstance(
                                content_obj["value"], list
                            ):
                                await emit_status(
                                    f"ğŸ“„ å¤„ç† {len(content_obj['value'])} ä¸ªAIæœç´¢ç»“æœ..."
                                )

                                for i, item in enumerate(content_obj["value"]):
                                    search_content = item.get(
                                        "summary", ""
                                    ) or item.get("snippet", "")
                                    if not search_content:
                                        continue

                                    result_item = {
                                        "content": search_content,
                                        "title": item.get("name", ""),
                                        "url": item.get("url", ""),
                                        "site_name": item.get("siteName", ""),
                                        "date_published": item.get("datePublished", ""),
                                        "source_type": "é«˜çº§AIæ™ºèƒ½æœç´¢",
                                    }
                                    source_context_list.append(result_item)

                        except json.JSONDecodeError as e:
                            debug_log("è§£æAIæœç´¢ç»“æœJSONå¤±è´¥", e)

                    elif (
                        msg_role == "assistant"
                        and msg_type == "answer"
                        and content_type == "text"
                    ):
                        ai_answers.append(f"ğŸ¤– {content}")
                        await emit_status(f"âœ¨ AIç”Ÿæˆäº†ç¬¬ {len(ai_answers)} ä¸ªå›ç­”...")

                    elif (
                        msg_role == "assistant"
                        and msg_type == "follow_up"
                        and content_type == "text"
                    ):
                        follow_up_questions.append(f"ğŸ’­ {content}")
                        await emit_status(
                            f"ğŸ’¡ AIå»ºè®®äº†ç¬¬ {len(follow_up_questions)} ä¸ªè¿½é—®..."
                        )

            # å‘é€å¼•ç”¨
            for idx, r in enumerate(source_context_list):
                await emit_citation_data(r, __event_emitter__, run_id, idx)

            results_data = []
            for r in source_context_list:
                result_item = {
                    "title": (r.get("title") or ""),
                    "url": r.get("url"),
                    "snippet": take_text(r.get("content", ""), 450),
                }
                if self.valves.RETURN_CONTENT_IN_RESULTS:
                    result_item["content"] = take_text(
                        r.get("content", ""), self.valves.RETURN_CONTENT_MAX_CHARS
                    )
                results_data.append(result_item)

            result = {
                "search_results": results_data,
                "ai_answers": ai_answers,
                "follow_up_questions": follow_up_questions,
                "summary": {
                    "total_results": len(source_context_list),
                    "ai_answers_count": len(ai_answers),
                    "follow_up_count": len(follow_up_questions),
                    "search_type": "ğŸ¤– é«˜çº§AIæ™ºèƒ½æœç´¢",
                    "timestamp": datetime.now().isoformat(),
                },
            }

            await emit_status(
                status="complete",
                description=f"ğŸ‰ AIæ™ºèƒ½æœç´¢å®Œæˆï¼{len(source_context_list)} ä¸ªç»“æœï¼Œ{len(ai_answers)} ä¸ªAIç­”æ¡ˆ",
                done=True,
            )

            return json.dumps(result, ensure_ascii=False, indent=2)

        except Exception as e:
            debug_log("AIæ™ºèƒ½æœç´¢å¤±è´¥", e)
            error_details = {
                "error": str(e),
                "type": "âŒ AIæ™ºèƒ½æœç´¢é”™è¯¯",
                "debug_info": (
                    traceback.format_exc() if self.valves.DEBUG_MODE else None
                ),
            }
            await emit_status(
                status="error", description=f"âŒ AIæœç´¢å‡ºé”™: {str(e)}", done=True
            )
            return json.dumps(error_details, ensure_ascii=False, indent=2)


# ======================== Functionç±» - æš´éœ²å·¥å…·å‡½æ•° ========================
class Function:
    def __init__(self):
        self.tools = Tools()

    # Kimi AIåŸºç¡€æœç´¢ï¼ˆä¿®å¤ç‰ˆï¼šå¼ºåˆ¶è”ç½‘ï¼‰
    async def kimi_ai_search(
        self,
        search_query: str,
        context: str = "",
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ™ Kimi AIè”ç½‘æœç´¢ - å¼ºåˆ¶ä½¿ç”¨å†…ç½®$web_searchå·¥å…·è¿›è¡ŒçœŸå®è”ç½‘æœç´¢"""
        return await self.tools.kimi_ai_search(search_query, context, __event_emitter__)

    # ä¸“ä¸šä¸­æ–‡ç½‘é¡µæœç´¢
    async def search_chinese_web(
        self,
        query: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸ‡¨ğŸ‡³ ä¸“ä¸šä¸­æ–‡ç½‘é¡µæœç´¢å·¥å…·"""
        return await self.tools.search_chinese_web(query, __event_emitter__)

    # ä¸“ä¸šè‹±æ–‡ç½‘é¡µæœç´¢
    async def search_english_web(
        self,
        query: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ ä¸“ä¸šè‹±æ–‡ç½‘é¡µæœç´¢å·¥å…·"""
        return await self.tools.search_english_web(query, __event_emitter__)

    # æ™ºèƒ½ç½‘é¡µè¯»å–ï¼ˆä¿®å¤ç‰ˆï¼‰
    async def web_scrape(
        self,
        urls: List[str],
        user_request: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ æ™ºèƒ½ç½‘é¡µè¯»å–å·¥å…·ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        return await self.tools.web_scrape(urls, user_request, __event_emitter__)

    # Rawç½‘é¡µè¯»å–ï¼ˆä¸åšå¤„ç†ï¼‰
    async def web_scrape_raw(
        self,
        urls: List[str],
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸŒ Rawç½‘é¡µè¯»å–å·¥å…·"""
        return await self.tools.web_scrape_raw(urls, __event_emitter__)

    # é«˜çº§AIæ™ºèƒ½æœç´¢
    async def search_ai_intelligent(
        self,
        query: str,
        __event_emitter__: Optional[Callable[[dict], Any]] = None,
    ) -> str:
        """ğŸ¤– é«˜çº§AIæ™ºèƒ½æœç´¢å·¥å…·"""
        return await self.tools.search_ai_intelligent(query, __event_emitter__)
